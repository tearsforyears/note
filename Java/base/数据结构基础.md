# java数据结构的实现

[TOC]

---

## ArrayList

顾名思义是利用Array去存储的list,对于ArrayList我们需要关注其扩容机制

```java
transient Object[] elementData;
private int size;
```

ArrayList里面大量使用了`java.util.Arrays.copyOf`来赋值两个数组

其初始化逻辑如下

```java
public ArrayList(int initialCapacity) { 
if (initialCapacity > 0) {
   this.elementData = new Object[initialCapacity]; // initialCapacity=10
 } else if (initialCapacity == 0) {
   this.elementData = EMPTY_ELEMENTDATA; // 空数组 不带参数的构造函数相当于此
 } else {
   throw new IllegalArgumentException("Illegal Capacity: "+
                                      initialCapacity);
 }
}
```

add

```java
public boolean add(E e) {
  ensureCapacityInternal(size + 1);
  elementData[size++] = e;
  return true;
}
private void ensureCapacityInternal(int minCapacity) {
  if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 是否为空
    minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); // 最小为10
  }
  ensureExplicitCapacity(minCapacity);
}
private void ensureExplicitCapacity(int minCapacity) {
  // 该字段定义在ArrayList的父类AbstractList,用于存储结构修改次数,这确保了快速失败机制。
  modCount++;
  if (minCapacity - elementData.length > 0)
    grow(minCapacity);
}
private void grow(int minCapacity) {
  int oldCapacity = elementData.length;
  int newCapacity = oldCapacity + (oldCapacity >> 1);

  if (newCapacity - minCapacity < 0)
    newCapacity = minCapacity;
  if (newCapacity - MAX_ARRAY_SIZE > 0)
    // 调整新容量上限或者抛出OutOfMemoryError
    newCapacity = hugeCapacity(minCapacity);
  elementData = Arrays.copyOf(elementData, newCapacity);
}
private static int hugeCapacity(int minCapacity) {
  if (minCapacity < 0) // overflow
    throw new OutOfMemoryError();
  return (minCapacity > MAX_ARRAY_SIZE) ?
    Integer.MAX_VALUE :
  MAX_ARRAY_SIZE; // 初始化为最大数组长度
}
```

`System.arraycopy`和`Arrays.copyOf`方法

前者是native方法,用于已经有了两个数组把一个复制到另一个上,而copyOf则是在其内部new了个数组.

```java
public static native void arraycopy(Object src,  int  srcPos,Object dest, int destPos,int length);
// 数组复制 src->dest
// 从srcPos和destPos开始,src数组的srcPos以后的内容都赋值到dest相应的位置上了
// length要复制的元素的个数,(参考c++的复制)
// 其底层是用while直接操作内存地址 其还是一个原子操作,其根据复制类型的不同会引用不同的
// 复制函数
```

```java
public static int[] copyOf(int[] original, int newLength) {
  int[] copy = new int[newLength]; // 多分配了内存空间
  System.arraycopy(original, 0, copy, 0,
                   Math.min(original.length, newLength));
  return copy;
}
```

我们看下其remove方法的实现

```java
public E remove(int index) {
  rangeCheck(index);

  modCount++;
  E oldValue = elementData(index);

  int numMoved = size - index - 1;
  if (numMoved > 0)
    System.arraycopy(elementData, index+1, elementData, index,
                     numMoved); // 从这里可以看到其本质上还是用了向后缩紧的办法
  elementData[--size] = null; // clear to let GC do its work

  return oldValue;
}
```



## Vector

相比于ArrayList还多了一个`capacityIncrement`属性

```java
public synchronized boolean add(E e) {
  modCount++;
  ensureCapacityHelper(elementCount + 1);
  elementData[elementCount++] = e;
  return true;
}


private void ensureCapacityHelper(int minCapacity) {
  if (minCapacity - elementData.length > 0)
    grow(minCapacity);
}


private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;


private void grow(int minCapacity) {
  int oldCapacity = elementData.length;
  int newCapacity = oldCapacity + ((capacityIncrement > 0) ?
                                   capacityIncrement : oldCapacity);
  // 增长曲线在这,如果增长>0才有用,不然就默认扩充一倍
  if (newCapacity - minCapacity < 0)
    newCapacity = minCapacity;
  if (newCapacity - MAX_ARRAY_SIZE > 0)
    newCapacity = hugeCapacity(minCapacity);
  elementData = Arrays.copyOf(elementData, newCapacity);
}
```

除了synchronized我们可以看到其代码和ArrayList一样.因为其性能关系,其后续不再被建议使用,如果要考虑线程安全的问题,移步到ConpyOnWriteArrayList

## Stack

```java
public class Stack<E> extends Vector<E> {}
public E push(E item) {
    addElement(item);
    return item;
}
public synchronized E pop() {
    E obj;
    int len = size();
    obj = peek();
    removeElementAt(len - 1);
    return obj;
}
```

其本质上就是个Vector,不被建议使用主要方法如上.

## LinkedList

```java
public class LinkedList<E> extends AbstractSequentialList<E> implements List<E>, Deque<E>, Cloneable, java.io.Serializable{
}
```

其主要维护的数据结构

```java
transient int size = 0;
transient Node<E> first;
transient Node<E> last;
private static class Node<E> {
  E item;
  Node<E> next;
  Node<E> prev;

  Node(Node<E> prev, E element, Node<E> next) {
    this.item = element;
    this.next = next;
    this.prev = prev;
  }
}
```

其方法实现和数据结构上的基本一致

```java
public boolean add(E e) {
    linkLast(e);
    return true;
}

void linkLast(E e) {
    final Node<E> l = last;
    final Node<E> newNode = new Node<>(l, e, null);
    last = newNode;
    if (l == null)
        first = newNode;
    else
        l.next = newNode;
    size++;
    modCount++;
}
public void add(int index, E element) {
    checkPositionIndex(index);
    if (index == size)
        linkLast(element);
    else
        linkBefore(element, node(index));
}
/*这里有个思想是如果节点的index大于一半就从后面开始找*/
Node<E> node(int index) {
    if (index < (size >> 1)) {
        Node<E> x = first;
        for (int i = 0; i < index; i++)
            x = x.next;
        return x;
    } else {
        Node<E> x = last;
        for (int i = size - 1; i > index; i--)
            x = x.prev;
        return x;
    }
}
void linkBefore(E e, Node<E> succ) {
    final Node<E> pred = succ.prev;
    final Node<E> newNode = new Node<>(pred, e, succ);
    succ.prev = newNode;
    if (pred == null)
        first = newNode;
    else
        pred.next = newNode;
    size++;
    modCount++;
}
```

其remove实现如下

```java
public E removeFirst() {
    final Node<E> f = first;
    if (f == null)
        throw new NoSuchElementException();
    return unlinkFirst(f);
}

private E unlinkFirst(Node<E> f) {
    final E element = f.item;
    final LinkedLisNode<E> next = f.next;
    f.item = null;
    f.next = null;
    first = next;
    if (next == null)
        last = null; // 一个元素的时候,认为是没有尾节点的
    else
        next.prev = null;
    size--;
    modCount++;
    return element;
}

public E remove(int index) {
    checkElementIndex(index);
    return unlink(node(index));
}

E unlink(Node<E> x) {
    final E element = x.item;
    final Node<E> next = x.next;
    final Node<E> prev = x.prev;
    if (prev == null) {
        first = next;
    } else {
        prev.next = next;
        x.prev = null;
    }
  	
  	// 如果下个节点是空,则该节点是尾结点
    if (next == null) {
        last = prev;
    } else {
        next.prev = prev;
        x.next = null;
    }

    x.item = null;
    size--;
    modCount++;
    return element;
}
```

## ArrayDeque

```java
public class ArrayDeque<E> extends AbstractCollection<E> implements Deque<E>, Cloneable, Serializable{
}
```

可以看到其没有实现List接口

```java
transient Object[] elements; // 可以看到数据结构仍是数组形式
transient int head; // 头的索引
transient int tail; // 尾索引
private static final int MIN_INITIAL_CAPACITY = 8;
// 最小容量为8,其容量必须是2的幂
```

其初始化最大为$$2^{30}$$最小为8,如果初始化的数值不为2的幂,那么就就近取2的幂,下面是追加到队尾

```java
public void addLast(E e) {
    if (e == null)
        throw new NullPointerException();
    // 将元素存放在tail位置,即原尾节点的下一个空节点位置
    elements[tail] = e;
   // 下面这个运算是如果tail超过8就会都变成8,否则就全归零
    if ((tail = (tail + 1) & (elements.length - 1)) == head)
        // 扩容的方法
        doubleCapacity();
}
```

这里如果`elements.length`不是2的幂,那么其位与就不会是`0b1111`的形式,也就相当于%运算了,如果到头了就扩容,同理添加到头部

```java
public void addFirst(E e) {-10
    if (e == null)
        throw new NullPointerException();
    elements[head = (head - 1) & (elements.length - 1)] = e;
    // 负数的位与需要注意一定要换成补码去进行计算,否则没有任何意义
  	// 逻辑和上面一样不够了就扩容
    if (head == tail)
        doubleCapacity();
}
```

知道了计算坐标其删除就比较简单了

```java
public E pollFirst() {
    int h = head;
    @SuppressWarnings("unchecked")
    E result = (E) elements[h];
    if (result == null)
        return null;
    elements[h] = null;    
    head = (h + 1) & (elements.length - 1);
    return result;
}
public E pollLast() {
    int t = (tail - 1) & (elements.length - 1);
    @SuppressWarnings("unchecked")
    E result = (E) elements[t];
    if (result == null)
        return null;
    elements[t] = null;
    tail = t;
    return result;
}
```



## *HashMap

hashmap的在JDK1.7和JDK1.8有着不同的实现,在JDK1.7数据结构用的是链表加数组,而1.8用的是链表加数组加红黑树,红黑树用于解决hash冲突性能上要比链表更高.除此之外,JDK1.8在容量升级上进行了优化,使得扩容时候数组迁移的效率大大提高(参考下文位运算和HashMap的关系)

```java
public class HashMap<K,V> extends AbstractMap<K,V> implements Map<K,V>, Cloneable, Serializable{}
```

HashMap的底层是用数组做的存储结构,同时支持链表和红黑树的操作方法,其实现方法也很简单我们看其主要数据结构,如下的Node结构和TreeNode结构

```java
transient Node<K,V>[] table; // 存储的结构
transient Set<Map.Entry<K,V>> entrySet; // 存放kv对的结构,即Node
transient int size;
static class Node<K,V> implements Map.Entry<K,V> {
  final int hash; // 存放key的hashCode
  final K key;
  V value;
  Node<K,V> next; // 链表指针指向下
} 

static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
  TreeNode<K,V> parent;  // red-black tree links
  TreeNode<K,V> left;
  TreeNode<K,V> right;
  TreeNode<K,V> prev;    // needed to unlink next upon deletion
  boolean red;
}
// 下面这个类是LinkedHashMap里面的,把单向链表换成双向的
static class Entry<K,V> extends HashMap.Node<K,V> {
  Entry<K,V> before, after;
  Entry(int hash, K key, V value, Node<K,V> next) {
    super(hash, key, value, next); // 调用的是Node的构造
  }
}
```

![](https://img-blog.csdnimg.cn/20200622102700374.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc2NzAxNQ==,size_16,color_FFFFFF,t_70)

我们可以看到其三个数据结构,在HashMap内就只有Node管理的单向链表,TreeNode所管理的红黑树.从存储结构`Node<K,V>[] table;`看,这个Node可以是双向链表的Entry,也可以是红黑树的TreeNode,所以存储的节点通过继承通用.HashMap的容量扩大为16开始,每次扩容都是一倍.

HashMap一系里面的数据结构就四种

-   Node数组 或者叫哈希表
-   Node所代表的单链表
-   LinkedHashMap.Entry所代表的双链表
-   TreeNode所代表的红黑树

![](https://img-blog.csdnimg.cn/20200621201115134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc2NzAxNQ==,size_16,color_FFFFFF,t_70)

我们在看一下其他数据结构

```java
static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // 16 为默认容量
static final int MAXIMUM_CAPACITY = 1 << 30; // 2^30 最大容量
static final float DEFAULT_LOAD_FACTOR = 0.75f; // 负载因子(*)

static final int TREEIFY_THRESHOLD = 8; // 链表变成红黑树的阈值,这个阈值指的是
// 冲突的个数即存储节点,即存储冲突节点的链表长度而不是存储Node[]的实际长度
// 红黑树在用resize(),split()方法时才会用到该字段,在remove里面没有用到,所以在

// remove中判断普通链表的个数可能不是6个
static final int UNTREEIFY_THRESHOLD = 6; // 红黑树还原为链表的阈值

// 哈希表树化的最小容量,如果容量小于64就不允许树形化
static final int MIN_TREEIFY_CAPACITY = 64;

// 扩容阈值(实际个数),当数组实际元素小于该值,则数组就会扩容
// 同时这个数值也作为tableSizeFor的赋值存储,用于计算初始化时候的大小
// 主要看put的代码和resize的代码即可理解此部分
int threshold;
// 数组的加载因子,也就是说,这个数组最多元素的百分比
final float loadFactor;
```

`扩容阈值<容量*加载因子`就需要扩容,在每次put方法里都会判断到resize执行,其中加载因子可以大于1,来解释下这个大于1的意思,因为hashmap是一个数组作为主要存储的,数组的头结点又可以代表链表,这些链表元素不在数组里,但会在链表里,但是如果大于1那证明一定会发生冲突,如果设置hash大于1就有点损失性能.不符合hash表建立的初衷.

```java
// 其主要构造方法 
public HashMap(int initialCapacity, float loadFactor) {
  if (initialCapacity < 0)
    throw new IllegalArgumentException("Illegal initial capacity: " +
                                       initialCapacity);
  if (initialCapacity > MAXIMUM_CAPACITY)
    initialCapacity = MAXIMUM_CAPACITY;
  if (loadFactor <= 0 || Float.isNaN(loadFactor))
    throw new IllegalArgumentException("Illegal load factor: " +
                                       loadFactor);
  this.loadFactor = loadFactor;
  this.threshold = tableSizeFor(initialCapacity); // 设置扩容阈值
}
// 这个方法能返回一个数比cap大的且是2的指数
// 该算法首先让指定容量cap的二进制的最高位后面的数全部变成了1
static final int tableSizeFor(int cap) {
  int n = cap - 1;
  n |= n >>> 1;
  n |= n >>> 2;
  n |= n >>> 4;
  n |= n >>> 8;
  n |= n >>> 16;
  return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
}
```

我们来看其主要方法

```java
// evict参数在其他用到,这里不做讨论
final void putMapEntries(Map<? extends K, ? extends V> m, boolean evict) {
  int s = m.size();
  if (s > 0) {
    
    if (table == null) { // 判断table是否已经初始化
      // s代表
      float ft = ((float)s / loadFactor) + 1.0F; // 计算了存储容量
      int t = ((ft < (float)MAXIMUM_CAPACITY) ?
               (int)ft : MAXIMUM_CAPACITY); // 如果存储容量合适就用这个初始化了
      if (t > threshold) // 如果容量比扩容阈值还大,那么下次就应该是这个值了
        threshold = tableSizeFor(t); // 计算得到的t大于阈值，则初始化阈值
      
    }else if (s > threshold) // 已初始化，并且m元素个数大于阈值，进行扩容处理
      // s是实际元素的个数,threshold是数组扩容阈值
      // (即数组中真实存在的元素而非数组大小)
      resize(); // * 这个方法需要关注
    
		/* 
		上面的扩容部分分为两个,一个是数组未初始化时,threashold表达的含义是给数组分配的内存的大小,并且在resize中完成初始化,二是数组初始化之后作为扩容的一个阈值,即大于这个值就扩容
    */
    for (Map.Entry<? extends K, ? extends V> e : m.entrySet()) {
      K key = e.getKey();
      V value = e.getValue();
      putVal(hash(key), key, value, false, evict); 
      // 该方法也会初始化数组,内部调用resize
    }
  }
}
```

我们看下resize,resize这个方法既可以初始化也可以扩容,这是哈希表使用的扩容方法

这个扩容是重点,包括扩充容量的大小,新hash值的计算等等,其数组的扩容如下按照2的倍数进行扩展,对于其他几个重要的参数,需要冲突大于8且数组整体大小大于64才进行树形化.

从下面我们也可以看出,什么时候HashMap才需要扩容,即当**加入元素个数大于阈值的时候**,即所谓的阈值是用来限制一次加入的元素的数量的大小.其实从这个角度我们也能看出数组扩容的一些机制.resize要计算新的阈值,其无论是初始化还是扩容之后,threshold的含义都变更为数组扩充的阈值而不是分配内存的大小.

```java
final Node<K, V>[] resize() {
  Node<K, V>[] oldTab = table;
  int oldCap = (oldTab == null) ? 0 : oldTab.length;
  int oldThr = threshold; 
  // 阈值存储,这里两个阈值都有用到,newCap的计算是最后内存分配的实际大小
  // 利用到threshold都是初始化
  int newCap, newThr = 0; // 新容量,新阈值

  if (oldCap > 0) { // oldCap是实际元素个数
    if (oldCap >= MAXIMUM_CAPACITY) {
      threshold = Integer.MAX_VALUE;
      return oldTab; // 不用扩容
      
    }else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
             oldCap >= DEFAULT_INITIAL_CAPACITY) {
      // 上面的elseif是有table存在正常状况的扩容,计算扩容大小
      // newCap = oldCap * 2
      // newThr = oldThr * 2
      newThr = oldThr << 1; 
    }
    // cap翻倍,阈值不变
  } else if (oldThr > 0) { // 走到这说明是没有初始化的,其阈值全是2的次方数
		// 这是 采用HashMap(int initialCapacity)、
    // HashMap(initialCapacity, loadFactor)这两个构造器创建Map对象
    // 第一次初始化哈希表，新容量newCap就设置为老的阈值
    newCap = oldThr; // 从这里就可以看出tableSizeFor临时存放的东西就是新的阈值
  }else {
  	// 否则，那就是oldThr等于0，oldCap等于0。
    // 实际上就是采用 无参构造器 HashMap() 创建Map对象
    // 容量和阈值都直接初始化为默认值
    newCap = DEFAULT_INITIAL_CAPACITY;
    newThr = (int) (DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
  }
  // 在上面的三种情况中走完之后，如果新阈值newThr还是等于0，那说明newCap >=MAXIMUM_CAPACITY  或者 oldCap < DEFAULT_INITIAL_CAPACITY 的时候都会走到这一步
  if (newThr == 0) {
    float ft = (float) newCap * loadFactor;
    // 第二个判断的原因是因为负载因子可以大于1
    newThr = (newCap < MAXIMUM_CAPACITY && ft < (float) MAXIMUM_CAPACITY ?
              (int) ft : Integer.MAX_VALUE);
  }
  
  
  // 经过重新计算得到新的阈值,这个阈值影响的是
  threshold = newThr;
  
  // 初始化分配孔融数组的内存内存
  Node<K, V>[] newTab = (Node<K, V>[]) new Node[newCap]; // 旧数组是oldTab
  table = newTab;
  
  // 开始转移原来map里面的值
  if (oldTab != null) {
    for (int j = 0; j < oldCap; ++j) {
      Node<K, V> e; // 暂时存放oldTable的Node节点
      if ((e = oldTab[j]) != null) {
        // 将旧数组该位置置空
        oldTab[j] = null;
     		// e.next通过链表查询下个节点是否为空,下个节点为空证明可以直接移动
        // 这里还涉及到两种情况,树的节点或者链表的节点(链地址法)
        if (e.next == null) {
          // 计算新的地址,
          newTab[e.hash & (newCap - 1)] = e;
        }else if (e instanceof TreeNode) {
          // 调用split方法，将红黑树节点也转移到新数组中
          // split方法中具有将红黑树还原为链表的方法
          ((TreeNode<K, V>) e).split(this, newTab, j, oldCap);
        }else {// 证明Node为链表节点
          // 这里我们注意一点前提就是数组扩容的时候每次都是*2相当于左移,
          // 下面的代码思考得带入二进制的补码进行思考
          Node<K, V> loHead = null, loTail = null;
          Node<K, V> hiHead = null, hiTail = null;
          Node<K, V> next;
          do {
            next = e.next;
             // 判断hash的最高位是0还是1,解释在下面
            if ((e.hash & oldCap) == 0) {
              // 高位为0的构成个链表
              if (loTail == null)
                loHead = e;
              else 
                loTail.next = e;
              loTail = e;
            }else { // hash高位是1
              // 高位为1的构成个链表
              if (hiTail == null)
                hiHead = e;
              else
                hiTail.next = e;。
              hiTail = e;
            }
          } while ((e = next) != null);
          
          // 这里我们讲个前提,index值的计算是 hash & cap-1 即计算出的哈希值
          // 取数组长度的模,而扩容每次又是两倍,可以知道新index和旧index之间的关系
          // index1 = hash & cap - 1
          // index2 = hash & 2 * cap -1
          // 以二进制思考上面两个算式 可知 cap-1和2*cap - 1在二进制都是一堆1
          // 唯一的不同是这两个数差了一位,hash值又相同的情况下,易知
          // index1的二进制头部加上一位=index2 => (index1 + oldCap=index2)
          // 例如hash = 0b1011 1010 cap-1=0b0111 2*cap-1=0b1111
          // index1 = 0b010,index2 = 0b1010 
          // 还有另一种情况是,如果index1的结果如下
          // hash = 0b1010 0010  则index1 = 0b010 index2=0b0010
          // 那么就是index1=index2了,这个条件是index2的最高位为0,这是由于hash的
          // 值所导致的,我们需要用如上的(e.hash & oldCap) == 0来获取其最高位
          if (loTail != null) {
            loTail.next = null;
            newTab[j] = loHead; // 计算新hash值且赋值
            // 这里只需要赋值一次的原因是一个链表(根据链地址的相同hash值)
          }
          if (hiTail != null) {
            hiTail.next = null;
            newTab[j + oldCap] = hiHead; //计算新hash值且赋值
          }
        }
      }
    }
  }
  return newTab;
}
```

我们看下普通的`putVal`的实现

```java
public V put(K key, V value) {
  return putVal(hash(key), key, value, false, true); 
  // 这里就确保了值必定会被替换
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) {
  Node<K,V>[] tab; // 存放原来的table
  Node<K,V> p; // 存放索引所在的节点
  int n, i; // n用来存放table长度(实际元素个数),i用来记录index(由hash算出来)
  if ((tab = table) == null || (n = tab.length) == 0)
    n = (tab = resize()).length; // **如果为空初始化数组**
  if ((p = tab[i = (n - 1) & hash]) == null) // 这里不是取模而是&
    tab[i] = newNode(hash, key, value, null); // 不为空给封装成节点放入
  else {
    // 发生hash冲突了,来解决hash冲突,冲突分为3种情况
    Node<K,V> e; // 临时存放新节点
    K k;
    if (p.hash == hash &&
        ((k = p.key) == key || (key != null && key.equals(k))))
      // 如果key已经在相应的位置了就替换value
      e = p; // 此处的e已经拿到了对应的节点,后续才做修改
    else if (p instanceof TreeNode)
      e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
    	// 红黑树的话,自然是调用红黑树的节点放置进去
    else {
      // 这里是普通链表,我们得找一下该节点
      for (int binCount = 0; ; ++binCount){
        if ((e = p.next) == null){ // 遍历到不为空,就添加到节点上
          p.next = newNode(hash, key, value, null);
          if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
            // 如果大于此值则,则树形化
            treeifyBin(tab, hash); // 二叉树化链表
          break;
        }
        if (e.hash == hash &&
            ((k = e.key) == key || (key != null && key.equals(k))))
          // 如果此时某个节点的key值和要加入的key值相等
          break;
        p = e; // 获取节点引用
      }
    }
    
    if (e != null) { // existing mapping for key
      V oldValue = e.value;
      if (!onlyIfAbsent || oldValue == null)
        e.value = value; // 当onlyIfAbsent为false或oldValue为空时候进行替换
      // 否则则不进行替换,即其没有值的时候可以快速完成赋值
      afterNodeAccess(e); // 在HashMap没有实现,给LinkedHashMap等实现了
      return oldValue;
    }
  }
  
  // 如果走到这就是,
  ++modCount; // 这个字段是结构改变次数,即新建节点次数
  if (++size > threshold) // 如果大于阈值,则要进行扩容
    resize();
  afterNodeInsertion(evict); // 和上面同样是空实现
  return null;
}
```

关于红黑树,此处不多介绍.后会专门介绍该树的结构和运行机制,其是一颗高性能的AVLTree.

这里纯粹的代码的扩容研究可能不是很透彻和深刻,我们选择用数学的方法进行研究,利用下面代码导出数据在进行绘图,其threshold和capacity始终保持的关系是 threshold = capacity * loadFactor

```java
Class<HashMap> clz = HashMap.class;
HashMap<Integer, Double> map = clz.newInstance();
Field size = clz.getDeclaredField("size"); // 实际元素的大小
Field threshold = clz.getDeclaredField("threshold"); // 阈值
Field table = clz.getDeclaredField("table"); // 底层数组
size.setAccessible(true);
threshold.setAccessible(true);
table.setAccessible(true);

for (int i = 0; i < 10000; i++) {
  map.put(new Random().nextInt(), new Random().nextDouble());
  int sizeVal = size.getInt(map);
  int thresholdVal = threshold.getInt(map);
  int capacity = ((Object[]) table.get(map)).length;
  System.out.println("size=" + sizeVal + ",threshold=" + thresholdVal + ",capacity=" + capacity);
}
```

所以回答其何时会扩容时,应该是size超过了threshold时,也就是说loadFactor所代表的百分比一旦超过就会发生内存分配.从代码实现层面也是如此

我们来看其另一核心方法,remove如下

```java
public V remove(Object key) {
  Node<K,V> e;
  return (e = removeNode(hash(key), key, null, false, true)) == null ?
    null : e.value;
}
/*
	matchValue 是要key和value都匹配的情况才能移除
	movable 是否移动其他节点,此属性与红黑树有关
*/
final Node<K,V> removeNode(int hash, Object key, Object value,boolean matchValue, boolean movable) {
  Node<K,V>[] tab; // tab临时存放table
  Node<K,V> p; // p存放要修改的元素(利用hash值计算出的对应位置的节点)
  int n, index; // n表示table中的元素个数,index存放hash计算出的索引
  if ((tab = table) != null && (n = tab.length) > 0 &&
      (p = tab[index = (n - 1) & hash]) != null) {
    
    Node<K,V> node = null, e; // node记录清除节点的地址
    K k; V v; // 用来存放key和value即找出的节点的
    if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
      // 如果节点的key刚好就是我们要remove的key,那就是要修改的节点了
      node = p; 
    else if ((e = p.next) != null) { 
      // 走到这说明有hash冲突,且我们要找的key在hash冲突里面
      if (p instanceof TreeNode)
        // 如果是红黑树,就按照红黑树的方式去寻找节点
        node = ((TreeNode<K,V>)p).getTreeNode(hash, key); 
      else {
        // 说明是链表的节点,那就找到链表中对应的节点
        do {
          if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) {
            node = e;
            break;
          }
          p = e; // 根据循环,最后赋值实际结果是p = e.front;
        } while ((e = e.next) != null);
      }
    }
    
    // matchValue是要不要匹配所以这里有处断点表达,true才执行后面断点,
    // 否则其先决条件就是node!=null,否则触发v的赋值且如果value是空直接不执行
    if (node != null && (!matchValue || (v = node.value) == value ||
                         (value != null && value.equals(v)))) {
      if (node instanceof TreeNode)
        // 按照红黑树方法移除节点
        ((TreeNode<K,V>)node).removeTreeNode(this, tab, movable);
      else if (node == p)
        // 如果是无hash冲突节点,直接移除
        tab[index] = node.next;
      else
        // 这里按照上面逻辑走的话,p指的不再是刚开始的节点,而是和e只差一个
        // 即node = e = p.next,此时p代表的含义是e之前的节点,e的含义是匹配上
        // key的节点
        p.next = node.next; // 直接赋值,中间节点丢失,完成链表的remove
      ++modCount; // 改变结构
      --size;
      afterNodeRemoval(node); // 空实现,留给LinkedHashMap的接口
      return node;
    }
  }
  return null;
}
```

同样作为相对复杂的红黑树,这里不展开细节.其他方法相对简单不少,如get,containsKey

```java
public V get(Object key) {
    Node<K, V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}
public boolean containsKey(Object key) {
    return getNode(hash(key), key) != null;
}
final Node<K, V> getNode(int hash, Object key) {
  Node<K, V>[] tab; // 存放table
  Node<K, V> first, e; // first存放hash对应index的Node指针
  // e存放first.next.next...的一个遍历用指针
  int n; // 存放table长度
  K k; // 存放e的key
  if ((tab = table) != null && (n = tab.length) > 0 &&
      (first = tab[(n - 1) & hash]) != null) {
    if (first.hash == hash && 
        ((k = first.key) == key || (key != null && key.equals(k))))
      // 如果是匹配上了头结点就返回(此时没有hash冲突)
      return first;
    if ((e = first.next) != null) {
      if (first instanceof TreeNode) // 红黑树的处理
        return ((TreeNode<K, V>) first).getTreeNode(hash, key);
      // 普通链表,找到冲突中能匹配上的节点
      do {
        if (e.hash == hash &&
            ((k = e.key) == key || (key != null && key.equals(k))))
          return e; // 返回匹配到的节点
      } while ((e = e.next) != null);
    }
  }
  // 没找到
  return null;
}
```

寻找value是一个低性能的方法,即全部遍历hash表

```java
public boolean containsValue(Object value) {
  Node<K, V>[] tab;
  V v;
  if ((tab = table) != null && size > 0) {
    for (int i = 0; i < tab.length; ++i) { 
      for (Node<K, V> e = tab[i]; e != null; e = e.next) {
        if ((v = e.value) == value ||
            (value != null && value.equals(v)))
          return true;
      }
    }
  }
  return false;
}
```

putAll

```java
public void putAll(Map<? extends K, ? extends V> m) {
    putMapEntries(m, true);
}

final void putMapEntries(Map<? extends K, ? extends V> m, boolean evict) {
    int s = m.size();
    if (s > 0) {
        if (table == null) {
            float ft = ((float) s / loadFactor) + 1.0F;
            int t = ((ft < (float) MAXIMUM_CAPACITY) ?
                    (int) ft : MAXIMUM_CAPACITY);
            if (t > threshold)
                threshold = tableSizeFor(t);
          // 这里其实就看出来,threshold就是临时存放初始化的大小
        }else if (s > threshold)
            resize();
        for (Map.Entry<? extends K, ? extends V> e : m.entrySet()) {
            K key = e.getKey();
            V value = e.getValue();
            putVal(hash(key), key, value, false, evict); // 循环调用putVal
        }
    }
}
```

## HashTable

这是JDK1.0实现的一个数据结构都不属于Collection接口,在此处不多加赘述,其用于线程安全上,但是因为synchronized粒度太大,所以不用.此处就不多介绍此类了,如果要线程安全的类使用下面的ConcurrentHashMap.

## LinkedHashMap

来自JDK1.4,直接继承了HashMap,其代码相对较少,主要是复用了父类的代码.其与HashMap最大的不同应该是底层的数据结构变成一张双链表.这张双链表维护着访问顺序,这样一来可以让LinkedHashMap同时具有链表和HashMap的特性.**其最主要的应用场景就是大型的LRU缓存的实现**.

先看其数据结构,出了hashmap的基本数据结构之外,LinkedHashMap还有如下结构,在前文中可以找到其继承关系

```java
static class Entry<K,V> extends HashMap.Node<K,V> {
  Entry<K,V> before, after; // 双向链表的结构基础
  Entry(int hash, K key, V value, Node<K,V> next) {
    super(hash, key, value, next);
  }
}
transient LinkedHashMap.Entry<K,V> head;
transient LinkedHashMap.Entry<K,V> tail;
final boolean accessOrder; // true 为按照访问顺序访问, false为按照插入顺序访问
```

默认是按照插入顺序进行访问的.通过下列构造器可以改变其accessOrder属性

```java
public LinkedHashMap(int initialCapacity,float loadFactor, boolean 
accessOrder) {
    super(initialCapacity, loadFactor);
    this.accessOrder = accessOrder;
}
```

其put,remove方法如下,可以看到其基本由父类实现方法

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}
public V remove(Object key) {
    HashMap.Node<K,V> e;
    return (e = removeNode(hash(key), key, null, false, true)) == null ?
            null : e.value;
  // removeNode方法为HashMap里面的方法,其源码在HashMap里面,如上文提到
}
```

其自己实现的方法如下

```java
public V get(Object key) {
    Node<K, V> e;
    if ((e = getNode(hash(key), key)) == null)
        return null;
    // getNode找到节点之后通过判断标志位，来判断是否调用afterNodeAccess回调方法
    if (accessOrder)
        afterNodeAccess(e);
    return e.value;
}

public boolean containsValue(Object value) {
    for (LinkedHashMap.Entry<K, V> e = head; e != null; e = e.after) {
        V v = e.value;
        if (v == value || (value != null && value.equals(v)))
            return true;
    }
    return false;
}

public void clear() {
    super.clear();
    //自身维护的大链表头尾节点head、tail置空
    head = tail = null;
}
```

从containsValue中我们可以看到头指针在遍历,所以LinkedHashMap维护的链表结构不只是针对HashMap中的数组元素的,其针对的是所有加入HashMap中的元素,以他们的先后顺序维护起一个链表,这个方法就是通过实现HashMap中的一些空实现的函数接口实现的(例如afterNodeAccess)

这个做法让HashMap具有的链表的特性,使其可以通过链表的方法来进行索引,能够记录加入顺序.但因为其相当于多了额外的一些指针,所以所用的空间以及在执行这个链表的维护时候额外消耗的一些时间都要被考虑进去,如果是针对于一般场景使用HashMap足够了,如果需要快速索引,又需要额外的维护记录加入顺序(比如某些特殊的队列)就可以用该数据结构进行实现.

我们看下此链表的维护方法的实现

```java
private void linkNodeLast(LinkedHashMap.Entry<K, V> p) {
    LinkedHashMap.Entry<K, V> last = tail;
    tail = p;
    if (last == null)
        head = p;
    else {
        p.before = last;
        last.after = p;
    }
  // 很标准的双向链表的维护,没啥可说的
}
// putVal里面封装成节点的时候使用该方法
Node<K, V> newNode(int hash, K key, V value, Node<K, V> e) {
    LinkedHashMap.Entry<K, V> p =
            new LinkedHashMap.Entry<K, V>(hash, key, value, e);
    linkNodeLast(p);
    return p;
}
TreeNode<K, V> newTreeNode(int hash, K key, V value, Node<K, V> next) {
    TreeNode<K, V> p = new TreeNode<K, V>(hash, key, value, next);
    linkNodeLast(p);
    return p;
}

// 实现了父类的空实现,该方法在removeNode里面有被调用
void afterNodeRemoval(HashMap.Node<K, V> e) {
  LinkedHashMap.Entry<K, V> p = (LinkedHashMap.Entry<K, V>) e, 
  b = p.before, // 记录前驱
  a = p.after; // 记录后继
  p.before = p.after = null; // 删除该节点的前驱后继
  if (b == null)
    head = a; // 前驱为空则是头结点
  else
    b.after = a; // 否则删除前驱节点的后继指针
	
  // 同理如是维护该节点的后继节点即可
  if (a == null)
    tail = b;
  else
    a.before = b;
}
```

还有一方法比较重要单独拿出来说.afterNodeAccess,前文所述有两种顺序,一种是按照加入顺序进行访问,另一种是按照访问顺序进行访问,后者要表达的意思是,如果访问了某个节点,那么该节点的会被`提前`这个时候我们就可以解释afterNodeAccess方法是把最近访问的节点移到链表的尾部(因为在尾部的节点都是最新加入的节点)

```java
// 用于将被访问到的节点移动到大链表末尾
void afterNodeAccess(Node<K, V> e) { // move node to last
  LinkedHashMap.Entry<K, V> last;
  // 如果e不是尾节点，那么尝试移动e到尾部
  if (accessOrder && (last = tail) != e) {
    LinkedHashMap.Entry<K, V> p =(LinkedHashMap.Entry<K, V>) e, 
    b = p.before,
    a = p.after;
    // 移动到尾部的思路是删除节点的前后关系,然后在把尾节点.after=现在节点
    // 然后在更改尾部就可以了
    
    p.after = null;
    if (b == null) // p为头结点
      head = a;
    else
      b.after = a;
    
    if (a != null)
      a.before = b;
    else // p为尾结点
      last = b;
    
    
    if (last == null) // 当只有一个节点的时候会发生这个情况,即b=null且a=null
      head = p; //  至于尾部属于相同操作在后面
    else {// 就是正常节点,加入链表尾部
      p.before = last;
      last.after = p;
    }
    tail = p; 
    ++modCount; // 结构更改次数自增
  }
}
```



## 红黑树

-   未完待续

## LRU缓存的实现思路

其实看LinkedHashMap就给我们提供了一种实现,我们也知道作为一个双端链表如何实现,那就是如果访问元素的时候调用一个方法,把该元素放到链表/队列尾部.

而至于有限制大小的LRU缓存则更好实现,采取压栈的方式即可实现.不过考虑到数组的扩容问题,我们可以实现逻辑栈,即利用栈的指针,如果重复就冒泡上去可以解决问题,如果上面的操作经常遇到命中缓存的情况,我们可以按照循环队列的思路添加固定的元素,只要申请两倍于缓存大小的空间,便可通过循环队列来命中存在的元素,不过在命中过程中还是要进行搜索且空间复杂度更大.

如果是链表等链式结构要实现上面的栈,可以参考LinkedHashMap的思路,用LinkedList进行实现即可.相比较而言,双向链表具有更强大的性能,消耗的空间复杂度(指针数据的一倍).如果是链表很长,完全可以直接使用LinkedHashMap去实现.

## 关于数据结构的总结

从上面的种种数据结构中我们可以分为普通结构和索引结构,各种数据结构又可以互相融合即只要适当的增加指针域就能让数据结构具有其他数据结构的特性,上面的两种索引**红黑树**和**HashMap**更是可以极大的提高索引的效率.极大的优化了内存的访问,而其他的普通结构的数据结构解决的问题主要是扩容.总而言之,每个数据结构在不同的应用场景下有不同的性能合理的利用这些数据结构才能提高系统的整体性能

## JUC容器

JUC容器是一类特殊的数据结构,因为JUC就是为了解决线程安全而存在的,他们又基于AQS同步队列和基本的锁,这些工具共同作用着下面这些数据结构的形成,而下面这些数据结构也是为了解决线程安全而存在的,具有细粒度的锁和相对较高的性能,但无线程安全问题时,应当尽量使用上面的非线程安全的数据结构来解决问题,我们介绍三种最常见的线程安全集合.

我们在讲容器之前,看下锁基本代码是如何实现的,如果不知道其含义的可以参考另一文档

```java
static final Unsafe U; // init unsafe
static final long statusOffset = U.objectFieldOffset(xxx.class.getDeclaredField("sizeCtl"));
// 获得C++ cas操作的参数

static volatile int status = 0; // 自旋标志

for(;;){
  if(U.compareAndSwapInt(this,statusOffset,0,1)){
    try{
    	// 上锁之后的代码
    }finally{
      status = 0; // 释放锁
      // LockSupport.unpark(threadInstance); // 和下面对应重量级锁
      // FreeLock的情况下什么都不用加
    }
  }else{
    // Thread.yield(); // 如果前面操作时间够久的话可以重新插入队列或是挂起
    // LockSupport.park(); // 调度操作系统挂起,重量锁可不加就变成FreeLock
  }
}
```

### *CocurrentHashMap

ConcurrentHashMap,这是一个对HashMap进行改进的类.其不但实现了线程安全且其锁是分段锁即粒度比HashTable实现的好的多.同HashTable,ConcurrentHashMap也不允许null的key或者value.

该容器在JDK1.8之前采用分段锁技术.1.8之后采用synchronized和CAS技术,1.8的结构并发更高,由于有HashMap的基础,此处源码介绍将会以处理并发问题为关键.ConcurrentHashMap采用了和HashMap一样的数据结构,数组加链表加红黑树.

![](https://img-blog.csdnimg.cn/20200621201115134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc2NzAxNQ==,size_16,color_FFFFFF,t_70)

我们对此作出说明,上面的所有节点都是HashMap的节点,我们能够直接索引到的节点都在数组里面,数组的容量叫capacity,所有节点的个数叫size,负载因子(0.75)关注的是capacity而不是size,而转换成二叉树的大小是链表的长度(8)和capacity(64)共同决定的,以下再看这些数据结构.HashMap的数组节点我们又叫Hash桶.以下参考文章 [三万字的ConcurrentHashMap源码解读](https://blog.csdn.net/weixin_43767015/article/details/107318247)

ConcurrentHashMap的并发控制更加复杂.用了相当复杂的状态和控制结构.以下仅代表对ConcurrentHashMap一点粗浅的认知,只介绍主要的方法和思路.

```java
public class ConcurrentHashMap<K,V> extends AbstractMap<K,V>
    implements ConcurrentMap<K,V>, Serializable {

private static final int MAXIMUM_CAPACITY = 1 << 30;
private static final int DEFAULT_CAPACITY = 16;
// 并发级别,兼容了JDK1.7以前的版本
private static final int DEFAULT_CONCURRENCY_LEVEL = 16;
// 加载因子
private static final float LOAD_FACTOR = 0.75f;

  
static final int TREEIFY_THRESHOLD = 8;
static final int UNTREEIFY_THRESHOLD = 6;
static final int MIN_TREEIFY_CAPACITY = 64; // 允许树形化的最小大小
  

// 用在transfer方法中，transfer可以并发，每个线程所需要处理的连续的桶的个数，最少16。
private static final int MIN_TRANSFER_STRIDE = 16;
  
/**
 * 用于辅助生成扩容版本唯一标记，最小是6。这里是一个非final的变量，但是也没有提供修改的方法
 * 每次扩容都会有一个唯一的标记，一次扩容完毕之后，才会进行下一次扩容
 */
private static int RESIZE_STAMP_BITS = 16;
// 扩容最大线程数
private static final int MAX_RESIZERS = (1 << (32 - RESIZE_STAMP_BITS)) - 1;
/**
 * 扩容版本标记移位之后会保存到sizeCtl中当作扩容线程的基数，然后在反向移位可以获取到扩容版本标记
 */
private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;

  
// 特殊节点的hash值
// ForwardingNode的hash值,一种临时节点,用于扩容时辅助扩容,相当于标志节点
static final int MOVED = -1;

/* TreeBin结点的hash值，用于代理红黑树根节点，会存储数据
 * 红黑树添加删除节点时，树结构可能发生改变，因此额外维护了一个读写锁
 */
static final int TREEBIN = -2;
/**
 * ReservationNode的hash值，也相当于标志节点
 * 也是相当于占位符，在JDK1.8才出现的新属性，用于computeIfAbsent、compute方法
 */
static final int RESERVED = -3;

// CPU的可用数量 
static final int NCPU = Runtime.getRuntime().availableProcessors();

// 原始数组
transient volatile Node<K, V>[] table;

// 扩容的新数组
private transient volatile ConcurrentHashMap.Node<K, V>[] nextTable;

/**
 * JDK1.8的新属性
 * 控制标识符，用来控制table的出于初始化、扩容等操作，不同的值有不同的含义：
 * 当为正数时：表示初始化容量或者下一次进行扩容的阈值，即如果hash表的实际大小>=sizeCtl，则进行扩容，阈值是当前ConcurrentHashMap容量的0.75倍，不能改变
 
 * 当为0时：代表当时的table还没有被初始化
 * 当为负数时：
 * -1代表线程正在初始化哈希表；
 * 其他负数，表示正在进行扩容操作，RESIZE_STAMP_SHIFT = 16
 	sizeCtl = (rs << RESIZE_STAMP_SHIFT) + n + 1，(计算扩容)
 	即此时的sizeCtl由版本号rs左移16位+并发扩容的线程数n+1组成
 */
// 该字段非常重要,存储了几乎所有的大小信息和控制信息务必注意
private transient volatile int sizeCtl;


// CAS标志位
private transient volatile int cellsBusy;

// 元素个数的基本计数器
private transient volatile long baseCount;

// 优化统计数量的结构,参考LongAddr的结构,分散资源量
private transient volatile CounterCell[] counterCells;

// 下次transfer扩容分配的指针
private transient volatile int transferIndex;
}
```

同时我们看其使用的Unsafe数据结构,还有一些同步

```java
// Unsafe mechanics
private static final sun.misc.Unsafe U;
private static final long SIZECTL;
private static final long TRANSFERINDEX;
private static final long BASECOUNT;
private static final long CELLSBUSY;
private static final long CELLVALUE;
private static final long ABASE; // 数组Node元素大小
private static final int ASHIFT; // 记录数组大小元素的左移位数

static {
  try {
    U = sun.misc.Unsafe.getUnsafe();
    Class<?> k = ConcurrentHashMap.class;
    SIZECTL = U.objectFieldOffset
      (k.getDeclaredField("sizeCtl"));
    TRANSFERINDEX = U.objectFieldOffset
      (k.getDeclaredField("transferIndex"));
    BASECOUNT = U.objectFieldOffset
      (k.getDeclaredField("baseCount"));
    CELLSBUSY = U.objectFieldOffset
      (k.getDeclaredField("cellsBusy"));
    Class<?> ck = CounterCell.class;
    CELLVALUE = U.objectFieldOffset
      (ck.getDeclaredField("value"));
    
    Class<?> ak = Node[].class;
    ABASE = U.arrayBaseOffset(ak);
    int scale = U.arrayIndexScale(ak); // 获取数组长度
    if ((scale & (scale - 1)) != 0)
      throw new Error("data type scale not a power of two");
    ASHIFT = 31 - Integer.numberOfLeadingZeros(scale); 
  } catch (Exception e) {
    throw new Error(e);
  }
}
```

可以看到其数据结构是非常复杂的.尤其是相对HashMap来说,其并发的控制部分和控制本身都极其复杂.我们看下其主要内部类,有一点要注意的这细节内部节点自己实现的find方法是用于后面的get方法上的,这里得注意

```java
static class Node<K, V> implements Map.Entry<K, V> {
  final int hash;
  final K key;
  // val和next都会在扩容时发生变化，所以加上volatile来保持可见性和禁止重排序
  // 以及在多线程环境下线程A修改结点的val或者新增结点的时候是对线程B可见的。
  volatile V val;
  volatile Node<K, V> next;

  public final K getKey()       { return key; }
  public final V getValue()     { return val; }
  public final int hashCode() { return key.hashCode() ^ val.hashCode(); }
  public final String toString(){ return key + "=" + val; }
  public final V setValue(V value)
  {throw new UnsupportedOperationException();}
  // 不允许更新value,不允许直接改变value的值

  // 辅助map.get 顺序寻找对应hash值的链表节点
  Node<K, V> find(int h, Object k) {
    Node<K, V> e = this;
    if (k != null) {
      do {
        K ek;
        if (e.hash == h &&
            ((ek = e.key) == k || (ek != null && k.equals(ek))))
          return e;
      } while ((e = e.next) != null);
    }
    return null;
  }
}
```

其红黑树节点,其并不直接参与对红黑树的节点,由TreeBin代理完成,TreeBin的继承关系和TreeNode一致.JUC中没有基于LinkedHashMap的并发工具.TreeBin是在数组上的哨兵节点.红黑树自身的修改还使用了读写锁来进行制约.

```java
static final class TreeNode<K,V> extends Node<K,V> {
    TreeNode<K, V> parent;  // red-black tree links
    TreeNode<K, V> left;
    TreeNode<K, V> right;
    // 删除节点时使用到的辅助节点，指向原链表的前一个节点
    TreeNode<K, V> prev;
    boolean red;
  	
  	// ... 红黑树方法
  	Node<K, V> find(int h, Object k) {...}
}

static final class TreeBin<K,V> extends Node<K,V> {
    // 红黑树的真正根节点
    TreeNode<K, V> root;
    // 链表头节点，红黑树实际上还维护了一个双端链表
    volatile TreeNode<K, V> first;
    volatile Thread waiter;
    
  
  	volatile int lockState;
    // 锁状态单位
    // 写锁，写锁是独占锁 二进制是 001
    static final int WRITER = 1; // set while holding write lock
    // 等待获取写锁  二进制是 010
    static final int WAITER = 2; // set when waiting for write lock
    // 读锁单位，读锁了是共享锁  二进制是 100
    static final int READER = 4; // increment value for setting read lock
  	// 关于二叉树的并发控制我们不再赘述
}
```

临时扩容节点,当需要扩容时出现的临时节点,会存储下个Hash表的引用,用于存放旧数组的Hash桶位.旧数组的桶位上会存放此节点.其find方法可以看下其实就是找到新的数组去读!forwardingNode意为转发的节点,其目的就是在数组扩容时找到扩容的数组去读.而旧数组的相应位置会放上一个forwardingnode节点.可以看下面的find方法指导如何在刚刚new好的数组还没赋值的时候读到节点

`当读操作遇到ForwardingNode时，会被转发到新数组的上去继续读；而当写操作遇到ForwardingNode时，表示正在扩容，那么写线程加入到扩容操作中去，提升扩容效率`

```java
static final class ForwardingNode<K,V> extends Node<K,V> {
  final Node<K,V>[] nextTable; // 新数组的引用,用于存放扩容好的数组
  ForwardingNode(Node<K,V>[] tab) {
    super(MOVED, null, null, null);
    this.nextTable = tab;
  }
	
  // 查找操作直接转移到新数组 h是hash值,k是key本身
  Node<K,V> find(int h, Object k) {
    // loop to avoid arbitrarily deep recursion on forwarding nodes
    outer: 
    for (Node<K,V>[] tab = nextTable;;) {
      Node<K,V> e; // e是存放在对应位置的元素
      int n; // n是存放table的长度
      // 下面的h代表hash值 n-1 & h 表示取hash值的前几位(二进制)和HashMap一样
      // 试着找到hash桶的位置,如果找不到就返回空
      if (k == null || tab == null || (n = tab.length) == 0 ||
          (e = tabAt(tab, (n - 1) & h)) == null)
        return null;
      
      for (;;) {
        int eh; K ek;
        if ((eh = e.hash) == h &&
            ((ek = e.key) == k || (ek != null && k.equals(ek))))
          return e; // 如果只有一个元素就那个
        
        if (eh < 0) { // 如果其hash值小于0,意味着这节点是红黑树节点或者扩容
          
          if (e instanceof ForwardingNode) { // 另一个转发节点就读在转发就是
            tab = ((ForwardingNode<K,V>)e).nextTable; // 往下递归找下张表
            continue outer;
          }
          else
            return e.find(h, k); // 直接调用再去找,等其他线程操作结束
        }
        if ((e = e.next) == null) // 递增往下继续循环
          return null;
      }
    }
  }
}
```

我们看其主要方法,在JDK1.8中loadFactor,concurrencyLevel实际上并没有啥作用,仅用于计算初始容量,为了兼容旧版本(旧版本中可能有用到这个字段的).

我们看其主要构造器

```java
public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) {
    if (!(loadFactor>0.0f) || initialCapacity<0 || concurrencyLevel<=0)
        throw new IllegalArgumentException();
    if (initialCapacity < concurrencyLevel)   // Use at least as many bins
        initialCapacity = concurrencyLevel;   // as estimated threads
  	// 给初始容量赋值,如果初始容量在最小并发级别的话,这是1.7之前的分段锁的最小并发级别
  
    // 比如initialCapacity设为16,loadFactor设为0.75,那么计算的size为22.3,那么tableSizeFor((int)size),会得到32,当然
  	// size 表示下一次扩容的阈值,计算下一个size
    long size = (long) (1.0 + (long) initialCapacity / loadFactor);
    int cap = (size >= (long) MAXIMUM_CAPACITY) ?
            MAXIMUM_CAPACITY : tableSizeFor((int) size); 
  					// tableSizeFor和hashmap方法一致
    // 初始容量赋值给sizeCtl变量,此时其表示的是下次扩容的阈值
    this.sizeCtl = cap;
}
```

我们看其主要方法

```java
public V put(K key, V value) {
    return putVal(key, value, false);
}

static final int spread(int h) {
  return (h ^ (h >>> 16)) & HASH_BITS; // HASH_BITS = 0x7fffffff
  // 这是一个扰动算法,减少hash冲突,最终会让hash变成正数
}

final V putVal(K key, V value, boolean onlyIfAbsent) {
  if (key == null || value == null) throw new NullPointerException();
  
  int hash = spread(key.hashCode());
  int binCount = 0;
  // 用于记录在什么位置操作了,0表示在一个空位置插入新节点,1表示链表操作,2表示红黑树
  
  for (Node<K,V>[] tab = table;;) {
    Node<K,V> f; // f是table中的临时节点
    int n, i, fh; // n代表table长度,i是hash值的前i位,fh是f节点的hash值
    if (tab == null || (n = tab.length) == 0) // 如果没初始化
      tab = initTable(); // 初始化table 
    
    else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
      // 如果找到空位置,就用CAS放上去
      if (casTabAt(tab, i, null,new Node<K,V>(hash, key, value, null)))
        break; // no lock when adding to empty bin
    
    }else if ((fh = f.hash) == MOVED) // 如果是ForwardingNode,证明在扩容
      tab = helpTransfer(tab, f);  // 帮助扩容,然后等其完毕的时候在修改(for)
    
    else {// 链表和红黑树
      V oldVal = null;
      synchronized (f) { // 对一个节点进行操作
        if (tabAt(tab, i) == f) { // 如果那个节点在
          if (fh >= 0) { // 是链表
            binCount = 1;
            for (Node<K,V> e = f;; ++binCount) {
              K ek; // 存储该节点的key值
              
              // 如果找到了节点
              if (e.hash == hash &&
                  ((ek = e.key) == key || 
                   (ek != null && key.equals(ek)))) {
                oldVal = e.val;
                if (!onlyIfAbsent) // 当该key存在的时候是否赋值
                  // 和hashmap一样putIfAbsent不会给有key的值二次赋值
                  e.val = value;
                break; // 这个是打破里面的for
              }
              
              // 如果e的后继节点没有找到相应的值那么就插到链表尾部
              Node<K,V> pred = e;
              if ((e = e.next) == null) {
                pred.next = new Node<K,V>(hash, key,value, null);
                break; // 因为没有结束条件,所以跳出循环
              }
              
            } // for
          }
          else if (f instanceof TreeBin) { // 如果是个树节点
            Node<K,V> p;
            binCount = 2; // 记录树节点操作
            // 调用putTreeValue的方法放入节点
            if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key, value)) != null) {
              oldVal = p.val;
              if (!onlyIfAbsent)
                p.val = value;
            }
          }
        }
      } // synchronized
      
      if (binCount != 0) { // 如果在红黑树或链表插入新节点的话
        if (binCount >= TREEIFY_THRESHOLD) 
          // binCount在链表赋值了大于8就红黑树
          treeifyBin(tab, i); // 看看要不要树形化
        
        if (oldVal != null)
          return oldVal; // 如果不是空的就直接返回结束循环,如果是正在扩容就等
        break;
      }
    } // else
  } // for
  addCount(1L, binCount); // 更新统计数量有可能发生扩容
  return null;
}
```

关于数组分配内存的一些方法.initTable,treeifyBin和transfer相关,

```java
private final Node<K,V>[] initTable() {
  Node<K,V>[] tab; // 存放底层数组
  int sc; // 存放sizeCtl
  while ((tab = table) == null || tab.length == 0) { 
    // 当线程没有初始化的时候就一直循环,任意一线程初始化了就能够自动跳出
    
    if ((sc = sizeCtl) < 0) // 失去了初始化的机会就自旋,初始化完成会跳出
      Thread.yield(); // lost initialization race; just spin
    	// 这里用yield是因为线程切换的时间比init table时间要短,为了防止CPU
    	// 空等用了这个,让其在放弃时间片给初始化的线程时间片 [多线程CAS的理解]
    
    else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { // U = unsafe
      // 这里cas交换SIZECTL的值,如果没交换就继续交换成了就成-1,
      // 那么其他线程走的不是空转而是Thread.yield()加入cu就绪队列等待
      // 这个小设计是提高性能的一个小办法不用CPU在空转
      
      try {
        if ((tab = table) == null || tab.length == 0) {
          int n = (sc > 0) ? sc : DEFAULT_CAPACITY; // 初始化容量
          
          @SuppressWarnings("unchecked")
          Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
          table = tab = nt; // 新数组赋值,然后退出循环
          sc = n - (n >>> 2); // 阈值计算 n = 0.75*n 所以负载因子没用
        }
      } finally {
        sizeCtl = sc; // 更新下一次阈值
      }
      
      break; // 交换成功了就break,不用走其他判断
    }// while
  }
  return tab;
}
```

我们看下树形化的TreeifyBin,

```java
private final void treeifyBin(Node<K,V>[] tab, int index) {
  Node<K,V> b; int n, sc;
  if (tab != null) {
    if ((n = tab.length) < MIN_TREEIFY_CAPACITY)
      tryPresize(n << 1); // 如果链表够了但是大小不到就得对数组进行一次扩容
    
    else if ((b = tabAt(tab, index)) != null && b.hash >= 0) {
      // 这里加了一把大锁,对树形化的进行相应的处理,树的部分依然不做重点
      synchronized (b) {
        if (tabAt(tab, index) == b) {
          TreeNode<K,V> hd = null, tl = null;
          for (Node<K,V> e = b; e != null; e = e.next) {
            TreeNode<K,V> p =
              new TreeNode<K,V>(e.hash, e.key, e.val,null, null);
            if ((p.prev = tl) == null)
              hd = p;
            else
              tl.next = p;
            tl = p;
          }
          setTabAt(tab, index, new TreeBin<K,V>(hd));
        }
      }
    }
  }
}
```

我们看其扩容方法`tryPresize`

```java
private final void tryPresize(int size) {
  // 参数在putAll里面传入的是集合的大小
  // 参数在树形化里面传入的是 table.length << 1,即底层数组的长度的两倍
  int c = (size >= (MAXIMUM_CAPACITY >>> 1)) ? MAXIMUM_CAPACITY :
  tableSizeFor(size + (size >>> 1) + 1); // 扩容至2的倍数
  // size + size >>> 1 = size * 1.5
  // 如果传入的容量>最大容量的一半就扩容到最大容量
  // 不然就是扩容两倍
  
  int sc; // 用于存放sizeCtl
  while ((sc = sizeCtl) >= 0) { // 没有初始化,或者初始化了之后没分配好内存
    Node<K,V>[] tab = table; 
    int n; // n存放table长度
    
    // 如果没有分配好内存,那么和上面内存的分配逻辑是一样的
    if (tab == null || (n = tab.length) == 0) {
      n = (sc > c) ? sc : c;
      if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { // CAS换
        try {
          if (table == tab) {
            @SuppressWarnings("unchecked")
            Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
            table = nt;
            sc = n - (n >>> 2); // 阈值计算 n = 0.75*n 所以负载因子没用
          }
        } finally {
          sizeCtl = sc;
        }
      }
    }
    // 否则已经初始化了,sc是扩容阈值,c是真正需要的容量,大部分情况下应该是一样的
    // 其不等号发生的原因可能是sc并没有被附加值,也证明其未扩容
    else if (c <= sc || n >= MAXIMUM_CAPACITY)
      break;
    
    else if (tab == table) { // 如果其他没有赋值的化
      
      int rs = resizeStamp(n); // 算出resize的位数,
      if (sc < 0) { // 其他线程正在扩容
        Node<K,V>[] nt;
        /**
        1 (sc >>> RESIZE_STAMP_SHIFT) != rs ：扩容线程数 > MAX_RESIZERS-1
        2 sc == rs + 1 和 sc == rs + MAX_RESIZERS ：如果扩容线程
        3 (nt = nextTable) == null ：表示nextTable正在初始化
        4 transferIndex <= 0 ：表示所有hash桶均分配出去*/
        if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
            sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
            transferIndex <= 0) 
          break;
        
        // CAS交换sz,成功了就开始用transfer扩容
        if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
          transfer(tab, nt);
        
        // 下面这个是第一个扩容线程,把sc设置为rs << RESIZE_STAMP_SHIFT) + 2
      	}else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs << RESIZE_STAMP_SHIFT) + 2))
        transfer(tab, null); // 然后扩容
    }
  }
}
```

transfer是真正的扩容,我们来看看addCount如何调用此函数,其有一设计思路是热点数据分离更新,和LongAddr思路一致

![](https://img-blog.csdnimg.cn/img_convert/5f2a71bb1d91988b429d1ed830c094ba.png)

简单说下上面是如何实现的,当发生竞争的时候(CAS失败了),那么就创建Cell数组通过Thread的hashCode & cell.length的方式给数组里面的元素CAS,这样就变相增加了资源量,也减少了并发的争抢,之后再把数组里面的元素进行求和就行了.

```java
// hash表之后的节点统计,同时检查是否要扩容,因为可能是多个线程调用的所以先划分到栈里面在求和,详细的思路可以参考代码或者LongAddr
private final void addCount(long x, int check) {
  CounterCell[] as; // CounterCell是一种热点数据分离的变量如上图.
  long b, s; // b是baseCount,s是sum,临时存放b+x的和
  if ((as = counterCells) != null ||
      !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) {
    // 这里进行了累加,查看有没有竞争,如果有竞争
    
    // 实现了上面类似LongAddr的设计思路,分散资源量到cell数组
    CounterCell a; long v; int m;
    boolean uncontended = true;
    if (as == null || (m = as.length - 1) < 0 ||
        (a = as[ThreadLocalRandom.getProbe() & m]) == null ||
        !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) {
      // 用于记录个数,如果cell长度不够还要对其进行扩容
      fullAddCount(x, uncontended);
      return;
    }
    // check <= 1说明更新成功了不需要扩容,有前面方法中的binCount决定
    // 这个在链表中就代表链表长度,0表示往空位置上填上节点,1表示往链表上append
    // 如果是2可能是二叉树的节点操作也可能是链表长度大于2 ++binCount
    if (check <= 1)
      return;
    s = sumCount(); // 计算整个数组元素数量的和
  }
  if (check >= 0) { // >=0表示不是删除操作需要注意扩容
    Node<K,V>[] tab, nt;  // table,nextTable
    int n, sc; // n 存table长度,sc存sizeCtl
    while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
           (n = tab.length) < MAXIMUM_CAPACITY) {
      
      int rs = resizeStamp(n);
      if (sc < 0) {
//1 如果 sc无符号右移RESIZE_STAMP_SHIFT(16)位不等于此次扩容版本的唯一标志rs，如果是在同一个版本的扩容过程中应该是相等的，如果不等那说明不是同一个版本，不能协助
//2 否则 如果sizeCtl等于此次扩容版本的唯一标志rs+1，表示扩容结束了，不需要协助，因为第一次调用扩容方法之前sizeCtl == rs + 2
//3 否则 如果sizeCtl等于此次扩容版本的唯一标志rs+MAX_RESIZERS，表示协助扩容的线程数量达到了最大值MAX_RESIZERS，不需要协助
//4 否则 如果记录的新数组等于null，表示扩容结束了，不需要协助
//5 否则 如果transferIndex <= 0，表示数组桶位分配完了，不需要协助
// 以上五种情况成立一种，就表示不需要协助，那么结束循环，addCount方法结束
// 第2、3个条件，实际上是一个JDK1.8的BUG，因为我们会发现，在我们的JDK版本中这两个判断永远都会返回false，正确的判断应该是： sc == (rs<<16) + 1 和 sc == (rs<<16) + MAX_RESIZERS 或者 (sc >>> RESIZE_STAMP_SHIFT) == rs + 1 ||  (sc >>> RESIZE_STAMP_SHIFT) == rs + MAX_RESIZERS
// 另外，在helpTransfer方法中也存在类似的BUG，BUG地址：https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8214427，该BUG已在高版本的JDK中修复
        if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
            sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
            transferIndex <= 0)
          break;
        if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) 
          // 多线程情况帮助扩容
          transfer(tab, nt);
      }
      else if (U.compareAndSwapInt(this, SIZECTL, sc,(rs << RESIZE_STAMP_SHIFT) + 2))
        // 第一个扩容的线程记录版本号.
        transfer(tab, null);
      s = sumCount();
    }
  }
}
```

我们来看其transfer方法,即扩容或帮助扩容.其为多线程协作的重点.

ConcurrentHashMap利用了两个指针,一个指针指向原来的table,另一个指针指向nextTable,nextTable是存放扩容后的数组,然后这个数组就会进行节点的复制转移等,然后如果此时有读写的话写时通过等待扩容完成之后才行的,所以此时线程会帮助扩容即调用helpTransfer方法本质上也是调用transfer方法,当扩容完成时,table = nextTable, 然后nextTable = null,然后其他写操作就可以进行了,如果这个时候发生了读操作,那么读是可以完全并发的转发到nextTable上去读的,由ForwardingNode里面维护的nextTable指针以及find方法区get到相应key值的节点.

我们看过ForwardingNode的代码之后可能会发现一个问题,如果下面赋值的不够快就有可能读不到,其损失了数据的即时可读却提高了并发能力.但是一般情况,这种扩容的速度很快并不需要担心,且金有可能在扩容的时候发生数据丢失.

从这个处理方式我们也能看出来ConcurrentHashMap是读写并发支持的,读是永远支持的.

```java
private final void transfer(Node<K,V>[] tab, Node<K,V>[] nextTab) {
  int n = tab.length, stride; // stride步长,这里表示index走到了哪的指针
  
  // 计算每个CPU线程需要处理的桶数量 (n >>> 3) / NCPU 尽量让每线程分配到均匀的任务
  // 最小为16,也就是每个核心至少要处理16和hash桶
  if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)
    stride = MIN_TRANSFER_STRIDE; // subdivide range
  
  if (nextTab == null) {  // 即第一个线程进入 // initiating
    // 因为不是第一个线程进入就是扩容了
    try {
      @SuppressWarnings("unchecked")
      Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1]; // 翻倍扩容
      nextTab = nt; // 更新栈中的值
    } catch (Throwable ex) {      // try to cope with OOME
      sizeCtl = Integer.MAX_VALUE; // 扩容失败的标记
      return;
    }
    nextTable = nextTab; // 更新类中的值
    transferIndex = n; // 下一个线程要分配的从哪开始的指针
  }
  
  // 执行到这说明是其他帮助扩容的线程,或者是已经初始化好的第一个线程
  int nextn = nextTab.length; // 该从哪个位置开始扩容
  
  // 新建扩容节点封装nextTable,nextTable就是我们翻倍扩容出来的数组
  ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);
  boolean advance = true; // 是否完成扩容状态,即推进标志
  boolean finishing = false; // to ensure sweep before committing nextTab
  // 清理标志
  for (int i = 0, bound = 0;;) {
    Node<K,V> f; 
   	int fh;
    
    // 尝试为每个线程分配任务
    while (advance) { // 刚进来默认为true
      int nextIndex, nextBound;
      // nextIndex 用来记录transferIndex;
      if (--i >= bound || finishing)  
        // 第一次这里不会执行,最终也不会修改finishing?用于否面判断
        // 要通过外面的CAS的for去进行判断才会修改,表示是否扩容完毕
        advance = false;
      
      else if ((nextIndex = transferIndex) <= 0) {
        // 表示桶位被分配完毕了,就不要推进了
        i = -1; 
        advance = false; // 标志无法被推进,准备跳出循环
      }
      // 把transferIndex字段从nextIndex CAS 修改成nextBound,根据stride决定
      // nextBound = nextIndex > stride ? nextIndex - stride : 0
      // 上面这行代码,如果nextIndex > stride,那么就还有nextIndex - stride个可以
      // 被线程初始化,否则就不能初始化即0
      // 这里算出了nextBound的值然后尝试修改transferIndex,修改成功就可以扩容
      else if (U.compareAndSwapInt(this, TRANSFERINDEX, nextIndex,
                nextBound = (nextIndex > stride ?
                             nextIndex - stride : 0))) {
        bound = nextBound; 
        i = nextIndex - 1;
        advance = false;
        // 更新bound值,标记一下,然后退出循环,要不就一直cas修改至
      }
    }// while
    
    // i<0 分配结束了或者扩容完毕
    // i>n 扩容伦次不一样
    // n>=nextn 说明轮次可能也不一致
    if (i < 0 || i >= n || i + n >= nextn) {
      
      int sc;
      if (finishing) { // 扩容完成了没
        nextTable = null;
        table = nextTab; // 这里更新类的table,并且把原来的table置空
        sizeCtl = (n << 1) - (n >>> 1); 
        // 修改控制字段,这个相当于阈值现在的0.75倍
        return;
      }
      // 让后尝试退出扩容
      if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {
        if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)
          return; // 当前线程不是本轮扩容的最后一个线程退出
        finishing = advance = true; // 最后一个线程进行后续处理
        i = n; // recheck before commit
      }
    }
    else if ((f = tabAt(tab, i)) == null)
      advance = casTabAt(tab, i, null, fwd); // 把ForwardingNode CAS上去
    else if ((fh = f.hash) == MOVED) // 如果该节点是ForwardingNode状态
      // 那就证明已经处理过这个节点了直接跳过
      advance = true; // already processed
    else {
      // 否则该节点就是红黑树或者链表了,就得迁移其他数据节点
      // synchronized加锁粒度大,防止激烈竞争资源
      synchronized (f) {
        if (tabAt(tab, i) == f) {
          Node<K,V> ln, hn;
          if (fh >= 0) { // 如果是链表
            int runBit = fh & n; // 计算新hash值,比HashMap1.8要复杂很多
            // 参考1.7以前hashmap的设计,此段源码就是不断计算新的hahs值
            // 和resize中新节点位于k和k+oldCap索引位置处
            Node<K,V> lastRun = f;
            for (Node<K,V> p = f.next; p != null; p = p.next) {
              int b = p.hash & n; // 取p的前n位
              if (b != runBit) { // 根据其hashcode特点分类到两堆链表中
                runBit = b;
                lastRun = p;
              }
            }
            if (runBit == 0) {
              ln = lastRun;
              hn = null;
            }
            else {
              hn = lastRun;
              ln = null;
            }
            for (Node<K,V> p = f; p != lastRun; p = p.next) {
              int ph = p.hash; K pk = p.key; V pv = p.val;
              if ((ph & n) == 0)
                ln = new Node<K,V>(ph, pk, pv, ln);
              else
                hn = new Node<K,V>(ph, pk, pv, hn);
            }
            setTabAt(nextTab, i, ln);
            setTabAt(nextTab, i + n, hn); // 根据HashCode在对应桶位置填入链表
            setTabAt(tab, i, fwd); //在对应位置上放好我们初始化好的节点
            advance = true; // 扩容完成
          }
          else if (f instanceof TreeBin) {
            // 下面就按照树的方法区扩容了,具体内容不做细致分析
            TreeBin<K,V> t = (TreeBin<K,V>)f;
            TreeNode<K,V> lo = null, loTail = null;
            TreeNode<K,V> hi = null, hiTail = null;
            int lc = 0, hc = 0;
            for (Node<K,V> e = t.first; e != null; e = e.next) {
              int h = e.hash;
              TreeNode<K,V> p = new TreeNode<K,V>
                (h, e.key, e.val, null, null);
              if ((h & n) == 0) {
                if ((p.prev = loTail) == null)
                  lo = p;
                else
                  loTail.next = p;
                loTail = p;
                ++lc;
              }
              else {
                if ((p.prev = hiTail) == null)
                  hi = p;
                else
                  hiTail.next = p;
                hiTail = p;
                ++hc;
              }
            }
            // 判断树是否需要还原
            ln = (lc <= UNTREEIFY_THRESHOLD) ? untreeify(lo) :
            (hc != 0) ? new TreeBin<K,V>(lo) : t;
            hn = (hc <= UNTREEIFY_THRESHOLD) ? untreeify(hi) :
            (lc != 0) ? new TreeBin<K,V>(hi) : t;
            setTabAt(nextTab, i, ln);
            setTabAt(nextTab, i + n, hn);
            setTabAt(tab, i, fwd);
            advance = true;
          }
        }
      }
    }
  }
}
```

我们从上面其实就能够看到多个线程如何协作完成这么一个工作了,从后面的读取ForwardingNode的过程,其思路也比较简单,因为并发的关系可能造成若干线程同时来扩容的情况,那么就弄个节点,节点指向新的table节点,然后通过新的ForwardingNode访问新的table节点.

我们可以看到读操作通过nextTable转发到了新数组中继续读.下面看其他线程是如何辅助扩容的.

```java
final Node<K,V>[] helpTransfer(Node<K,V>[] tab, Node<K,V> f) {
  Node<K,V>[] nextTab; // 记录转移节点的nextTable;
  int sc; // sizeCtl
  if (tab != null && (f instanceof ForwardingNode) &&
      (nextTab = ((ForwardingNode<K,V>)f).nextTable) != null) {
    // 如果已经放上了Forwading节点且nextTable不为空才可以帮忙
    
    int rs = resizeStamp(tab.length);
    while (nextTab == nextTable && table == tab &&
           (sc = sizeCtl) < 0) {
      // 这5种条件我们之前已经判断过了,表示不需要协助,直接跳出循环
      if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
          sc == rs + MAX_RESIZERS || transferIndex <= 0)
        break;
      if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) {
        // 尝试CAS修改其线程数量然后该线程调用transfer帮助扩容
        transfer(tab, nextTab);
        break;
      }
    }
    return nextTab;
  }
  return table; // 扩容结束了,table就是最新的数组
}
```

至此我们终于结束了其增加一个新节点包括扩容等情况的所有处理,能够看到ConcurrentHashMap里面还有很多线程安全性质的设计,比如LongAddr的分散资源处理,比如各种CAS,比如充斥的FreeLock的设计思维,以及多个线程帮助扩容时候借助的ForwardingNode节点.nextTable等.

我们来看其remove方法

```java
public V remove(Object key) {
    return replaceNode(key, null, null);
}
public boolean remove(Object key, Object value) {
    if (key == null)
        throw new NullPointerException();
    return value != null && replaceNode(key, null, value) != null;
}
public V replace(K key, V value) {
    if (key == null || value == null)
        throw new NullPointerException();
    return replaceNode(key, value, null);
}
```

可以看到其都用了`replaceNode`这一方法.

```java
final V replaceNode(Object key, V value, Object cv) {
  // cv是个删除控制字段,看上面,如果replace就为null,如果是remove就是value
  
  int hash = spread(key.hashCode()); // 扰动算法算hash
  for (Node<K,V>[] tab = table;;) {
    Node<K,V> f; // f用来存放临时节点
    int n, i, fh; // n是链表长度,i是算出的index,fh是节点的hashcode
    
    // 如果hash表为空,或者长度为0或者没有节点,那就不用replace了直接出循环 
    if (tab == null || (n = tab.length) == 0 ||
        (f = tabAt(tab, i = (n - 1) & hash)) == null)
      break;
    else if ((fh = f.hash) == MOVED) // 如果该节点在扩容就帮助其扩容
      tab = helpTransfer(tab, f);
    else { // 否则的话我们要开始替换值了
      V oldVal = null;
      boolean validated = false; // 作为一个是否可以退出循环的标志
      synchronized (f) {
        if (tabAt(tab, i) == f) {
          if (fh >= 0) { // 如果是链表的话
            validated = true; 
            for (Node<K,V> e = f, pred = null;;) {
              K ek; // 节点的key
              if (e.hash == hash &&
                  ((ek = e.key) == key ||
                   (ek != null && key.equals(ek)))) {
                V ev = e.val; // 如果找到了这么个节点获取value值
                
                // 如果传入了value 或者传入的value匹配成功
                // cv是删除用的字段,也可能为null,这里的前提是匹配到了节点
                if (cv == null || cv == ev ||
                    (ev != null && cv.equals(ev))) {
                  oldVal = ev; // 记录old的value
                  if (value != null)
                    e.val = value; // 如果有value的话就替换值然后就可以结束了
                  else if (pred != null)
                    pred.next = e.next; // 否则就是链表的删除,pred表示上的节点
                  else // pred == null,表明是头结点
                    setTabAt(tab, i, e.next); // 重新设值头结点
                }
                break;
              }// 如果匹配不到节点,那就下一个e = e.next
              pred = e; // 匹配到的上一个节点的引用
              if ((e = e.next) == null) // 都匹配不到就跳出循环了
                break;
            }
          }
          else if (f instanceof TreeBin) { // 红黑树
            validated = true;
            TreeBin<K,V> t = (TreeBin<K,V>)f;
            TreeNode<K,V> r, p;
            if ((r = t.root) != null &&
                (p = r.findTreeNode(hash, key, null)) != null) {
              // 找到红黑树的节点
              V pv = p.val;
              if (cv == null || cv == pv ||
                  (pv != null && cv.equals(pv))) {
                oldVal = pv;
                if (value != null) // 修改值
                  p.val = value; 
                else if (t.removeTreeNode(p)) // 或者是直接删除,还要考虑退化
                  setTabAt(tab, i, untreeify(t.first));
              }
            }
          }
        }
      }
      if (validated) { // 收尾操作如果修改过及诶点的值
        if (oldVal != null) {
          if (value == null)
            addCount(-1L, -1); // 节点数量减少且不需要扩容
          return oldVal; // 返回oldvalue
        }
        break;
      }
    }
  }
  // break掉循环就是没有找到返回null
  return null;
}
```

我们来看其get方法,通过上面的操作和下面的get我们会总结出来ConcurrentHashMap是读写并发的数据结构(红黑树有不是)

```java
public V get(Object key) {
  Node<K,V>[] tab; // 存储table的引用
  Node<K,V> e, p; // e临时存放table元素
  int n, eh; //eh代表e的hashcode
  K ek; // ek代表节点的key
  int h = spread(key.hashCode()); // 扰动算法计算hash值
  
  if ((tab = table) != null && (n = tab.length) > 0 &&
      (e = tabAt(tab, (n - 1) & h)) != null) {
    // 如果对应位置上有节点的话
    if ((eh = e.hash) == h) { // 找到了对应的key的hash桶位
      if ((ek = e.key) == key || (ek != null && key.equals(ek)))
        // 判断key值相等于否为我们要找的节点,如果是就直接返回
        // 如果不是就执行下面的while循环
        return e.val;
    }else if (eh < 0) // 如果该位置是红黑树或者在扩容
      return (p = e.find(h, key)) != null ? p.val : null;
    	// 利用各种数据结构自己实现的find方法,如果找的到节点就返回不然返回null
    
    // 不是第一个节点的情况,那么就while循环往下找,如果找到同样返回
    while ((e = e.next) != null) {
      if (e.hash == h &&
          ((ek = e.key) == key || (ek != null && key.equals(ek))))
        return e.val;
    }
  }
  // 如果while循环没找到,那么最终也返回null
  return null;
}
```

至此ConcurrentHashMap的主要方法已经全部结束.我们没有介绍红黑树的部分实际上红黑树还有个读写锁,但是其扩容机制的复杂程度和线程处理的复杂难度告诉我们最好提前设计好HashMap的容量大小,避免过多的扩容才是正经的解决方法.

对于其并发特点的总结

-   ConcurrentHashMap是读写并发支持的,读是永远支持的,写对于桶位进行加锁synchronized
-   其写还要受到transfer扩容的制约,所以其写的操作比读性能要求更大
-   ConcurrentHashMap通过两个指针以及ForwardingNode的方法

JDK1.7和1.8的区别如下,JDK1.7的源码设计思路主要是分段锁,有机会考虑补上

| 区别     | JDK1.7                                                       | JDK1.8                                                       |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据结构 | 由一个Segment数组以及每个索引位置上包含的一个HashEntry数组构成，HashEntry数组的桶位存放的是一个的HashEntry链表，HashEntry数组和JDK1.7的HashMap结构差不多。 | 由一个Node数组组成，Node数组的桶位存放的可能是一张Node链表，或者一颗Node红黑树。整体结构和JDK1.8的HashMap结构非常相似，还是一整张的哈希表，并且相关参数和HashMap差不多，不如链表树形化阈值为大于8等等。而Node类似于JDK1.7的ConcurrentHashMap中的HashEntry。    虽然还能看到Segment的数据结构，但是已经没有实际意义，只是为了兼容旧版本，并不参与任何结点的操作！构造器中指定的loadFactor以及concurrencyLevel参数同样只是为了兼容旧版本，以及初始化容量设定，并无其他意义。 |
| 同步机制 | Lock“分段锁”机制 + volatile。Segment继承了ReentrantLock，一个Segment位置持有一把锁，内部是一个HashEntry数组，相当于把一个大的哈希表拆分成了多段的小哈希表，每一段使用不同的Segment对象作为锁，每次锁住一小段的桶位，保证了同步并提升并发度。另外HashEntry中的val和next属性使用volatile修饰，保证了单个变量的单次操作的原子性和可见性。 | CAS+synchronized+volatile。对于每一个哈希表的桶为加synchronized锁，每次只锁住一个桶位，保证一批代码线程安全并进一步提升并发度。同时使用CAS来完成对于单个变量的读-比-写等复合操作，保证了线程安全的同时避免了加锁。另外Node中的val和next属性使用volatile修饰，保证了单个变量的读、写等单次操作的原子性和可见性。    采用和JDK1.8的HashMap相同的结构的的原因是：对于过长的链表，顺序遍历时间复杂度为O(n)，会消耗大量时间，而对于长链表采用红黑树替换，可以降低时间复杂度至O(logn)。    另外，**抛弃了“分段锁”机制，而是锁住每一个桶位**，相比于“分段锁”机制的锁住一批桶位，可以说降低了锁的粒度，并且降低锁力度还减少了需要同步的代码的数量，这样就提升了更多的并发度。JDK1.7的ConcurrentHashMap并发度默认为16（Segment数组长度为16，因此只有16把锁），且初始化之后不可更改。    由于增加了CAS机制，因此很多代码不需要加锁即可实现同步，比如put()方法中初始化数组的代码，使用一个 sizeCtl 变量，如果CAS将这个变量置为-1，就表明table正在初始化，此时其他线程则自旋等待，如果初始化完毕那么都能安全的获得最新的初始化哈希表。 |
| hash操作 | 定位结点需要经历两次hash操作，第一次hash操作获取key的hash值，然后再第二次hash操作，根据获取的hash值定位某个Segment桶位，最后在该桶位下根据hash值定位到HashEntry中的某个桶位，需要消耗更多的时间； | 由于结构的改变，取消了Segment数组，只需要一次hash操作即可定位到Node数组的某个桶位。 |
| 写操作   | 除了需要定位两次之外，由于数据结构比较简单，因此整体而言比较简单。每一次的写锁住一个Segment位，但是下面可能有多桶位，虽然最终只会写某一个桶位，却可能会影响其他桶位的写。但是所有桶位只需要获取一次Segment锁即可。 | 每一次只会锁住一个桶位，不影响其他桶位的写。另外，由于采用了更加复杂的红黑树结构，因此写操作需要考虑更多的可能性，比如链表转红黑树、比如树还原为链表，如果在写红黑树时除了获取基于该桶位的synchronized锁，还需要再获取一个基于该桶位的读写锁中的写锁，然后才能继续操作，更加复杂。 |
| 读操作   | 除了需要定位两次之外，不会加锁，因为内部的val和next属性使用volatile修饰，因此保证每次读取都能获取到最新的数据 | 只需要定位一次，也不需要加锁，因为内部的val和next属性使用volatile修饰，因此保证每次读取都能获取到最新的数据。    但是，不需要加锁不代表一定不会加锁，在读取的桶位是红黑树的时候，由于红黑树结构的特性，会因为节点增删而发生较大改变，因此如果在读红黑树读的时候，该桶位的写锁没被获取 或者 没有线程在等待获取该桶位的写锁，那么读线程会尝试获取该桶位的读锁，因为获取了读锁之后写线程就不能获取写锁，红黑树结构便不会发生较大改变，此时读线程就能使用红黑树的方式去遍历这个桶位，读的效率更高。ConcurrentHashMap中读写锁的具体关系和代码实现在上面的章节已经介绍了。 |
| 锁的选择 | 采用Lock锁（Segment继承ReentrantLock）                       | 采用synchronized锁，之所以在更高级JDK版本使用更加原始的锁机制，是因为现在synchronized的优化已经非常好了，比如锁升级优化，性能和Lock锁相差无几，并且Lock锁采用了Java实现（底层的唤醒、阻塞等仍然采用了JVM实现），会消耗更高的内存空间。 |
| size操作 | 每一个Segment中具有一个count的volatile类型的变量，用于统计每一个Segment的元素数量，在调用size方法时，并不是简单的将所有Segment的count相加，那样可会导致得到的结果和真正的数量不一致，但也不是在统计的时候，停掉所有的写操作，那样会导致性能较低。   先尝试最多3次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，没有变化则直接返回，如果容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。判断count是否发生变化的方式是使用modCount变量，该变量用于记录ConcurrentHashMap内部的哈希表结构改变次数，每次put、remove等操作成功之后modCount自增一。    因此size方法至少统计两次。即使是在最后不得已使用了加锁的方式，最终返回的结果和此时的实际数量仍然可能不一致，因为是堆每一个Segment一次加锁统计，一个Segment统计完成之后就放开了去统计下一个Segment，此时写线程就可以去已经统计的Segment中获取锁并操作数据。 | 采用一个baseCount变量和一个counterCells数组来计数，当CAS更新baseCount变量出现线程竞争的时候，就会初始化一个counterCells数组来计数，这里的counterCells就相当于一个LongAdder，用于降低并发更新数值的时候发生冲突的概率。最终的size方法会将baseCount和counterCells数组元素的和相加，仍然获得一个近似值，因为JDK1.8的size操作根本就没有加锁。并且提供了一个更加精准的mappingCount方法用于处理总量超过int范围的情况，因此推荐使用mappingCount计数。相比于JDK1.7版本的繁琐的计数方式，JDK1.8的size方法的性能得到了极大的提升，但是也牺牲了准确度。 |
| 其他     | 类似于JDK1.8的HasmMap相对于JDK1.7的HashMap的优点。 比如扰动算法，JDK1.8的版本更加精简，比如尾插法，比如使用规律转移数据 |                                                              |









### LinkedBlockingQuque

LinkedBlockingQueue用了JUC的工具,相比这些工具频繁使用AQS,直接使用这些工具显得简单很多.

LinkedBlockingQuque为阻塞队列,其主要是线程操作队列可能会进入阻塞,(如果队列满了生产线程生产和队列空了消费线程消费).其为一单链表实现的同步数据结构如下.

```java
public class LinkedBlockingQueue<E> extends AbstractQueue<E>
        implements BlockingQueue<E>, java.io.Serializable {
  
  private final AtomicInteger count = new AtomicInteger();
  private final int capacity;
  transient Node<E> head;
  private transient Node<E> last;
  
  // 用于同步的锁和条件
  private final ReentrantLock takeLock = new ReentrantLock();
  private final Condition notEmpty = takeLock.newCondition();
  private final ReentrantLock putLock = new ReentrantLock();
  private final Condition notFull = putLock.newCondition();
  
  // 链表节点如下
  static class Node<E> {
      E item;
      Node<E> next;
      Node(E x) {
          item = x;
      }
  }
}
```

其构造器相对简单,如果我们自己不指定长度将会选择Integer的最大值.我们看其对于同步控制的结构可以发现,其使用了两把锁,这两把锁会用在队列的入队和出队上.

我们看其入队和出队操作

```java
public void put(E e) throws InterruptedException {
  if (e == null) throw new NullPointerException();
  int c = -1;
  Node<E> node = new Node<E>(e); // 封装好节点
  final ReentrantLock putLock = this.putLock;
  final AtomicInteger count = this.count;
  
  putLock.lockInterruptibly(); // 给入队操作上锁
  try {
    while (count.get() == capacity) {
      notFull.await(); // 如果count满的话,等待不满条件,满就进入CLH阻塞队列
    }
    
    enqueue(node); // 线程生产出的元素入队
    
    c = count.getAndIncrement(); // 自增
    
    if (c + 1 < capacity)
      notFull.signal();  
    // 上面这两行的理解有些困难,是因为如果这么一个情况,
    // 生产者线程有几个卡在notFull.await这,如果take时一次消费了多个产品
    // 那么却只会发生一次signalNotFull,这样子的话还有几个线程在阻塞,所以在这补充
    // 这个时候就可以连续唤醒线程了,相当于补全了signalAll的功能,如果等待队列为空
    // 那么无所谓不消耗性能
    
  } finally {
    putLock.unlock();
  }
  
  if (c == 0) // 如果take方法被更多的使用就会让c==0,根据上面到此处至少有一个节点了
    signalNotEmpty(); // 走到这说明自增结束所以可以通知take线程了
  // 这里面的操作使我们常见的使用两个消息队列实现生产者消费者模式的,只不过这里因为是用了两把锁所以得有一个先获取锁的过程
}
private void enqueue(Node<E> node) {
  // assert putLock.isHeldByCurrentThread();
  // assert last.next == null;
  last = last.next = node;
}
```

如果理解了上面的操作对照下take的操作

```java
public E take() throws InterruptedException {
  E x;
  int c = -1;
  final AtomicInteger count = this.count;
  final ReentrantLock takeLock = this.takeLock;
  takeLock.lockInterruptibly();
  
  try {
    while (count.get() == 0) {
      notEmpty.await();
    }
    x = dequeue();
    c = count.getAndDecrement();
    if (c > 1)
      notEmpty.signal(); 
    	// 这个操作同样是和上面一样,假如若干线程在notEmpty.await等待
    	// 那么假设一次性生产了很多产品又只有一次signal那么这里就可以持续下去
    	// 效果和signalAll类似
  } finally {
    takeLock.unlock();
  }
  
  if (c == capacity)
    signalNotFull();
  
  return x;
}

private E dequeue() {
  // assert takeLock.isHeldByCurrentThread();
  // assert head.item == null;
  Node<E> h = head;
  Node<E> first = h.next;
  
  h.next = h; // help GC 
  // 这里指向了自己形成了闭环,如果指向null的话,在单链表中就是到达了尾部不可
  
  head = first;
  E x = first.item;
  first.item = null;
  return x;
}
```

这两个方法就是BlockingQueue的主要方法了,其实用了JUC工具之后实现这些数据结构反而不难



### CopyOnWriteArrayList

COW(CopyOnWriteArray)写时复制是一种多线程程序设计思路,而JUC的CopyOnWrite实现了这一思路.CopyOnWrite操作是一个极其消耗内存的操作,其思路是如果要对一个数据结构(这里是数组)进行写,那么就把原来的数据结构复制一份,然后写复制的副本,等写完了在把原先的数据引用改向我们修改后的数据引用,不然会出现`ConcurrentModificationException`错误.

在操作系统一些资源的使用者Caller要同时访问内存或者磁盘上的数据,他们会共同获得指向资源的指针,当某个调用者修改资源的时候系统会复制一份专用的副本private copy给使用者.很显然该种结构适合读多写少的情况.Redis的RDB写快照备份的时候就用到linux的cow机制,

CopyOnWriteArrayList只对写写进行互斥,是可以同时读写的,而ReentrantReadWriteLock是写的时候不允许读的,而因为CopyOnWrite写时复制的原理,我们写的时候是可以读到和没写之前的数据一样的数据的.

这个类的设计初衷是为了解决读多写少的多线程性能问题.我们直接来看其如何实现的吧

```java
public class CopyOnWriteArrayList<E>
    implements List<E>, RandomAccess, Cloneable, java.io.Serializable {
    
  	final transient ReentrantLock lock = new ReentrantLock();
		private transient volatile Object[] array; // 如其名,底层是Array
}
```

来看其构造器,方法都是用Arrays.copyOf转移数组元素

```java
public CopyOnWriteArrayList() {
    setArray(new Object[0]);
}
public CopyOnWriteArrayList(Collection<? extends E> c) {
  Object[] elements;
  if (c.getClass() == CopyOnWriteArrayList.class)
    elements = ((CopyOnWriteArrayList<?>) c).getArray();
  else {
    elements = c.toArray();
    if (elements.getClass() != Object[].class)
      elements = Arrays.copyOf(elements, elements.length, Object[].class);
  }
  setArray(elements);
}
public CopyOnWriteArrayList(E[] toCopyIn) {
    setArray(Arrays.copyOf(toCopyIn, toCopyIn.length, Object[].class));
}
```

来看其关键操作,当某个线程调用add,其先复制一份数组,然后在放回去,这个过程加锁的,所以其他写线程给等待,不能进入.但是这个读线程是不加锁的,其就会读原来的数据结构.即只有写写互斥,无论是对应位置的写也好还是直接分配内存写后面也好,其逻辑的重点都在写时候加锁,但这样一来会损失数据的一致性.但其读的性能,因为不需要收到写锁制约就会非常快也是该数据结构的核心理念之一.

```java
public boolean add(E e) {
  final ReentrantLock lock = this.lock;
  lock.lock();
  try {
    Object[] elements = getArray();
    int len = elements.length;
    Object[] newElements = Arrays.copyOf(elements, len + 1);
    // 新数组分配内存
    newElements[len] = e; // 在数组的最后位置上添加元素
    setArray(newElements); // 放回去
    return true;
  } finally {
    lock.unlock();
  }
}
public void add(int index, E element) {
  final ReentrantLock lock = this.lock;
  lock.lock();
  try {
    Object[] elements = getArray();
    int len = elements.length; 
    if (index > len || index < 0) // 长度超出预期就直接报错
      throw new IndexOutOfBoundsException("Index: "+index+
                                          ", Size: "+len);
    Object[] newElements; 
    int numMoved = len - index; // 数组长度-索引 = 离终点距离
    if (numMoved == 0)
      newElements = Arrays.copyOf(elements, len + 1); // 复制了个新数组
    else {
      newElements = new Object[len + 1]; // 创建爱你个新的数组
      System.arraycopy(elements, 0, newElements, 0, index);
      System.arraycopy(elements, index, newElements, index + 1,
                       numMoved);
      // 前后的部分给复制上去
    }
    newElements[index] = element; // 对应位置赋值
    setArray(newElements); // 返回给原来的数组
  } finally {
    lock.unlock();
  }
}
```

我们看其读操作.太过于简单粗暴,所以可以看到其极其高的读性能,简直不讲道理

```java
public E get(int index) {
  return get(getArray(), index);
}
private E get(Object[] a, int index) {
  return (E) a[index];
}
```

我们看看其他的操作

```java
public boolean remove(Object o) {
  Object[] snapshot = getArray();
  int index = indexOf(o, snapshot, 0, snapshot.length); // 找到index
  return (index < 0) ? false : remove(o, snapshot, index);
}

public E remove(int index) {
  final ReentrantLock lock = this.lock;
  lock.lock();
  try {
    Object[] elements = getArray();
    int len = elements.length;
    E oldValue = get(elements, index);
    int numMoved = len - index - 1;
    if (numMoved == 0) // 删除头元素的特殊情况
      setArray(Arrays.copyOf(elements, len - 1));
    else { 
      Object[] newElements = new Object[len - 1]; // 申请新数组
      System.arraycopy(elements, 0, newElements, 0, index);
      System.arraycopy(elements, index + 1, newElements, index,
                       numMoved);
      setArray(newElements); // 复制数组然后给设置
    }
    return oldValue;
  } finally {
    lock.unlock();
  }
}
```

我们看下其set方法,我们会发现这一并发结构是相对简单的实现方式.

```java
public E set(int index, E element) {
  final ReentrantLock lock = this.lock;
  lock.lock();
  try {
    Object[] elements = getArray(); 
    E oldValue = get(elements, index); // 得到老元素
    if (oldValue != element) {
      int len = elements.length;
      Object[] newElements = Arrays.copyOf(elements, len);
      newElements[index] = element; // 给相应的地方设置值
      setArray(newElements);
    } else {
      // Not quite a no-op; ensures volatile write semantics
      setArray(elements);
    }
    return oldValue;
  } finally {
    lock.unlock();
  }
}
```



