# 分布式系统

---

所谓的分布式系统就是 应用集群之上编写能调用集群各个组件能力的系统 不同于单机处理 利用软件实时提升性能完成计算存储的系统 是分而治之思想的体现

分布式系统的一个重要实现是微服务,微服务是以服务为单独的系统去进行拆分再由其他组件来进行联合调度.

我们将从微服务应用入手,到探究更为一般的分布式系统如何去构建以及基本原理.

---



## 微服务介绍

### 微服务

即把每一个独立的服务从原本的服务器中拆分出来.分别放到相对小的服务器上,通过微服务之间的联动让整个系统能够完成正常的功能.微服务是分布式系统的一种构建思路,也是传统服务的一种解耦合服务.微服务是一种典型的纵向扩展.即垂直拆分.

### SOA

SOA即面向服务架构

### 分布式与集群

**分布式(distributed)**是指在**多台不同的服务器**中部署**不同的服务模块**,通过远程调用协同工作,对外提供服务.

**集群(cluster)**是指在**多台不同的服务器**中部署**相同应用或服务模块**,构成一个集群,通过负载均衡设备对外提供服务.



## 微服务的基本思路

---

分布式服务即是微服务,指的是一种服务拆分方式,其服务的划分会直接影响到系统的性能,不同的微服务系统根据不同的业务而定而非一成不变的系统.

微服务就是一种分布式的实现思路,其不同服务可能持有不同的数据库,对于服务的拆分极为重要,但是拆分也是相对而言的,不好的设计可能会浪费掉系统性能,比如交易系统拆分成订单服务,投标服务,转让服务等等.

### 服务拆分思路

**水平拆分**: 顾名思义 通过添加服务数量直接实现系统级请求的分流(负载均衡) 一般针对于无状态的服务,比如swarm集群中的replicas,nginx的轮询代理

**垂直拆分**: 只对业务进行拆分 不要让所有业务都运行在同一台机器上 单独的模块以单独的处理机单独的数据库去处理(分表或者使用数据库集群) 则不同模块之间的调用通过RPC框架实现(自己实现也可) 

除了上面两种拆分方法之外,在垂直划分中,我们如果要做各数据库的**联合查询**则会有以下三种方向.

1.  严格按照微服务的拆分思路,各微服务的数据库独立,需要展示数据时调用各个微服务的接口来获取
2.  将某一类业务高度相关的表放在同一数据库中,不是特别紧密的服务按照微服务拆分
3.  数据库严格按照微服务来拆分,然后一些高并发数据,实时性很强的数据同步到Nosql数据库中,在同步的过程中

### 微服务和高并发

高并发问题主要是性能问题,或者说是系统的可用性要求极高,在保证基本可用的前提下可以牺牲一部分一致性和可用性去换取更高的可用性,除了利用多线程,NIO等技术解决单点的服务问题之外,整体的性能也会被不当的分布式锁.要求过高的HA给拖累.下面基于平常构建应用的过程中的一些思路.

#### 单点并发优化

显然单点并发优化指的是我们利用NIO,AIO,多线程技术去提高单点CPU的使用性能,比如通过系统级别NIO服务器,比如使用RPC到和额外线程去处理存储I/O,比如使用消息队列(BlockingQueue)的通知机制去实现解耦合,提高本地IO并发性能.

#### 缓存

我们平时使用的redis就是很重要的缓存,直接访问缓存和我们在后端维护缓存可以大大降低持久化数据的获取时间,提高性能,且降低数据库负载.且利用好缓存能够进行数据预热等,使得系统的HA提高,且使用集群化的缓存本就提供了良好的可用性.

#### 限流

限流很好理解,就是限制接口访问的次数,包括前端,包括后端,包括数据库(即下面的异步削峰).从各层级上的限制某个接口的调用次数就可以省去接口需要处理的东西.从而使得系统的负载下降.

#### 异步削峰

数据库的限流我们没法通过数据库自己去实现高性能(性能不高可以实现),所以我们借助MQ等消息队列,对访问数据库的数据进行限流,监听消息队列的变化,如果消息队列有数据就去消费数据插入数据库,每次进行数据库访问的请求都进入到消息队列(过多阻塞),这就是一种很好的削峰思路.一种限流思想,另外一些其他的耗时操作我们也可以利用MQ实现缓存操作,然后异步操作的思路.

#### 服务降级

服务降级指的是在某些服务的需求量瞬间增加的时候,整体响应慢,只保留核心服务或者说优先对核心服务响应的弃车保帅的思想.在服务降级之前首先要保证系统的某些服务是可以被降级的.可以根据超时和成功率进行服务的降级预案,可以是人工降级,可以是自动化降级.例如参考降级预案

>    一般:比如有些服务偶尔因为网络抖动或者服务正在上线而超时,可以自动降级
>
>    警告:有些服务在一段时间内成功率有波动(如在95~100%之间),可以自动降级或人工降级,并发送警告.
>
>    错误:比如可用率低于90%,或者数据库连接池被打爆了,或者访问量突然猛增到系统能承受的最大阀值,此时可以根据情况自动降级或者人工降级
>
>    严重错误:比如因为特殊原因数据错误了,此时需要紧急人工降级.

这个降级意味着提供服务的质量下降.如下实际情况

-   错误等,默认页面是返回出错提示
-   一些不太重要的服务:相关分类,热销榜等,而这些服务在异常情况下直接不获取
-   读写降级,如果后端服务有问题,可以降级为只读写缓存
-   **爬虫降级**,在大促活动时,可以将爬虫流量导向静态页或者返回空数据.
-   额外服务,排队页面(将用户导流到排队页面等一会重试),无货(直接告知用户没货了),错误页(如活动太火爆了,稍后重试)
-   人工降级,在zookeeper或者redis的配置中设置服务调用方式,性能要求高的时候同步变异步,牺牲一致性换更高的性能.



---

下面是构建分布式系统的基本理论性问题,更偏向于从微服务过度到更一般的分布式系统,并用于理解和扩展现有的微服务架构.

---



## 构建分布式系统

---

高并发问题 解耦合 高可用 高性能,为了处理更高的数据量,处理足够多的并发请求,我们需要集群.

和集中式系统对比其能够提供

-   更高的计算能力
-   更强的存储能力
-   避免单点故障等

同样会导致一些问题

-   网络故障
-   需要保证数据的可用性
-   需要保证数据的一致性
-   并发读写

集中式系统能够通过事务处理不同的并发读写,而分布式系统得借助分布式锁,分布式协议等去完成数据同步.见下文分布式协议



## 解决分布式系统构建的技术

---

-   存储技术: Hadoop HDFS
-   计算技术: Hadoop MapReduce Spark Storm(流式计算) Flink
-   数据仓库: hive
-   PRC: dubbo SpringCloud
-   消息队列: RabbitMQ RocketMQ kalfka
-   分布式缓存: redis
-   分布式协调服务: zookeeper
-   分布式**资源**调度: yarn
-   分布式服务: SpringCloud
-   分布式全局检索: ElasticSearch
-   日志: logstash
-   基础容器: docker

这里说明在zookeeper和yarn两种技术,我们知道在分布式系统中,有若干机制共同维护CAP保证分布式系统的正常运作,yarn和zookeeper都是保证正常运作的两种机制,互补而互不影响.yarn擅长资源(cpu 内存)分配,主要负责资源的同一管理和分配.其主要把JobTracker和TaskTracker的职责更加明细了负责调度Job和分配资源.zookeeper提供配置维护,域名服务,分布式同步(分布式锁),选举,队列的接口等.和yarn共同协作完成hadoop生态的系统.zookeeper也多用在分布式服务上.



## 分布式服务需要解决的技术问题

---

分布式会话,分布式锁,分布式事务,RPC

分布式搜索,分布式缓存,分布式消息队列。

统一配置中心,分布式存储,数据库分库分表,限流、熔断、降级等。

其中缓存机制则是由redis可以解决的





## CAP定理

---

在计算机科学中,任意一个分布式系统都有 CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:

-   **一致性(Consistency)** (所有节点在同一时间具有相同的数据)
-   **可用性(Availability)** (保证每个请求不管成功或者失败都有响应)
-   **分隔容忍(Partition tolerance)** (系统中任意信息的丢失或失败不会影响系统的继续运作)

一致性是最基本的分布式系统的内容,各个协议的实现区别仅在于同一时间的响应的优先级,可用性自然不必多说,是系统服务长期稳定运作和宕机时候的处理,而分割容忍则是类似熔断器在集群中的地位.

### 一致性问题

我们提出来讨论一致性问题,参考[分布式一致性的探究](https://www.hollischuang.com/archives/663).

一个分布式系统无论在CAP之间如何取舍,都不可放弃一致性.如果放弃了一致性,就说明其系统并没有意义,在架构中锁说的放弃一致性是指放弃强一致性.在集中式系统中,例如数据库,我们一般是使用事务来保证数据的一致性问题,或者说一致性问题本来是数据库系统中的问题,而分布式系统中主要是**指数据的复制,不同数据节点中的数据是否完整.**

按数据库系统的设计思路数据的一致性一般是由原子性,隔离性,持久性共同实现的.在分布式系统中我们可能会遇到以下的情况,也可以说其是没有保证数据一致性的.

>   比如我们在电商网站下单，需要经历扣减库存、扣减红包、扣减折扣券等一系列操作。如果库存库存扣减成功，但是红包和折扣券扣减失败的话，也可以说是数据没有保证一致性。

所以我们看到了,分布式系统的CAP中的一致性和数据库系统中的ACID拥有相同的特性.

分布式系统的复制是导致其一致性被破坏的具体原因,而导致复制的原因主要是以下两点(异地容灾,多地部署)

-   可用性,将数据复制到多台机器上,避免单点故障不可用
-   性能,通过负载均衡技术能让分布式在不同地方的副本对外提供服务

引入复制机制后,不同的数据节点由于网络延迟的作用,容易产生数据不一致的情况.RPC和HTTP的分布式网络会导致网络延迟.像上面的问题,因为RPC调用不同系统的api,如果减库存成功,但是减折扣券失败就需要通过一些机制来决定是继续减库存还是回滚折扣券.这种机制就是**分布式系统数据一致性的解决方案**.从中我们可以看到,因为分布式系统并没有办法保证多个系统写的原子性.一旦某个操作失败就会发生数据性不一致的问题.所以我们要在性能和一致性之间权衡.

### 一致性模型

-   强一致性,写操作对用户立即可见,后续线程都读取最新的值.缺点是性能开销大
-   弱一致性,系统并不保证线程完全可见,系统的写入也不会保证立即写入,但会保证在某个时间(比如若干秒之后)可以让数据达到一致性的状态.
-   最终一致性,**弱一致性**的特殊形式,在保证系统没有后续更新的情况下,系统返回上一次更新操作的值.

### 最终一致性与BASE

BASE即`Basic Available 基本可用, Soft state 软状态, Eventual consistency最终一致性`,BASE是CAP理论的延伸

-   Basic Available 基本可用,在系统出现故障的时候允许损失部分性能,保证核心可用,例如服务降级
-   Soft state 软状态,指系统可以存在中间状态,中间状态不会影响西戎的可用性(如一份数据有三个数据副本,副本的复制延时)
-   Eventual consistency 最终一致性,即经过一段时间后系统最终保持数据一致性的状态.

ACID是追求强一致性的,BASE是追求最终一致性的,两者是完全相反的哲学.ACID是保证数据库稳定的一个机制,而BASE则是分布式系统的设计理念,其提出需要BA来完成基本功能,S来保证系统的容灾,E来保证数据的一致性.

最终一致性就保证BASE,其确保系统在后续没有更多数据操作的前提下,所有对系统的访问都将产生相同的结果.BASE的核心是保证系统尽可能可用,在数据收敛之前他可能会返回不同结果.

采用最终一致性的数据系统通常**不要求**数据操作失败时执行**回滚**(rollback).用户或系统日志将得知操作失败，但在另一次成功的操作之前，数据的不一致问题并不会被自动修复

最终一致性的出现是由于性能问题和可用性问题,在对数据一致性有很高要求的系统中,一般还是使用强一致模型.

上面这种最终一致性模型有多种实现方式,参考[分布式事务？No, 最终一致性](https://zhuanlan.zhihu.com/p/25933039)和[二阶段提交模型](https://blog.csdn.net/lengxiao1993/article/details/88290514)

>   A给B转100元,A,B位于不同的数据库上

![](https://pic3.zhimg.com/80/v2-adeed5162805d0de2b9c08dc5f1f943e_1440w.jpg)

二阶段提交模型如上,其需要一个协调者Coordinator参与,Precommit阶段就要锁住相关资源,commit和rollback时进行实际的提交和释放资源.

预提交成功一般来说提交或回滚就会成功.但如果到了提交阶段发生了数据滞留,那么协调者只能不断进行重试,从这里来看下游的A和B必须实现**幂等性(多次调用接口返回相同)**,如果架构允许的话,重试可以扔到MQ去让其重试然后通知.超过重试次数则通知各个参与者回滚上面的所有操作.其有相应的问题,我们可以利用其它模型对系统做改进.

-   协调者存在单点(可以尝试集群化)问题.如果协调者挂了,整个2PC逻辑就彻底不能运行.
-   执行过程是完全同步的.各参与者在等待其他参与者响应的过程中都处于阻塞状态,大并发下有性能问题.
-   仍然存在不一致风险/如果由于网络异常等意外导致只有部分参与者收到了commit请求,就会造成部分参与者提交了事务而其他参与者未提交的情况.

根据上面模型出现的问题,根据Fischer, Lynch, Paterson提出定理如下.

>   no distributed asynchronous protocol can correctly agree in presence of crash-failures
>
>   在出现宕机时(最终没有修复并重启),不存在一种分布式的异步协议可以正确地达成一致结果(同时提供安全性和存活性).

从上面我们可以总结出**最终**一致性,即最终可以通过服务器修复,或者等待宕机服务器重启等**最终使得系统到达数据一致性**.

DNS也是一种实现了最终一致性的模型.当我们用浏览器访问网站时,首先查看这个域名在本地的缓存,若最后更新的时间距离现在没有超过 TTL,则直接使用,否则去 DNS server上获取该域名的IP.同样的各级DNS server会去查看本地缓存,如果本地缓存没有超时就不去更新,否则就去向上级要,依次递归.整个系统根据层级缓存会逐步更新,所以最低层级的节点会存在一定的延时性.即**我们在 Name Server 上修改某条记录后,不会立刻扩散(同步)到所有的缓存服务器上**.这就是最终一致性的含义.

最终一致性还发展出了以下一致性

>因果一致性：如果A进程在更新之后向B进程通知更新的完成，那么B的访问操作将会返回更新的值。如果没有因果关系的C进程将会遵循最终一致性的规则。
>
>读己所写一致性：因果一致性的特定形式。一个进程总可以读到自己更新的数据。
>
>会话一致性：读己所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程读己之所写。
>
>单调读一致性：如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。
>
>单调写一致性：系统保证对同一个进程的写操作串行化。

### 分布式事务

我们知道事务是为了解决单点的一致性问题,那分布式事务解决的就是分布式系统中系统的一致性问题,所以我们可以把分布式事务看成是CAP定理的一致性的实现.分布式事务会涉及到多个跨网络的数据库的操作.

>   分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果(全部提交或全部回滚)

所以根据此我们引入了协调者的概念去实现分布式事务.在每个节点上使用传统事务可以保证ACID,但每台机子不知道其他机子的事务执行情况,因此需要一个协调者去保证此种事务的执行(会选择使用zookeeper等)

关于分布式事务的解决方案比较少,参考阿里程立的一文档

```note
一、结合MQ消息中间件实现的可靠消息最终一致性 
二、TCC补偿性事务解决方案 
三、最大努力通知型方案 第一种方案：可靠消息最终一致性，需要业务系统结合MQ消息中间件实现，在实现过程中需要保证消息的成功发送及成功消费。即需要通过业务系统控制MQ的消息状态 
```



---



## 分布式协议

分布式协议解决的问题就是上面的一致性模型的问题,所以又叫**分布式一致性协议**.为了解决上面的一致性问题,在长期的探索中,建立起了如下分布式协议保证系统的数据一致性.

一些常见的协议和使用他们的软件.

-   2PC Two Phase Commitment Protocol 二阶段提交
-   3PC Three Phase Commitment Protocol 三阶段提交
-   Paxos: google的分布式锁系统应用了该协议,zookeeper也使用了该协议
-   Raft: docker-swarm
-   Zab: zookeeper

---

分布式协议包括选举以及一些协同的过程,向上面介绍过的2PC(二阶段提交)也是属于分布式协议,在介绍分布式协议我们先介绍XA规范

### XA规范

X/Open定义了分布式事务处理模型,包括四部分

-   应用程序(AP)
-   事务管理器(TM) 一般是协调者例如zookeeper
-   资源管理器(RM) 一般是节点
-   通信资源管理器(CRM)

在一般情况下,数据库是无法感知其他数据库在做什么的,所以必须要交易中间件来协调各个数据库的访问情况.而一般数据库把自己的全局操作映射到全局事务中.

>   XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 

2PC和3PC就是基于此协议衍生出来的.我们重新看下2PC的过程,二阶段提交协议可以概括为,参与者将操作成败通知协调者,再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作.

### 二阶段提交

其实这个二阶段是指,第一阶段**准备阶段**,第二阶段**提交阶段**.

**准备阶段**,协调者(TM)给给每个参与者(RM)发Prepare消息,每个参与者要么直接返回失败,要么在本地执行事务,写本地的redo-log(重做日志)和undo-log(回滚日志),但不提交.

-   协调者向各节点询问是否可以提交操作(vote),并等待各节点的响应.
-   节点执行询问发起为止所有事务的操作.并把redo/undo信息写入日志(这里已经执行了事务)
-   节点响应协调者发起的询问,如果节点的事务执行成功,则返回ACK,执行失败返回终止

**提交阶段**

如果协调者收到了参与者的失败消息或超时,就直接给每个参与者发送rollback,否则就commit消息.参与者根据协调者指令执行回滚或者提交操作释放所有事务处理中使用的锁资源.下面分两种情况讨论讨论提交阶段的过程.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/success.png)

-   协调者向参与者发送commit请求
-   参与者完成请求,并释放资源
-   参与者向协调者发送ACK
-   协调者收到所有ACK后完成事务

如果任何一参与者节点在准备阶段返回终止或者协调者获取其他节点的响应信息超时

![](https://www.hollischuang.com/wp-content/uploads/2015/12/fail.png)

-   协调者向所有节点发送rollback
-   参与者利用undo日志进行回滚,并且释放资源
-   参与者节点发送回滚ACK到协调者
-   协调者在收到所有ACK之后结束事务回滚

所以无论二阶段如何都会结束事务.二阶段提交有其缺点

>   1.同步阻塞,所有节点都是同步阻塞的,性能低下
>
>   2.单点故障,协调者故障,系统完蛋
>
>   3.数据不一致,在commit阶段收到了请求但是因为网络故障无法到达等
>
>   协调者再发出commit消息之后宕机,而唯一接收到这条消息的参与者同时也宕机了.那么即使协调者通过选举协议产生了新的协调者,这条事务的状态也是不确定的,没人知道事务是否被已经提交.

---

### 三阶段提交

基于二阶段提交的种种问题,研究者们在基础上提出了三阶段提交模型.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/3.png)

其相对于二阶段调教有两个改进点,

1.  引入超时机制,同时在协调者和参与者中都引入超时机制.
2.  在第一阶段和第二阶段中插入一个准备阶段,保证了在最后提交阶段之前各参与节点的状态是一致的.

这样子就有三个阶段CanCommit,PreCommit,doCommit.

#### CanCommit

-   事务询问,协调者向参与者发送CanCommit请求,然后等待参与者响应
-   参与者响应,可以提交返回yes,不可以提交返回no

#### PreCommit

假如协调者获得所有参与者获取到了yes,那么就会执行事务与执行.

-   发送预提交请求,协调者向参与者发送PreCommit,进入Prepare阶段
-   事务预提交,参与者收到PreCommit请求之后,把redo和undo的信息记录到日志中.
-   响应反馈,参与者成功执行了事务操作,返回ACK

如果参与者在上个阶段发送了no,或者等待超时后就执行事务的中断

-   发送中断请求,协调者向所有参与者发送abort请求
-   中断事务,参与者在收到协调者的abort请求后,**或者参与者超时**执行事务的中断.

#### doCommit

该阶段进行真正的事务提交

-   发送提交请求,协调者接收到所有参与者发送的ACK请求后发送doCommit请求.
-   事务提交,在进行doCommit提交后,释放资源
-   响应反馈,事务提交完成后向协调者发送ACK请求
-   完成事务,在收到所有参与者的ACK请求后结束事务

如果在上一步没有接受到所有的ACK请求

-   发送中断请求,向所有协调者发送abort请求
-   事务回滚,利用undo信息进行回滚,回滚完后释放资源
-   响应反馈,在参与者全部回滚之后发送ACK
-   协调者在收到这些ACK之后进行事务的中断.

>   在doCommit阶段,如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时,会在等待超时之后,会**继续进行事务的提交**.(其实这个应该是基于概率来决定的,当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求,那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前,收到所有参与者的CanCommit响应都是Yes.(一旦参与者收到了PreCommit,意味他知道大家其实都同意修改了)所以,一句话概括就是,当进入**第三阶段**时,由于网络超时等原因,虽然参与者**没有收到commit或者abort响应**,但是他有理由相信:成功提交的几率很大,即**会发生提交**.

3PC主要解决了单点故障的问题,并减少阻塞,如果在doCommit无法收到协调者的信息会执行commit.这样的机制也有数据一致性的问题,如果abort指令没有收到,在超时之后也会提交.从这里我们也可以看出2PC和3PC都无法完全解决数据的一致性问题.

>   Google Chubby的作者Mike Burrows说过， `there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos.` 意即**世上只有一种一致性算法，那就是Paxos**，所有其他一致性算法都是Paxos算法的不完整版。

我们来从Paxos入手了解分布式协议

---

### Paxos

>   在古希腊有一个岛屿叫做Paxos,这个岛屿通过议会的形式修订法律.执法者(legislators,后面称为牧师priest)在议会大厅(chamber)中表决通过法律,并通过服务员传递纸条的方式交流信息,每个执法者会将通过的法律记录在自己的账目(ledger)上.问题在于执法者和服务员都不可靠,他们随时会因为各种事情离开议会大厅,服务员也有可能重复传递消息(或者直接彻底离开),并随时可能有新的执法者(或者是刚暂时离开的)回到议会大厅进行法律表决,
>
>   因此,议会协议要求保证上述情况下可以能够**正确的修订法律并且不会产生冲突**.

如所有分布式协议一样,Paxos是用来解决分布式系统的数据一致性问题.如果每个节点都执行**相同的序列**,那么只要这些执行的指令顺序固定,那么系统就是最终一致性的.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/Package-Diagram2.png)

C1是客户端.N1是三个服务器实例.当客户端向服务器请求操作S0.N1经过了op1op2op3的操作之后最终变成了数据S1,并且保持一致的话,就报保证N1的三个实例严格按照顺序执行op1op2op3,要么全部执行成功,要么执行失败.

paxos就是用于解决opi的取值,也就是第i个操作是什么,在确定了opi的内容之后,就可以让各个副本执行opi的操作.

参考

-   [维基百科](https://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95)
-   [分布式一致性算法](https://www.hollischuang.com/archives/693)

把上面场景简化,就变成如何确定一个不可变变量的取值(标识着这些操作).

问题抽象

>   设计一个系统，来存储名称为var的变量。
>
>   >   var的取值可以是任意二进制数,系统内部由多个**Accepter**组成,负责管理和存储var变量.
>   >
>   >   >   系统对外提供api,用来设置var变量的值`propose(var,V) => <ok,f> or <error> `
>   >>   
>   >   >>   将var的值设置为V,系统会返回ok和系统中已经确定的取值f,或者返回error.
>   >   
>   >   外部有多个Proposer机器任意请求系统,调用系统API(`propose(var,V) => <ok,f> or <error>`)来设置var变量的值.**CAS操作**
>   >   
>   >>   如果系统成功的将var设置成了V，那么返回的f应该就是V的值。否则，系统返回的f就是其他的**proposer**设置的值。
>
>   系统需要保证var的取值满足一致性
>
>   >   如果var没有被设置过,那么他的初始值为null
>   >
>   >   一旦var的值被设置成功,则不可被更改,并且可以一直都能获取到这个值
>
>   系统需要满足容错特性
>
>   >   可以容忍任意**proposer**出现故障,
>   >
>   >   可以容忍少数**acceptor**故障(半数以下)
>
>   暂时忽略网络分化问题和acceptor故障导致var丢失的问题.

上面的proposer可以理解为需要执行的数据库,acceptor是zookeeper协调者,多个acceptor是zookeeper集群.

#### 单个acceptor

单个acceptor可以由互斥锁去实现.并发去管理proposer的请求.proposer向acceptor申请互斥锁访问acceptor访问的资源.proposer需要按照获得互斥访问权的方式向acceptor持有的资源按顺序进行访问.而其他请求来设置该资源的值的话,就把原先设置好的资源返回给他(CAS).

>   acceptor会保存变量var的值和一个互斥锁lock.
>
>   提供接口prepare()
>
>   >   加互斥锁，给予var的互斥访问权，并返回当前var的取值
>
>   提供接口accept(var, v)
>
>   >   如果已经加锁，并且当前var没有值，则将var的值设置成v，并释放锁。
>
>   提供接口release()
>
>   >   用于释放互斥访问权

单个acceptor的proposer采用2PC(二阶段)实现.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/success.png)

下面这些操作均为请求提交阶段发送的请求,等到提交的时候在进行响应.

>   Step1,通过调用prepare接口来获取互斥性访问权和当前var的取值
>
>   >   如果无法获取到互斥性访问权,则返回,并不能进入到下一个阶段,因为其他proposer获取到了互斥性访问权.
>
>   Step2,根据当前var的取值f选择执行
>
>   >   1.如果f的取值为null,说明没有被设置过值,则调用接口accept(var ,v)来将var的取值设置成v,并释放掉互斥性访问权.
>   >
>   >   2.如果f的取值不为null,说明var已经被其他proposer设置过值,则调用release接口释放掉互斥性访问权.

显然该方案和2PC的缺点一致.效率低且容易发生死锁,协调节点若发生问题,系统直接崩溃.另一种解决方案不基于独占锁而基于抢占式访问.

每次proporser腰带一个编号(epoch)去访问acceptor,序号间要存在全序关系,一旦acceptor收到proposoer请求中包含一个比较大的epoch的时候立刻让其他epoch失效,不再接受他们提交的值,然后给新的epoch发放访问权,即可以设置资源的值.不同epoch的proposer之间遵循后者认同前者的原则

>在确保旧的epoch已经失效后,并且旧的epoch没有设置var变量的值,新的epoch会提交自己的值.
>当旧的epoch已经设置过var变量的取值,那么新的epoch应该认同旧的epoch设置过的值,并不在提交新的值.

---

基于上面对paxos的基本认识开始介绍分布式系统中的paxos,在paxos中包含着以下角色

-   **Proposer** 提出提案(Proposal).Proposal信息包括提案编号(Proposal ID)和提议的值(Value)。
-   **Acceptor** 参与决策,回应Proposers的提案.收到Proposal后可以接受提案,若Proposer获得多数Acceptors的接受,则称该Proposal被批准.
-   **Learner**不参与决策,从Proposers/Acceptors学习最新达成一致的提案(Value).

paxos要求2N+1个节点保证了最多允许N个节点同时出现问题,保证了2N+1的容错能力.多副本状态机中每个副本可能同时具有多个角色的能力.

![](https://pic1.zhimg.com/80/v2-2c0d971fcca713a8e045a93d7881aedc_1440w.jpg)











---

## 分布式锁