# 分布式系统

---

所谓的分布式系统就是 应用集群之上编写能调用集群各个组件能力的系统 不同于单机处理 利用软件实时提升性能完成计算存储的系统 是分而治之思想的体现

分布式系统的一个重要实现是微服务,微服务是以服务为单独的系统去进行拆分再由其他组件来进行联合调度.

我们将从微服务应用入手,到探究更为一般的分布式系统如何去构建以及基本原理.

---

[TOC]

## 微服务介绍

### 微服务

即把每一个独立的服务从原本的服务器中拆分出来.分别放到相对小的服务器上,通过微服务之间的联动让整个系统能够完成正常的功能.微服务是分布式系统的一种构建思路,也是传统服务的一种解耦合服务.微服务是一种典型的纵向扩展.即垂直拆分.

### SOA

SOA即面向服务架构

### 分布式与集群

**分布式(distributed)**是指在**多台不同的服务器**中部署**不同的服务模块**,通过远程调用协同工作,对外提供服务.

**集群(cluster)**是指在**多台不同的服务器**中部署**相同应用或服务模块**,构成一个集群,通过负载均衡设备对外提供服务.



## 微服务的基本思路

---

分布式服务即是微服务,指的是一种服务拆分方式,其服务的划分会直接影响到系统的性能,不同的微服务系统根据不同的业务而定而非一成不变的系统.

微服务就是一种分布式的实现思路,其不同服务可能持有不同的数据库,对于服务的拆分极为重要,但是拆分也是相对而言的,不好的设计可能会浪费掉系统性能,比如交易系统拆分成订单服务,投标服务,转让服务等等.

### 服务拆分思路

**水平拆分**: 顾名思义 通过添加服务数量直接实现系统级请求的分流(负载均衡) 一般针对于无状态的服务,比如swarm集群中的replicas,nginx的轮询代理

**垂直拆分**: 只对业务进行拆分 不要让所有业务都运行在同一台机器上 单独的模块以单独的处理机单独的数据库去处理(分表或者使用数据库集群) 则不同模块之间的调用通过RPC框架实现(自己实现也可) 

除了上面两种拆分方法之外,在垂直划分中,我们如果要做各数据库的**联合查询**则会有以下三种方向.

1.  严格按照微服务的拆分思路,各微服务的数据库独立,需要展示数据时调用各个微服务的接口来获取
2.  将某一类业务高度相关的表放在同一数据库中,不是特别紧密的服务按照微服务拆分
3.  数据库严格按照微服务来拆分,然后一些高并发数据,实时性很强的数据同步到Nosql数据库中,在同步的过程中

### 微服务和高并发

高并发问题主要是性能问题,或者说是系统的可用性要求极高,在保证基本可用的前提下可以牺牲一部分一致性和可用性去换取更高的可用性,除了利用多线程,NIO等技术解决单点的服务问题之外,整体的性能也会被不当的分布式锁.要求过高的HA给拖累.下面基于平常构建应用的过程中的一些思路.

#### 单点并发优化

显然单点并发优化指的是我们利用NIO,AIO,多线程技术去提高单点CPU的使用性能,比如通过系统级别NIO服务器,比如使用RPC到和额外线程去处理存储I/O,比如使用消息队列(BlockingQueue)的通知机制去实现解耦合,提高本地IO并发性能.

#### 缓存

我们平时使用的redis就是很重要的缓存,直接访问缓存和我们在后端维护缓存可以大大降低持久化数据的获取时间,提高性能,且降低数据库负载.且利用好缓存能够进行数据预热等,使得系统的HA提高,且使用集群化的缓存本就提供了良好的可用性.

#### 限流

限流很好理解,就是限制接口访问的次数,包括前端,包括后端,包括数据库(即下面的异步削峰).从各层级上的限制某个接口的调用次数就可以省去接口需要处理的东西.从而使得系统的负载下降.

#### 异步削峰

数据库的限流我们没法通过数据库自己去实现高性能(性能不高可以实现),所以我们借助MQ等消息队列,对访问数据库的数据进行限流,监听消息队列的变化,如果消息队列有数据就去消费数据插入数据库,每次进行数据库访问的请求都进入到消息队列(过多阻塞),这就是一种很好的削峰思路.一种限流思想,另外一些其他的耗时操作我们也可以利用MQ实现缓存操作,然后异步操作的思路.

#### 服务降级

服务降级指的是在某些服务的需求量瞬间增加的时候,整体响应慢,只保留核心服务或者说优先对核心服务响应的弃车保帅的思想.在服务降级之前首先要保证系统的某些服务是可以被降级的.可以根据超时和成功率进行服务的降级预案,可以是人工降级,可以是自动化降级.例如参考降级预案

>    一般:比如有些服务偶尔因为网络抖动或者服务正在上线而超时,可以自动降级
>
>    警告:有些服务在一段时间内成功率有波动(如在95~100%之间),可以自动降级或人工降级,并发送警告.
>
>    错误:比如可用率低于90%,或者数据库连接池被打爆了,或者访问量突然猛增到系统能承受的最大阀值,此时可以根据情况自动降级或者人工降级
>
>    严重错误:比如因为特殊原因数据错误了,此时需要紧急人工降级.

这个降级意味着提供服务的质量下降.如下实际情况

-   错误等,默认页面是返回出错提示
-   一些不太重要的服务:相关分类,热销榜等,而这些服务在异常情况下直接不获取
-   读写降级,如果后端服务有问题,可以降级为只读写缓存
-   **爬虫降级**,在大促活动时,可以将爬虫流量导向静态页或者返回空数据.
-   额外服务,排队页面(将用户导流到排队页面等一会重试),无货(直接告知用户没货了),错误页(如活动太火爆了,稍后重试)
-   人工降级,在zookeeper或者redis的配置中设置服务调用方式,性能要求高的时候同步变异步,牺牲一致性换更高的性能.



---

下面是构建分布式系统的基本理论性问题,更偏向于从微服务过度到更一般的分布式系统,并用于理解和扩展现有的微服务架构.

---



## 构建分布式系统

---

高并发问题 解耦合 高可用 高性能,为了处理更高的数据量,处理足够多的并发请求,我们需要集群.

和集中式系统对比其能够提供

-   更高的计算能力
-   更强的存储能力
-   避免单点故障等

同样会导致一些问题

-   网络故障
-   需要保证数据的可用性
-   需要保证数据的一致性
-   并发读写

集中式系统能够通过事务处理不同的并发读写,而分布式系统得借助分布式锁,分布式协议等去完成数据同步.见下文分布式协议



## 解决分布式系统构建的技术

---

-   存储技术: Hadoop HDFS
-   计算技术: Hadoop MapReduce Spark Storm(流式计算) Flink
-   数据仓库: hive
-   PRC: dubbo SpringCloud
-   消息队列: RabbitMQ RocketMQ kalfka
-   分布式缓存: redis
-   分布式协调服务: zookeeper
-   分布式**资源**调度: yarn
-   分布式服务: SpringCloud
-   分布式全局检索: ElasticSearch
-   日志: logstash
-   基础容器: docker

这里说明在zookeeper和yarn两种技术,我们知道在分布式系统中,有若干机制共同维护CAP保证分布式系统的正常运作,yarn和zookeeper都是保证正常运作的两种机制,互补而互不影响.yarn擅长资源(cpu 内存)分配,主要负责资源的同一管理和分配.其主要把JobTracker和TaskTracker的职责更加明细了负责调度Job和分配资源.zookeeper提供配置维护,域名服务,分布式同步(分布式锁),选举,队列的接口等.和yarn共同协作完成hadoop生态的系统.zookeeper也多用在分布式服务上.



## 分布式服务需要解决的技术问题

---

分布式会话,分布式锁,分布式事务,RPC

分布式搜索,分布式缓存,分布式消息队列。

统一配置中心,分布式存储,数据库分库分表,限流、熔断、降级等。

其中缓存机制则是由redis可以解决的





## CAP定理

---

在计算机科学中,任意一个分布式系统都有 CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:

-   **一致性(Consistency)** (所有节点在同一时间具有相同的数据)
-   **可用性(Availability)** (保证每个请求不管成功或者失败都有响应)
-   **分隔容忍(Partition tolerance)** (系统中任意信息的丢失或失败不会影响系统的继续运作)

一致性是最基本的分布式系统的内容,各个协议的实现区别仅在于同一时间的响应的优先级,可用性自然不必多说,是系统服务长期稳定运作和宕机时候的处理,而分割容忍则是类似熔断器在集群中的地位.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/cap.jpg)

### 一致性问题

我们提出来讨论一致性问题,参考[分布式一致性的探究](https://www.hollischuang.com/archives/663).

一个分布式系统无论在CAP之间如何取舍,都不可放弃一致性.如果放弃了一致性,就说明其系统并没有意义,在架构中锁说的放弃一致性是指放弃强一致性.在集中式系统中,例如数据库,我们一般是使用事务来保证数据的一致性问题,或者说一致性问题本来是数据库系统中的问题,而分布式系统中主要是**指数据的复制,不同数据节点中的数据是否完整.**

按数据库系统的设计思路数据的一致性一般是由原子性,隔离性,持久性共同实现的.在分布式系统中我们可能会遇到以下的情况,也可以说其是没有保证数据一致性的.

>   比如我们在电商网站下单，需要经历扣减库存、扣减红包、扣减折扣券等一系列操作。如果库存库存扣减成功，但是红包和折扣券扣减失败的话，也可以说是数据没有保证一致性。

所以我们看到了,分布式系统的CAP中的一致性和数据库系统中的ACID拥有相同的特性.

分布式系统的复制是导致其一致性被破坏的具体原因,而导致复制的原因主要是以下两点(异地容灾,多地部署)

-   可用性,将数据复制到多台机器上,避免单点故障不可用
-   性能,通过负载均衡技术能让分布式在不同地方的副本对外提供服务

引入复制机制后,不同的数据节点由于网络延迟的作用,容易产生数据不一致的情况.RPC和HTTP的分布式网络会导致网络延迟.像上面的问题,因为RPC调用不同系统的api,如果减库存成功,但是减折扣券失败就需要通过一些机制来决定是继续减库存还是回滚折扣券.这种机制就是**分布式系统数据一致性的解决方案**.从中我们可以看到,因为分布式系统并没有办法保证多个系统写的原子性.一旦某个操作失败就会发生数据性不一致的问题.所以我们要在性能和一致性之间权衡.

### 一致性模型

-   强一致性,写操作对用户立即可见,后续线程都读取最新的值.缺点是性能开销大
-   弱一致性,系统并不保证线程完全可见,系统的写入也不会保证立即写入,但会保证在某个时间(比如若干秒之后)可以让数据达到一致性的状态.
-   最终一致性,**弱一致性**的特殊形式,在保证系统没有后续更新的情况下,系统返回上一次更新操作的值.

### 最终一致性与BASE

BASE即`Basic Available 基本可用, Soft state 软状态, Eventual consistency最终一致性`,BASE是CAP理论的延伸

-   Basic Available 基本可用,在系统出现故障的时候允许损失部分性能,保证核心可用,例如服务降级
-   Soft state 软状态,指系统可以存在中间状态,中间状态不会影响系统的可用性(如一份数据有三个数据副本,副本的复制延时)
-   Eventual consistency 最终一致性,即经过一段时间后系统最终保持数据一致性的状态.

ACID是追求强一致性的,BASE是追求最终一致性的,两者是完全相反的哲学.ACID是保证数据库稳定的一个机制,而BASE则是分布式系统的设计理念,其提出需要BA来完成基本功能,S来保证系统的容灾,E来保证数据的一致性.

最终一致性就保证BASE,其确保系统在后续没有更多数据操作的前提下,所有对系统的访问都将产生相同的结果.BASE的核心是保证系统尽可能可用,在数据收敛之前他可能会返回不同结果.

采用最终一致性的数据系统通常**不要求**数据操作失败时执行**回滚**(rollback).用户或系统日志将得知操作失败，但在另一次成功的操作之前，数据的不一致问题并不会被自动修复

最终一致性的出现是由于性能问题和可用性问题,在对数据一致性有很高要求的系统中,一般还是使用强一致模型.

上面这种最终一致性模型有多种实现方式,参考[分布式事务？No, 最终一致性](https://zhuanlan.zhihu.com/p/25933039)和[二阶段提交模型](https://blog.csdn.net/lengxiao1993/article/details/88290514)

>   A给B转100元,A,B位于不同的数据库上

![](https://pic3.zhimg.com/80/v2-adeed5162805d0de2b9c08dc5f1f943e_1440w.jpg)

二阶段提交模型如上,其需要一个协调者Coordinator参与,Precommit阶段就要锁住相关资源,commit和rollback时进行实际的提交和释放资源.

预提交成功一般来说提交或回滚就会成功.但如果到了提交阶段发生了数据滞留,那么协调者只能不断进行重试,从这里来看下游的A和B必须实现**幂等性(多次调用接口返回相同)**,如果架构允许的话,重试可以扔到MQ去让其重试然后通知.超过重试次数则通知各个参与者回滚上面的所有操作.其有相应的问题,我们可以利用其它模型对系统做改进.

-   协调者存在单点(可以尝试集群化)问题.如果协调者挂了,整个2PC逻辑就彻底不能运行.
-   执行过程是完全同步的.各参与者在等待其他参与者响应的过程中都处于阻塞状态,大并发下有性能问题.
-   仍然存在不一致风险/如果由于网络异常等意外导致只有部分参与者收到了commit请求,就会造成部分参与者提交了事务而其他参与者未提交的情况.

根据上面模型出现的问题,根据Fischer, Lynch, Paterson提出定理如下.

>   no distributed asynchronous protocol can correctly agree in presence of crash-failures
>
>   在出现宕机时(最终没有修复并重启),不存在一种分布式的异步协议可以正确地达成一致结果(同时提供安全性和存活性).

从上面我们可以总结出**最终**一致性,即最终可以通过服务器修复,或者等待宕机服务器重启等**最终使得系统到达数据一致性**.

DNS也是一种实现了最终一致性的模型.当我们用浏览器访问网站时,首先查看这个域名在本地的缓存,若最后更新的时间距离现在没有超过 TTL,则直接使用,否则去 DNS server上获取该域名的IP.同样的各级DNS server会去查看本地缓存,如果本地缓存没有超时就不去更新,否则就去向上级要,依次递归.整个系统根据层级缓存会逐步更新,所以最低层级的节点会存在一定的延时性.即**我们在 Name Server 上修改某条记录后,不会立刻扩散(同步)到所有的缓存服务器上**.这就是最终一致性的含义.

最终一致性还发展出了以下一致性

>因果一致性：如果A进程在更新之后向B进程通知更新的完成，那么B的访问操作将会返回更新的值。如果没有因果关系的C进程将会遵循最终一致性的规则。
>
>读己所写一致性：因果一致性的特定形式。一个进程总可以读到自己更新的数据。
>
>会话一致性：读己所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程读己之所写。
>
>单调读一致性：如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。
>
>单调写一致性：系统保证对同一个进程的写操作串行化。

### 分布式事务

我们知道事务是为了解决单点的一致性问题,那分布式事务解决的就是分布式系统中系统的一致性问题,所以我们可以把分布式事务看成是CAP定理的一致性的实现.分布式事务会涉及到多个跨网络的数据库的操作.

>   分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果(全部提交或全部回滚)

所以根据此我们引入了协调者的概念去实现分布式事务.在每个节点上使用传统事务可以保证ACID,但每台机子不知道其他机子的事务执行情况,因此需要一个协调者去保证此种事务的执行(会选择使用zookeeper等)

关于分布式事务的解决方案比较少,参考阿里程立的一文档

```note
一、结合MQ消息中间件实现的可靠消息最终一致性 
二、TCC补偿性事务解决方案 
三、最大努力通知型方案 第一种方案：可靠消息最终一致性，需要业务系统结合MQ消息中间件实现，在实现过程中需要保证消息的成功发送及成功消费。即需要通过业务系统控制MQ的消息状态 
```



### CAP定理的不同倾向

CA: 不是分布式架构,就使用这种,关系数据库按照CA进行设计,放弃分区容错性,因为没有分区.

-   mysql

AP:加强可用性和分区容错性,放弃强一致性,追求最终一致性.

-   Eureka等常规应用集群,提供用户体验为主,追求BASE完备的系统
-   比如微信提现,提示两个小时到账(虽然有银行系统的周转),而不是马上到账

CP:强调强一致性和分区容错性,放弃可用性,比如Zookeeper,master在宕机后选举Leader期间不提供服务

-   比如跨行转账,就是立即到账,你这边转出,那边收进,方认为一个事务才算完成
-   zookeeper 强调各节点协调,故zookeeper又叫分布式协调组件

因为在分布式系统中无论如何都得满足P,即分区容忍性,要不就不能称之为分布式系统,故我们的系统设计一般选择CP或AP



---



## 分布式协议

分布式协议解决的问题就是上面的一致性模型的问题,所以又叫**分布式一致性协议**.为了解决上面的一致性问题,在长期的探索中,建立起了如下分布式协议保证系统的数据一致性.

一些常见的协议和使用他们的软件.

-   2PC Two Phase Commitment Protocol 二阶段提交
-   3PC Three Phase Commitment Protocol 三阶段提交
-   Paxos: google的分布式锁系统应用了该协议,zookeeper也使用了该协议
-   Raft: docker-swarm
-   Zab: zookeeper

---

分布式协议包括选举以及一些协同的过程,向上面介绍过的2PC(二阶段提交)也是属于分布式协议,在介绍分布式协议我们先介绍XA规范

### XA规范

X/Open定义了分布式事务处理模型,包括四部分

-   应用程序(AP)
-   事务管理器(TM) 一般是协调者例如zookeeper
-   资源管理器(RM) 一般是节点
-   通信资源管理器(CRM)

在一般情况下,数据库是无法感知其他数据库在做什么的,所以必须要交易中间件来协调各个数据库的访问情况.而一般数据库把自己的全局操作映射到全局事务中.

>   XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 

2PC和3PC就是基于此协议衍生出来的.我们重新看下2PC的过程,二阶段提交协议可以概括为,参与者将操作成败通知协调者,再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作.

### 二阶段提交

其实这个二阶段是指,第一阶段**准备阶段**,第二阶段**提交阶段**.

**准备阶段**,协调者(TM)给给每个参与者(RM)发Prepare消息,每个参与者要么直接返回失败,要么在本地执行事务,写本地的redo-log(重做日志)和undo-log(回滚日志),但不提交.

-   协调者向各节点询问是否可以提交操作(vote),并等待各节点的响应.
-   节点执行询问发起为止所有事务的操作.并把redo/undo信息写入日志(这里已经执行了事务)
-   节点响应协调者发起的询问,如果节点的事务执行成功,则返回ACK,执行失败返回终止

**提交阶段**

如果协调者收到了参与者的失败消息或超时,就直接给每个参与者发送rollback,否则就commit消息.参与者根据协调者指令执行回滚或者提交操作释放所有事务处理中使用的锁资源.下面分两种情况讨论讨论提交阶段的过程.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/success.png)

-   协调者向参与者发送commit请求
-   参与者完成请求,并释放资源
-   参与者向协调者发送ACK
-   协调者收到所有ACK后完成事务

如果任何一参与者节点在准备阶段返回终止或者协调者获取其他节点的响应信息超时

![](https://www.hollischuang.com/wp-content/uploads/2015/12/fail.png)

-   协调者向所有节点发送rollback
-   参与者利用undo日志进行回滚,并且释放资源
-   参与者节点发送回滚ACK到协调者
-   协调者在收到所有ACK之后结束事务回滚

所以无论二阶段如何都会结束事务.二阶段提交有其缺点

>   1.同步阻塞,所有节点都是同步阻塞的,性能低下
>
>   2.单点故障,协调者故障,系统完蛋
>
>   3.数据不一致,在commit阶段收到了请求但是因为网络故障无法到达等
>
>   协调者再发出commit消息之后宕机,而唯一接收到这条消息的参与者同时也宕机了.那么即使协调者通过选举协议产生了新的协调者,这条事务的状态也是不确定的,没人知道事务是否被已经提交.

---

### 三阶段提交

基于二阶段提交的种种问题,研究者们在基础上提出了三阶段提交模型.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/3.png)

其相对于二阶段调教有两个改进点,

1.  引入超时机制,同时在协调者和参与者中都引入超时机制.
2.  在第一阶段和第二阶段中插入一个准备阶段,保证了在最后提交阶段之前各参与节点的状态是一致的.

这样子就有三个阶段CanCommit,PreCommit,doCommit.

#### CanCommit

-   事务询问,协调者向参与者发送CanCommit请求,然后等待参与者响应
-   参与者响应,可以提交返回yes,不可以提交返回no

#### PreCommit

假如协调者获得所有参与者获取到了yes,那么就会执行事务与执行.

-   发送预提交请求,协调者向参与者发送PreCommit,进入Prepare阶段
-   事务预提交,参与者收到PreCommit请求之后,把redo和undo的信息记录到日志中.
-   响应反馈,参与者成功执行了事务操作,返回ACK

如果参与者在上个阶段发送了no,或者等待超时后就执行事务的中断

-   发送中断请求,协调者向所有参与者发送abort请求
-   中断事务,参与者在收到协调者的abort请求后,**或者参与者超时**执行事务的中断.

#### doCommit

该阶段进行真正的事务提交

-   发送提交请求,协调者接收到所有参与者发送的ACK请求后发送doCommit请求.
-   事务提交,在进行doCommit提交后,释放资源
-   响应反馈,事务提交完成后向协调者发送ACK请求
-   完成事务,在收到所有参与者的ACK请求后结束事务

如果在上一步没有接受到所有的ACK请求

-   发送中断请求,向所有协调者发送abort请求
-   事务回滚,利用undo信息进行回滚,回滚完后释放资源
-   响应反馈,在参与者全部回滚之后发送ACK
-   协调者在收到这些ACK之后进行事务的中断.

>   在doCommit阶段,如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时,会在等待超时之后,会**继续进行事务的提交**.(其实这个应该是基于概率来决定的,当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求,那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前,收到所有参与者的CanCommit响应都是Yes.(一旦参与者收到了PreCommit,意味他知道大家其实都同意修改了)所以,一句话概括就是,当进入**第三阶段**时,由于网络超时等原因,虽然参与者**没有收到commit或者abort响应**,但是他有理由相信:成功提交的几率很大,即**会发生提交**.

3PC主要解决了单点故障的问题,并减少阻塞,如果在doCommit无法收到协调者的信息会执行commit.这样的机制也有数据一致性的问题,如果abort指令没有收到,在超时之后也会提交.从这里我们也可以看出2PC和3PC都无法完全解决数据的一致性问题.

>   Google Chubby的作者Mike Burrows说过， `there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos.` 意即**世上只有一种一致性算法，那就是Paxos**，所有其他一致性算法都是Paxos算法的不完整版。

我们来从Paxos入手了解分布式协议

---

### Paxos

关于Paxos其实可以分为三个阶段,想明白这三个阶段的细节即可理解Paxos算法

-   Prepare-Promise
-   Propose-Accept
-   Learn

---

>   在古希腊有一个岛屿叫做Paxos,这个岛屿通过议会的形式修订法律.执法者(legislators,后面称为牧师priest)在议会大厅(chamber)中表决通过法律,并通过服务员传递纸条的方式交流信息,每个执法者会将通过的法律记录在自己的账目(ledger)上.问题在于执法者和服务员都不可靠,他们随时会因为各种事情离开议会大厅,服务员也有可能重复传递消息(或者直接彻底离开),并随时可能有新的执法者(或者是刚暂时离开的)回到议会大厅进行法律表决,
>
>   因此,议会协议要求保证上述情况下可以能够**正确的修订法律并且不会产生冲突**.

如所有分布式协议一样,Paxos是用来解决分布式系统的数据一致性问题.如果每个节点都执行**相同的序列**,那么只要这些执行的指令顺序固定,那么系统就是最终一致性的.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/Package-Diagram2.png)

C1是客户端.N1是三个服务器实例.当客户端向服务器请求操作S0.N1经过了op1op2op3的操作之后最终变成了数据S1,并且保持一致的话,就报保证N1的三个实例严格按照顺序执行op1op2op3,要么全部执行成功,要么执行失败.

paxos就是用于解决opi的取值,也就是第i个操作是什么,在确定了opi的内容之后,就可以让各个副本执行opi的操作.

参考

-   [维基百科](https://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95)
-   [分布式一致性算法](https://www.hollischuang.com/archives/693)

把上面场景简化,就变成如何确定一个不可变变量的取值(标识着这些操作).

问题抽象

>   设计一个系统，来存储名称为var的变量。
>
>   >   var的取值可以是任意二进制数,系统内部由多个**Accepter**组成,负责管理和存储var变量.
>   >
>   >   >   系统对外提供api,用来设置var变量的值`propose(var,V) => <ok,f> or <error> `
>   >>   
>   >   >>   将var的值设置为V,系统会返回ok和系统中已经确定的取值f,或者返回error.
>   >   
>   >   外部有多个Proposer机器任意请求系统,调用系统API(`propose(var,V) => <ok,f> or <error>`)来设置var变量的值.**CAS操作**
>   >   
>   >>   如果系统成功的将var设置成了V，那么返回的f应该就是V的值。否则，系统返回的f就是其他的**proposer**设置的值。
>
>   系统需要保证var的取值满足一致性
>
>   >   如果var没有被设置过,那么他的初始值为null
>   >
>   >   一旦var的值被设置成功,则不可被更改,并且可以一直都能获取到这个值
>
>   系统需要满足容错特性
>
>   >   可以容忍任意**proposer**出现故障,
>   >
>   >   可以容忍少数**acceptor**故障(半数以下)
>
>   暂时忽略网络分化问题和acceptor故障导致var丢失的问题.

上面的proposer可以理解为需要执行的数据库,acceptor是zookeeper协调者,多个acceptor是zookeeper集群.

#### 单个acceptor

单个acceptor可以由互斥锁去实现.并发去管理proposer的请求.proposer向acceptor申请互斥锁访问acceptor访问的资源.proposer需要按照获得互斥访问权的方式向acceptor持有的资源按顺序进行访问.而其他请求来设置该资源的值的话,就把原先设置好的资源返回给他(CAS).

>   acceptor会保存变量var的值和一个互斥锁lock.
>
>   提供接口prepare()
>
>   >   加互斥锁，给予var的互斥访问权，并返回当前var的取值
>
>   提供接口accept(var, v)
>
>   >   如果已经加锁，并且当前var没有值，则将var的值设置成v，并释放锁。
>
>   提供接口release()
>
>   >   用于释放互斥访问权

单个acceptor的proposer采用2PC(二阶段)实现.

![](https://www.hollischuang.com/wp-content/uploads/2015/12/success.png)

下面这些操作均为请求提交阶段发送的请求,等到提交的时候在进行响应.

>   Step1,通过调用prepare接口来获取互斥性访问权和当前var的取值
>
>   >   如果无法获取到互斥性访问权,则返回,并不能进入到下一个阶段,因为其他proposer获取到了互斥性访问权.
>
>   Step2,根据当前var的取值f选择执行
>
>   >   1.如果f的取值为null,说明没有被设置过值,则调用接口accept(var ,v)来将var的取值设置成v,并释放掉互斥性访问权.
>   >
>   >   2.如果f的取值不为null,说明var已经被其他proposer设置过值,则调用release接口释放掉互斥性访问权.

显然该方案和2PC的缺点一致.效率低且容易发生死锁,协调节点若发生问题,系统直接崩溃.另一种解决方案不基于独占锁而基于抢占式访问.

每次proporser腰带一个编号(epoch)去访问acceptor,序号间要存在全序关系,一旦acceptor收到proposoer请求中包含一个比较大的epoch的时候立刻让其他epoch失效,不再接受他们提交的值,然后给新的epoch发放访问权,即可以设置资源的值.不同epoch的proposer之间遵循后者认同前者的原则

>在确保旧的epoch已经失效后,并且旧的epoch没有设置var变量的值,新的epoch会提交自己的值.
>当旧的epoch已经设置过var变量的取值,那么新的epoch应该认同旧的epoch设置过的值,并不在提交新的值.

---

基于上面对paxos的基本认识开始介绍分布式系统中的paxos,在paxos中包含着以下角色

-   **Proposer** 提出提案(Proposal).Proposal信息包括提案编号(Proposal ID)和提议的值(Value)。
-   **Acceptor** 参与决策,回应Proposers的提案.收到Proposal后可以接受提案,若Proposer获得多数Acceptors的接受,则称该Proposal被批准.
-   **Learner**不参与决策,从Proposers/Acceptors学习最新达成一致的提案(Value).

paxos要求2N+1个节点保证了最多允许N个节点同时出现问题,保证了2N+1的容错能力.多副本状态机中每个副本可能同时具有多个角色的能力.在多副本状态机中,每个副本同时具有三种角色.下图为其阶段性角色的变化.

![](https://pic1.zhimg.com/80/v2-2c0d971fcca713a8e045a93d7881aedc_1440w.jpg)

其数据同步过程如下

1.  第一阶段：Prepare阶段。Proposer向Acceptors发出Prepare请求，Acceptors针对收到的Prepare请求进行Promise承诺。
2.  第二阶段：Accept阶段。Proposer收到多数Acceptors承诺的Promise后，向Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理。
3.  第三阶段：Learn阶段。Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners。

![](https://pic2.zhimg.com/80/v2-a6cd35d4045134b703f9d125b1ce9671_1440w.jpg)

其时序如上,可以看到其借助了2PC阶段思想,分为了以下几个过程,详细如上

-   Prepare-Promise prepare生成全局id,promise做出"两个承诺一个应答"

    两个承诺一个应答指的是

    -   不再接受Proposal ID小于等于(注意:这里是<= )当前请求的Prepare请求.
    -   不再接受Proposal ID小于(注意:这里是< )当前请求的Propose请求.(=号没有是因为本id可以提案)
    -   不违背以前作出的承诺下，回复**已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID**，没有则返回空值。

-   Propose-Accept 

    Propose: Proposer 收到多数Acceptors的应答后，从应答中选择**Proposal ID最大的提案的Value**，作为本次要发起的提案。如果所有应答的提案Value均为**空值**，则可以**自己随意决定**提案Value。然后携带当前Proposal ID，向所有Acceptors发送Propose请求。

    Accept: Acceptor收到Propose请求后，在不违背自己之前作出的承诺下，接受并持久化当前Proposal ID和提案Value。

-   Learn

单这么说还是很抽象下面来看几个例子

图中P代表Prepare阶段，A代表Accept阶段。3.1代表Proposal ID为3.1，其中3为时间戳，1为Server ID。X和Y代表提议Value。Si代表了不同的节点,

![](https://pic1.zhimg.com/80/v2-ac7e4a827f77dc57d316c77ae95e1940_1440w.jpg)

如图上面,我们看到了s1,s2,s3接受了(A 3,1 X)蓝色的节点值X,此时过了大多数,则此时是X,由s3节点下下一时间段提出了新的Propose(P 4.5)则后续s3,s4,s5都接受了此提议,所以节点值要变成绿色的X.

这里需要注意到的点是,其他节点虽然没有接受,但是超过了半数节点同意,其他节点就得去学习别人的Propose.

来看另一个例子

![](https://pic2.zhimg.com/80/v2-3ae48cb81d39079022666ccb35821c71_1440w.jpg)

如图则可见,5个节点因为网络等原因,在s3节点(A 3.1 X)时并未作出决策,而在更后面同时受到了大多数决策(A 4.5 X)和少数决策(A 3.1 X),则此时节点的提案值更新为由(A 4.5 X)所决定的值.

![](https://pic2.zhimg.com/80/v2-931f9487900f0f002867c9e116dec255_1440w.jpg)

如上则是另一种情况,如上s3节点(A 3.1 X)和(A 4.5 Y)决策同时作出,则根据两个承诺原则,(A 3.1 X)被废弃,使用(A 4,5 Y)达成大多数.

需要注意的是Paxos可能会发生活锁现象如下

![](https://pic1.zhimg.com/80/v2-0e18b29659367076ff1c0156ae46eca0_1440w.jpg)

但毫无疑问发生连续活锁的概率微乎其微,Paxos能很大程度的保证了分布式系统的一致性.

总结而言,Paxos算法在理论上提供了一种由节点共同决策分布式系统一致性的协议,该算法有非常广泛性的理论价值,工业中的算法是堆上述算法进行改进后的结果.



### Raft协议

工业界的Paxos在实现上比较闹心,因此更多采用的是Raft协议实现的分布式系统.当然也有基于Paxos改进算法实现的分布式系统(参考微信后台的分布式存储系统),相比较而言Zab协议就会比较难理解,我们这里介绍Raft算法.

Raft有三种角色

-   Leader 处理client的更新请求,本地处理后在分发到各个节点
-   Follower 同slave节点,保存Leader的数据副本,同步Leader的数据
-   Candidate Leader出故障了,slave变成此状态,直至选主结束

Leader向所有Follower发送心跳,如果收不到心跳,就转变为Candidate直至选出Leader,每个副本自己维护一个term,每发生一个动作就会递增,系统默认使用最新的term(在zab里面相当于使用最大的zxid),如果一个Candidate收到低于自己term的提议会直接忽略

-   [raft算法动态演示](http://thesecretlivesofdata.com/raft/)

#### 选举

-   当处于Candidate的节点收到心跳的时候会转变为Follower节点.
-   当处于Candidate的节点收到半数投票时候会变成Leader节点,并定期广播心跳信息维持统治.

election timeout

该数据结构由每个节点自己维护,该数据如果超时,那么就会进入Candidate状态,并执行选举流程.Leader的Heartbeat能够重置该数据结构的定时器.

**处于投票阶段的时候有可能有两个节点到达相同的票数(还未到大多数节点)**,那么节点就会等待未投票的节点的响应,如果没有未投票的节点,在超时后进行下一轮的选举,直至选出主节点.



#### 数据同步

和大多数master-slave结构一样,master负责读写,slave仅负责读.master在写之后请求各从节点进行数据同步,多数节点同步成功之后,才会给客户端返回成功.



#### 分区容错/脑裂

假设原本的 Leader 在双节点的集群里面，那么这个集群会照常运作。而新出现的三个节点的集群，由于没有收到心跳，会开始选举，然后选出新的 Leader。这时候，如果有客户端发起请求，有可能发送到两个不同的 Leader 上面，如果发送到原来的那个 Leader 上，即双节点的集群中，Leader 把操作同步给 Follower，会发现收不到足够多的 Follower 响应（因为这个 Follower 还以为自己的集群是五个节点），然后就没办法同步数据。而三节点的新集群，就可以顺利更新数据。

如果这时候网络恢复了，各个节点又可以正常通信，三节点集群中的 Leader 和 双节点集群中的 Leader 会互相通信，然后会发现三节点的 Leader 由于一直正常运行，term 值会不断增大，所以大家会采信他的数据。于是双节点的两台机器会回滚，然后全部接受新 Leader 的数据同步



### Zab协议

Zab协议,即是zookeeper atomic boardcast,即原子广播.顾名思义是zookeeper里面分布式协议的重要机制.Zab协议定义了四个阶段

-   选举election
-   发现discovery,同步sync (选主之后的恢复各个节点的数据)
-   广播boardcast (master把写的数据分发到所有节点)

其角色有三种

-   Leader
-   Follower
-   Observer

其节点状态如下

-   Looking: 正在选举状态
-   Leading 负责写
-   Following 不需要写,参与投票
-   Observing 不需要写,不参与投票

其各节点的服务器架构如下

![](https://images2015.cnblogs.com/blog/183233/201603/183233-20160316223234865-1124736424.png)

每次**写成功**的消息,都有一个全局唯一的标识.叫 zxid,是 64 bit 的正整数,高 32 为叫 epoch 表示选举纪元,低 32 位是自增的 id,每写一次加一.每换一次leader,epoch自增一次,每写一次id自增一次.

#### 选举

选举有多种算法

-   0 基于UDP的LeaderElection
-   1 基于UDP的FastLeaderElection
-   2 基于UDP和认证的FastLeaderElection
-   3 基于TCP的FastLeaderElection **(默认)**

且后续zookeeper以3的FastLeaderElection为主,后面介绍也以FastLeaderElection为例.选举流程如下

-   每个进入Looking状态的节点会清空投票箱,然后通过广播投票给自己,在把投票信息发给其他节点,投票信息包括: 轮数,被投票节点的zxid(轮数,写的次数),被投票节点的编号
-   每个节点和本地存储的轮数进行对比,小于就丢弃
-   如果大于就证明本地过期,更新轮数和收到的内容然后通知其他节点.
-   如果轮数相等,就比较投票的优先级,如果收到的票优先级更高就投优先级更高的,如果相等则更新对应节点的投票
-   每次收到投票后,更新结果列表,如果达到半数以上,则终止投票,宣布自己称为Leader.

#### 主从同步

和所有的master-slave结构一样,master主要负责写,其他slave节点负责读.slave节点如果收到写请求会转发到master节点进行写.

master的处理过程如下

-   Leader收到写操作时,生成zxid然后发给所有follower节点.
-   follower会把提议的事务写到本地磁盘,成功后返回Leader,Leader收到半数以上反馈在对所有Follower确认,让所有的Follower进行提交,Follower收到事务后进行提交,整个过程就就完成了.



### Raft和Zab的对比

#### 节点优先

-   Raft: term 大的优先,然后 entry 的 index 大的优先
-   ZooKeeper: peerEpoch 大的优先,然后 zxid 大的优先

ZooKeeper 有 2 个轮次,一个是选举轮次 electionEpoch,另一个是日志的轮次 peerEpoch(即表示这个日志是哪个轮次产生的).而 Raft 则是只有一个轮次,相当于日志轮次和选举轮次共用了.

ZooKeeper在选举完成之后是会对节点数据进行同步纠正.

#### 选举效率

-   Raft 中的每个节点在某个 term 轮次内只能**投一次票**,哪个 Candidate 先请求投票谁就可能先获得投票,这样就可能造成分区,即各个 Candidate 都没有收到过半的投票,Raft 通过 Candidate 设置不同的**超时时间**,来快速解决这个问题,使得先超时的Candidate(在其他人还未超时时)优先请求来获得过半投票.
-   ZooKeeper 中的每个节点,在某个 electionEpoch 轮次内,可以**投多次票,只要遇到更大的票就更新,然后分发新的投票给所有人**.这种情况下**不存在分区现象**,同时有利于选出含有更新更多的日志的 Server,但是选举时间理论上相对 Raft 要花费的多.

#### 集群伸缩

-   Raft:比较简单,该节点启动后,会收到 Leader 的 AppendEntries RPC,在这个 RPC 里面包含 Leader 信息,可以直接识别.
-   ZooKeeper: 启动后,会向所有的其他节点发送投票通知,然后收到其他节点的投票.该节点只需要判断上述投票是否过半,过半则可以确认 Leader.

上面Raft是通过心跳去确认,如果没有在即是之后就进行选主,zookeeper解决策略是完善选举策略,直接进入选主阶段发现主.

#### 选主之后超时

-   Raft: 如果 Follower 在倒计时时间内未收到 Leader 的心跳信息,则 Follower 转变成 Candidate,自增 term 发起新一轮的投票.
-   ZooKeeper: Leader 和 Follower 都有各自的检测超时方式,Leader 是检测是否过半 Follower 心跳回复了,Follower 检测 Leader 是否发送心跳了,一旦 Leader 检测失败,则 Leader 进入 Looking 状态.其他 Follower 过一段时间因收不到 Leader 心跳也会进入 Looking 状态,从而出发新的 Leader 选举.一旦 Follower 检测失败了,则该 Follower 进入 Looking 状态,此时 Leader 和其他 Follower 仍然保持良好,则该 Follower 仍然是去学习上述 Leader 的投票,而不是触发新一轮的 Leader 选举.

#### 残存数据处理

-   已过半复制的日志
-   未过半复制的日志
-   Raft:对于之前 term 的过半或未过半复制的日志采取的是保守的策略,全部判定为未提交,只有当前 term 的日志过半了,才会顺便将之前 term 的日志进行提交
-   ZooKeeper: 采取激进的策略,对于所有过半还是未过半的日志都判定为提交,都将其应用到状态机中

#### 顺序保证

-   Raft:对请求先转换成 entry,复制时,也是按照 Leader 中 **log 的顺序**复制给 Follower 的,对 entry 的提交是按 index 进行顺序提交的,是可以保证顺序的
-   ZooKeeper: 在提交议案的时候也是按顺序写入**各个 Follower 对应在 Leader 中的队列**,然后 Follower 必然是按照顺序来接收到议案的,对于议案的过半提交也都是一个个来进行的



## 分布式锁

所谓的分布式锁其实和单点的锁区别的点就在于锁信息的保存上,我们可以基于redis和mysql去实现分布式锁,本质上是利用了redis和mysql两个数据系统本身的一致性.

分布式锁有几种设计方案

-   基于数据库/缓存(mysql,redis,memcache)实现
-   基于zookeeper实现
-   自己完成分布式系统的构建

这把锁的特性最好有以下几个

-   可重入锁(避免死锁)
-   该锁最好是阻塞锁

其实分布式锁的设计理念对于程序员而言很简单,就是操作某些单点(或者分布式)系统的数据结构,只要这些数据结构是线程安全的(比如mysql的表(InnoDB的行级锁),redis中的list),那么就可以争抢这些资源即分布式锁的实现.

### 基于mysql实现

我们看下mysql实现的分布式锁,有如下表

其思路是直接使用InnoDB的行级锁实现数据一致性,即数据库插入是线程安全的这一特性,以及InnoDB行级锁仅会在索引时启动.

```sql
CREATE TABLE `methodLock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
  `desc` varchar(1024) NOT NULL DEFAULT '备注信息',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
  
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
  
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='锁定中的方法';
```

我们每次使用锁的时候就是操纵上面的数据结构,`method_name`是一个不可重复的字段,并且加上了BTREE的索引.

我们用下面方法加锁

```sql
insert into methodLock(method_name,desc) values (‘method_name’,‘desc’)
```

用下面方法释放锁

```sql
delete from methodLock where method_name ='method_name'
```

我们可以看到其借助了InnoDB的行级锁实现的分布式锁.

这种锁缺点很大

-   单点依赖性,即数据库挂了系统不可用
-   该锁无失效时间.如果解锁失败,其他线程无法获得
-   该锁非阻塞,insert失败不会进入排队队列
-   该锁非重入

其解决方案如下 [参考](https://www.hollischuang.com/archives/1716)

-   数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。
-   没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。
-   非阻塞的？搞一个while循环，直到insert成功再返回成功。
-   非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

我们用`for update`排它锁进行改进,下面伪代码

```java
public boolean lock(){
    connection.setAutoCommit(false)
    while(true){
        try{
            result = 
              select * from methodLock where method_name=xxx for update;
            if(result==null){
                return true;
            }
        }catch(Exception e){

        }
        sleep(1000);
    }
    return false;
}

public void unlock(){
    connection.commit();
}
```

这里有几个点,`for update`会自动提交,当他执行成功的时候是自动返回,但执行失败其就一直阻塞,服务器宕机的时候会自动释放掉锁.

这个方法其实还是有一些弊端的,比如mysql加锁逻辑要看执行计划是否真的对索引使用行级锁(表过小也是不会触发排它锁的),另外其消耗的是数据库连接这种资源

### 基于redis实现

我们来看基于缓存实现的分布式锁,显然基于缓存实现的性能比基于数据库实现的性能要高(参考存储读取数据存放的介质)实现思路和上面类似,都是基于分布式数据库系统本身的一致性来实现的.

redis提供了一函数setnx(set if exist)来实现一致性的操作.且redis作为缓存系统能够设置过期时间,相比于mysql其能提供能够实现分布式锁的功能相对较全.

我们利用RedisTemplate来实现,其中一方法叫**setIfAbsent**

```java
@Component
public class DistributedLock {
    @Autowired
    private StringRedisTemplate redisTemplate;
  
  	/**
  	* 获取锁
  	**/
    public boolean getLock(String lockId, long millisecond) {
      Boolean success = redisTemplate.opsForValue()
        .setIfAbsent(lockId,"lock",millisecond, TimeUnit.MILLISECONDS);
      return success != null && success;
    }
  	/**
  	* 释放锁
  	**/
   	public void releaseLock(String lockId) {
      redisTemplate.delete(lockId);
    }
}
```

上面还有个问题就是可重入锁的问题,这个只要换成不同的数据结构存储线程的信息可以解决.该方法唯一的弊端是不是阻塞的锁,我们可以用while循环来让其一致循环等待.

redis因为其线程安全特性还能够在多线程程序中进行以下两种重要的应用

-   同理因为其数据一致性,我们可以直接把list当成阻塞队列来用
-   还可以通过对channel的publish和subscribe实现订阅和发布的消息队列的数据一致性

关于publish和subscribe和rabbitmq中的不同点是,rabbitmq有可靠的机制会保证消息会被消费而redis则是没有这种机制

### 基于zookeeper实现

-   未完待续



## 集群模式

分布式系统一般是用集群作为系统构建的基础

### 主从模式

![](https://img2018.cnblogs.com/blog/1350922/201910/1350922-20191006113347736-1579349638.png)

这几乎是最简单的集群模式了,参考redis/zookeeper的服务器模式.主节点负责操作各种数据,并且把数据同步到其他从节点,而其他从节点从主节点的同步.

该数据系统有很强的分区容忍性,而缺少了部分一致性和可用性.从节点只是作为主节点的备份存在,读操作可以从各个从节点上读取,但主节点才能进行写.这种情况数据同步需要时间(如果在同步的时候有查询请求过来数据系统会返回之间的数据).可用性是当主节点挂了之后系统就会崩溃.

### 哨兵模式

哨兵模式是堆主从模式的进行改进,选择使用了哨兵集群来观测主从模式的master,如果主节点下线就选出新的slave节点成为master节点(分布式选举),结构如下

![](https://img2018.cnblogs.com/blog/1350922/201910/1350922-20191006122611921-809764078.png)

其优点继承了主从模式的优点,并且在可用性上有更高的提升.

### cluster模式

![](https://upload-images.jianshu.io/upload_images/12185313-0f55e1cc574cae70.png)

cluster模式区别于上述的两种模式,上述的两种模式都是以standby思想作为代替某一节点工作的,下面一个方式涉及到的是**数据分布**,我们回顾下哨兵模式和主从模式,他们都是一个节点完成写,其他节点完成读,且写数据需要所有节点数据同步,这会造成大量内存浪费**数据冗余**.

Cluster模式解决了这样的问题,每个节点都可以读写,我们只需要对要进行读写的数据进行一个hash就能确定其在哪个节点上了,例如redis-cluster采用的虚拟槽算法就是这么一种数据分布算法.或者是一致性hash计算在哪个节点上.

该种结构的好处是没有太多数据冗余(一个数据只有2个或3个副本)而不是每个节点都存储一份数据,空间复杂度由O(N)变成O(1).

