原文地址:https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431927781401bb47ccf187b24c3b955157bb12c5882d000
多线程编程技术
	真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。
	进程:process
	服务service:process执行所需要的子程序
	任务task:多个process共同执行事件的响应
	线程:中断进程
	并发:等待队列优先级机制,时间轮询算法,单cpu不断切换 类似多线程
	并行:多个cpu执行不同的任务 类似多进程或者分布式
	
	资源分配给进程
	处理机分配给线程
	线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步
	多线程并发性高
	多个线程共享内存
	一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行
	由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。
多任务的实现有3种方式：Master-Worker模式
	多进程模式；
	多线程模式；
	多进程+多线程模式。
多进程和多线程的程序涉及到同步、数据共享的问题，编写起来更复杂。
Linux多线程:
	Unix/Linux操作系统提供了一个fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。
	子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。
	有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。
	multiprocessing
	如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？
	由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。
多进程:
multiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：
	from multiprocessing import Process
	import os
	def child(arg):
	    ...
	if __name__=='__main__':
	    print(os.getpid())#得到的是整个python程序的pid
	    p=Process(target=child,args=(arg,))
	    p.start()#进程开始
	    p.join()#等待进程执行结束后往下运行,进程同步
	用Pool创建大量子进程
	from multiprocessing import Pool
	p = Pool(processes=4)#指定进程池数量
	#意思就是进程最多只有四个,多的要等待
	for i in range(6):
		p.apply_async(long_time_task, args=(i,))
		#允许异步
	p.close()#关闭进程池不能添加其他进程了
	p.join()
	#apply_async()方法把进程放入请求队列
	#实际上进程池就是通过队列实现的,然后按照Process填入参数、
	Pool()默认的参数是cpu的核心数
	或者调用multiprocessing.cpu_count()#来获得cpu核心数
	子进程:subprocess#可以调用cmd等
	进程通信process之间通过Queue或者Pipes交换数据
	from multiprocessing import Process, Queue
	import os, time, random

	# 写数据进程执行的代码:
	def write(q):
	    print('Process to write: %s' % os.getpid())
	    for value in ['A', 'B', 'C']:
	        print('Put %s to queue...' % value)
	        q.put(value)
	        time.sleep(random.random())

	# 读数据进程执行的代码:
	def read(q):
	    print('Process to read: %s' % os.getpid())
	    while True:
	        value = q.get(True)
	        print('Get %s from queue.' % value)

	if __name__=='__main__':
	    # 父进程创建Queue，并传给各个子进程：
	    q = Queue()
	    pw = Process(target=write, args=(q,))
	    pr = Process(target=read, args=(q,))
	    # 启动子进程pw，写入:
	    pw.start()
	    # 启动子进程pr，读取:
	    pr.start()
	    # 等待pw结束:
	    pw.join()
	    # pr进程里是死循环，无法等待其结束，只能强行终止:
	    pr.terminate()
	核心代码:
	q = Queue()#创建消息队列
	q.put(value)
	q.get(True)#一直等待
	process.terminate()#终止进程
	父进程所有Python对象都必须通过pickle序列化再传到子进程去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。
	Pool简化程序编程
	import multiprocessing as mp

	def job(x):
	    return x*x

	def multicore():
	    pool=mp.Pool(processes=2)#定义一个Pool，并定义CPU核数量为2
	    res=pool.map(job,range(10))
	    print(res)
	    res=pool.apply_async(job,(2,))
	    print(res.get())
	    multi_res=[pool.apply_async(job,(i,)) for i in range(10)]
	    print([res.get()for res in multi_res])

	if __name__=='__main__':
	    multicore()
	结果:
	[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
	4
	[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
	简化程序如下
	from multiprocessing import Pool
	p=Pool()#默认带核心数的参数
	mult_res=p.map(func,args=[x for x in range(100)])
	map函数返回值是一个list apply_async返回的是一个类
	共享内存及进程锁：
	mp.Value().value
	mp.Array()#存取数组不能用value
	import multiprocessing as mp
	import time
	def job(v,num):
	    for _ in range(10):
	        time.sleep(0.1)#暂停0.1s，让输出效果更明显
	        v.value+=num #v.value获取共享变量值
	        print(v.value)
	def multicore():
	    v=mp.Value('i',0)#定义共享变量
	    p1=mp.Process(target=job,args=(v,1))
	    p2=mp.Process(target=job,args=(v,3))
	    p1.start()
	    p2.start()
	    p1.join()
	    p2.join()
	if __name__=='__main__':
	    multicore()
	结果:
	两个进程相互修改v的值
	def job(v, num, l):
	    l.acquire() # 锁住
	    for _ in range(5):
	        time.sleep(0.1) 
	        v.value += num # 获取共享内存
	        print(v.value)
	    l.release() # 释放
	def multicore():
	    l = mp.Lock() # 定义一个进程锁
	    v = mp.Value('i', 0) # 定义共享内存
	    p1 = mp.Process(target=job, args=(v,1,l)) # 需要将lock传入
	    p2 = mp.Process(target=job, args=(v,3,l)) 
	    p1.start()
	    p2.start()
	    p1.join()
	    p2.join()
	if __name__ == '__main__':
	    multicore()
	核心代码:
	l=mp.Lock()
	l.acquire()
	l.release()
	#没有进程锁会导致的结果 最后可能达不到20
		因为存在相互调用内存的情况
		多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。
		所以多进程的其实是改自己拥有拷贝的变量,之于共享变量的话会造成同时进行出现中断,而多线程不存在这种情况
###多线程及多线程通信设计
	由于GIL的原因多线程显得没有那么快
	速度对比结果 多进程<正常<多线程
	import threading
	threading.current_thread().name#当前线程的名字
	t=threading.Thread(target= ,name= args=)
	主线程实例的名字叫MainThread
	锁机制:线程通信避免修改变量不同步
		多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。
		lock = threading.Lock()
		锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。
		python没法跑满cpu
		因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。
	###自己对多线程锁的理解 基于python和java
		锁不代表着获得权限,它代表着别人不能获得权限,不能获得锁对象的权限
		看一段代码
			mport time, threading
			v=1
			def job( num, l):
			    l.acquire() # 锁住
			    global v
			    for _ in range(5):
			        time.sleep(0.1)
			        v += num # 获取共享内存
			        print(v,threading.current_thread().name)
			    l.release() # 释放
			def multicore():
			    l = threading.Lock()
			    t1 = threading.Thread(target=job,args=(1,l)) # 需要将lock传入
			    t2 = threading.Thread(target=job, args=(3,l))
			    t1.start()
			    t2.start()
			    t1.join()
			    t2.join()
			if __name__ == '__main__':
			    multicore()
		结果：
			2 Thread-1
			3 Thread-1
			4 Thread-1
			5 Thread-1
			6 Thread-1
			9 Thread-2
			12 Thread-2
			15 Thread-2
			18 Thread-2
			21 Thread-2
		注释掉两行控制锁的代码结果:
			4 Thread-2
			5 Thread-1
			6 Thread-1
			9 Thread-2
			12 Thread-2
			13 Thread-1
			16 Thread-2
			17 Thread-1
			18 Thread-1
			21 Thread-2
		传入l锁对象后该资源就不能被调用了因为l.acquire()不允许其他程序获得使用权
		所以thread-2在执行的时候被告知不能使用锁,所以返回等待队列,所以结果如上
		注释掉的话两线程枪资源,抢不到也不会中断故有了上述结果,而多进程会shutdown
		再来个for循环的例子
			import time, threading
			def run_thread(n,lock):
			    for i in range(3):
			        # 先要获取锁:
			        lock.acquire()
			        try:
			            # 放心地改吧:
			            print(i,n)
			            time.sleep(0.1)
			        finally:
			            # 改完了一定要释放锁:
			            lock.release()
			def multicore():
			    l = threading.Lock()
			    # for i in range(10):
			    #     t = threading.Thread(target=run_thread,args=(i,l)) # 需要将lock传入
			    #     t.start()
			    for t in [threading.Thread(target=run_thread, args=(i, l)) for i in range(10)]:
			        t.start()

			if __name__ == '__main__':
			    multicore()
		结果
			0 0
			1 0
			2 0
			0 3
			1 3
			2 3
			0 6
			1 6
			2 6
			0 9
			1 9
			2 9
			0 4
			0 5
			1 5
			2 5
			0 1
			1 1
			2 1
			0 7
			0 8
			1 8
			2 8
			1 7
			2 7
			1 4
			2 4
			0 2
			1 2
			2 2
		可以看到线程明显在争抢cpu的资源,除非线程结束否则下次执行按照上次断点来(莫名想到yield)
	死锁：
		如果多个线程要调用多个对象，而A线程调用A锁占用了A对象，B线程调用了B锁占用了B对象,A线程不能调用B对象，B线程不能调用A对象，于是一直等待。这就造成了线程“死锁”。
		Threading模块中，也有一个类，RLock，称之为可重入锁。该锁对象内部维护着一个Lock和一个counter对象。counter对象记录了acquire的次数，使得资源可以被多次require。最后，当所有RLock被release后，其他线程才能获取资源。在同一个线程中，RLock.acquire可以被多次调用，利用该特性，可以解决部分死锁问题。
		就是把 lock=threading.Lock()改成lock = threading.RLock()
ThreadLocal
	多线程如果都修改全局变量的话会出问题,所以使用局部变量,但是函数调用的时候传递麻烦
	而全局变量的修改必须加锁
	传递变量如果一层层加调用会很麻烦的于是一种新的想法
	如果用一个全局dict存放所有的Student对象，然后以thread自身作为key获得线程对应的Student对象
	global_dict = {}

	def std_thread(name):
	    std = Student(name)
	    # 把std放到全局变量global_dict中：
	    global_dict[threading.current_thread()] = std
	    do_task_1()
	    do_task_2()

	def do_task_1():
	    # 不传入std，而是根据当前线程查找：
	    std = global_dict[threading.current_thread()]
	    ...

	def do_task_2():
	    # 任何函数都可以查找出当前线程的std变量：
	    std = global_dict[threading.current_thread()]
	    ...
	这种方法理还是有点麻烦,所以localThread应运而生
	import threading

	# 创建全局ThreadLocal对象:
	local_school = threading.local()#作用是存取线程的本地属性 而非全局变量

	def process_thread(name):
	    # 绑定ThreadLocal的student:
	    local_school.student = name #把name存到对应线程的.student属性
	    std = local_school.student #获取对应线程的.student值 如果不这么写的话 默认是本地的虽然
	    #std=name 下面这句话代替上面两行 但是为什么我们不用这种写法呢
	    #很不明显的是为了回收内存,使得写法更加规范,仅此而已
	    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

	t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
	t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
	t1.start()
	t2.start()
	t1.join()
	t2.join()
	ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。
多进程还是多线程
	我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。
	如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。
	假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。
	但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。
	所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源
	多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。
	多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。
	多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。
	在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。	
	计算密集型 vs. IO密集型
	是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。
	计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。
	计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。
	第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。
	IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。
	异步IO
	考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。
	现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。
	对应到Python语言，单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。
分布式进程
	在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。
	Python的multiprocessing模块不但支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于managers模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。
	举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？
	原有的Queue可以继续使用，但是，通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。
	我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务：	
	import random, time, queue
	from multiprocessing.managers import BaseManager

	# 发送任务的队列:
	task_queue = queue.Queue()
	# 接收结果的队列:
	result_queue = queue.Queue()

	# 从BaseManager继承的QueueManager:
	class QueueManager(BaseManager):
	    pass

	# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
	QueueManager.register('get_task_queue', callable=lambda: task_queue)
	QueueManager.register('get_result_queue', callable=lambda: result_queue)
	# 绑定端口5000, 设置验证码'abc':
	manager = QueueManager(address=('', 5000), authkey=b'abc')
	# 启动Queue:
	manager.start()
	# 获得通过网络访问的Queue对象:
	task = manager.get_task_queue()
	result = manager.get_result_queue()
	# 放几个任务进去:
	for i in range(10):
	    n = random.randint(0, 10000)
	    print('Put task %d...' % n)
	    task.put(n)
	# 从result队列读取结果:
	print('Try get results...')
	for i in range(10):
	    r = result.get(timeout=10)
	    print('Result: %s' % r)
	# 关闭:
	manager.shutdown()
	print('master exit.')
	请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得的Queue接口添加。
	然后，在另一台机器上启动任务进程（本机上启动也可以）：
	# task_worker.py

	import time, sys, queue
	from multiprocessing.managers import BaseManager

	# 创建类似的QueueManager:
	class QueueManager(BaseManager):
	    pass

	# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
	QueueManager.register('get_task_queue')
	QueueManager.register('get_result_queue')

	# 连接到服务器，也就是运行task_master.py的机器:
	server_addr = '127.0.0.1'
	print('Connect to server %s...' % server_addr)
	# 端口和验证码注意保持与task_master.py设置的完全一致:
	m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
	# 从网络连接:
	m.connect()
	# 获取Queue的对象:
	task = m.get_task_queue()
	result = m.get_result_queue()
	# 从task队列取任务,并把结果写入result队列:
	for i in range(10):
	    try:
	        n = task.get(timeout=1)
	        print('run task %d * %d...' % (n, n))
	        r = '%d * %d = %d' % (n, n, n*n)
	        time.sleep(1)
	        result.put(r)
	    except Queue.Empty:
	        print('task queue is empty.')
	# 处理结束:
	print('worker exit.')	
	这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算n*n的代码换成发送邮件，就实现了邮件队列的异步发送。
	Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，Queue对象存储在task_master.py进程中：
	而Queue之所以能通过网络访问，就是通过QueueManager实现的。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如get_task_queue。
	authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果task_worker.py的authkey和task_master.py的authkey不一致，肯定连接不上。
	小结
	Python的分布式进程接口简单，封装良好，适合需要把繁重任务分布到多台机器的环境下。
	注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。
	牛逼
协程与异步IO：
	在IO操作的过程中，当前线程被挂起，而其他需要CPU执行的代码就无法被当前线程执行了
	另一种解决IO问题的方法是异步IO。当代码需要执行一个耗时的IO操作时，它只发出IO指令，并不等待IO结果，然后就去执行其他代码了。一段时间后，当IO返回结果时，再通知CPU进行处理。
	异步IO模型需要一个消息循环，在消息循环中，主线程不断地重复“读取消息-处理消息”这一过程：
		loop = get_event_loop()
		while True:
		    event = loop.get_event()
		    process_event(event)
	消息模型其实早在应用在桌面应用程序中了。一个GUI程序的主线程就负责不停地读取消息并处理消息。所有的键盘、鼠标等消息都被发送到GUI程序的消息队列中，然后由GUI程序的主线程处理。
	消息模型是如何解决同步IO必须等待IO操作这一问题的呢？当遇到IO操作时，代码只负责发出IO请求，不等待IO结果，然后直接结束本轮消息处理，进入下一轮消息处理过程。当IO操作完成后，将收到一条“IO完成”的消息，处理该消息时就可以直接获取IO操作结果。
	协程，又称微线程，纤程。英文名Coroutine。
	协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。
	协程有点像多线程,
	最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。
	第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。
	因为协程是一个线程执行，那怎么利用多核CPU呢？最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。
	Python对协程的支持是通过generator实现的。
	在generator中，我们不但可以通过for循环来迭代，还可以不断调用next()函数获取由yield语句返回的下一个值。(深入了解生成器和yield javascript的实现)
yield关键字的细节:
	y= yield x
	拆开就是 y=x yield y
	实际这么做的话会报错,但是可以这么理解
asyncio标准库 pyhton3.4以上:
	asyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。
	用asyncio实现Hello world代码如下：
	import asyncio #async异步的
	@asyncio.coroutine
	def hello():
	    print("Hello world!")
	    # 异步调用asyncio.sleep(1):
	    r = yield from asyncio.sleep(1)
	    print("Hello again!")
	# 获取EventLoop:
	loop = asyncio.get_event_loop()
	# 执行coroutine
	loop.run_until_complete(hello())
	loop.close()
	@asyncio.coroutine把一个generator标记为coroutine类型，然后，我们就把这个coroutine扔到EventLoop中执行。
	hello()会首先打印出Hello world!，然后，yield from语法可以让我们方便地调用另一个generator。由于asyncio.sleep()也是一个coroutine，所以线程不会等待asyncio.sleep()，而是直接中断并执行下一个消息循环。当asyncio.sleep()返回时，线程就可以从yield from拿到返回值（此处是None），然后接着执行下一行语句。
	把asyncio.sleep(1)看成是一个耗时1秒的IO操作，在此期间，主线程并未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。	
	
	import threading
	import asyncio
	@asyncio.coroutine
	def hello():
	    print('Hello world! (%s)' % threading.currentThread())
	    yield from asyncio.sleep(1)
	    print('Hello again! (%s)' % threading.currentThread())
	    #为什么会有下面这些结果呢,因为hello执行的时候加入循环队列了
	    #里面的所有操作都并发了,因为你IO操作那些都去等结果了,
	    #结果回来了交给下一个函数处理,可是这里没有返回值仅此而已
	loop = asyncio.get_event_loop()
	tasks = [hello(), hello()]
	loop.run_until_complete(asyncio.wait(tasks))
	loop.close()
	也就是说我们利用协程加速了IO的执行速度,换句话说,我们可以加快网络请求速度
	Hello world! (<_MainThread(MainThread, started 140735195337472)>)
	Hello world! (<_MainThread(MainThread, started 140735195337472)>)
	(暂停约1秒)
	Hello again! (<_MainThread(MainThread, started 140735195337472)>)
	Hello again! (<_MainThread(MainThread, started 140735195337472)>)
	
	#####请求网络异步#####
	import asyncio
	@asyncio.coroutine
	def wget(host):
	    print('wget %s...' % host)
	    connect = asyncio.open_connection(host, 80)
	    reader, writer = yield from connect
	    header = 'GET / HTTP/1.0\r\nHost: %s\r\n\r\n' % host
	    writer.write(header.encode('utf-8'))
	    yield from writer.drain()
	    while True:
	        line = yield from reader.readline()
	        if line == b'\r\n':
	            break
	        print('%s header > %s' % (host, line.decode('utf-8').rstrip()))
	    # Ignore the body, close the socket
	    writer.close()

	loop = asyncio.get_event_loop()
	tasks = [wget(host) for host in ['www.sina.com.cn', 'www.sohu.com', 'www.163.com']]
	loop.run_until_complete(asyncio.wait(tasks))
	loop.close()


	wget www.sohu.com...
	wget www.sina.com.cn...
	wget www.163.com...
	(等待一段时间)
	(打印出sohu的header)
	www.sohu.com header > HTTP/1.1 200 OK
	www.sohu.com header > Content-Type: text/html
	...
	(打印出sina的header)
	www.sina.com.cn header > HTTP/1.1 200 OK
	www.sina.com.cn header > Date: Wed, 20 May 2015 04:56:33 GMT
	...
	(打印出163的header)
	www.163.com header > HTTP/1.0 302 Moved Temporarily
	www.163.com header > Server: Cdn Cache Server V2.0
	...
	#完全可以按照多线程去理解这东西,为啥不像多线程那样抢资源就是因为消息队列的调度算法不同
	为了简化并更好地标识异步IO，从Python3.5开始引入了新的语法async和await，可以让coroutine的代码更简洁易读。
	请注意，async和await是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换：
	把@asyncio.coroutine替换为async；
	把yield from替换为await。
	@asyncio.coroutine
	def hello():
	    print("Hello world!")
	    r = yield from asyncio.sleep(1)
	    print("Hello again!")
	用新语法重新编写如下：
	async def hello():
	    print("Hello world!")
	    r = await asyncio.sleep(1)#记住一点这个只能是耗时操作构建成生成器
	    print("Hello again!")
	其他操作不变
	loop = asyncio.get_event_loop()
	tasks = [....]
	loop.run_until_complete(asyncio.wait(tasks))
	loop.close()
	aiohttp (pip install aiohttp)异步网络请求的一个库
	https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014320981492785ba33cc96c524223b2ea4e444077708d000

