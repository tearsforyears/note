# 结巴中文分词
	import jieba
	import jieba.posseg as psg
	from collections import Counter

	# jieba.enable_parallel(5) # windows 环境下不支持,并行分词 基于multiprocessing
	str1 = '虎牙梨涡御姐音,超绝可爱赖美云。'
	apis:
		jieba.cut(str1) # 返回生成器
		psg.cut(str1) # 返回带词标注的生成器
		Counter(list).most_common(top) # 词频统计
	属性表:
		官方文档
# WordCloud 可视化数据
	from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
	wc = WordCloud(
        background_color='white',  # 设置背景颜色
        mask=None,  # 设置背景图片 ndarray 注意大小
        font_path='C:\Windows\Fonts\STZHONGS.TTF',  # 若是有中文的话，这句代码必须添加，不然会出现方框，不出现汉字
        max_words=300,
        stopwords=STOPWORDS,  # 设置停用词
        min_font_size=2,
        max_font_size=150,  # 设置字体最大值
        random_state=30,  # 设置有多少种随机生成状态，即有多少种配色方案
        width=1000,
        height=860,
        margin=1
	)
	wc.generate_from_text(text) # 自动分词自动词频统计 也可以用结巴预分词
	wc.generate_from_frequencies(key,value) # or generate_from_frequencies(dict)
	wc.to_file('wc.png')
	# 8分钟分解遮天 19.7M
# NLTK
	# 语料库
		from nltk.corpus import brown
		brown.sents()
		brown.words()
		gutenberg	一个有若干万部的小说语料库，多是古典作品
		webtext	收集的网络广告等内容
		nps_chat	有上万条聊天消息语料库，即时聊天消息为主
		brown	一个百万词级的英语语料库，按文体进行分类
		reuters	路透社语料库，上万篇新闻方档，约有1百万字，分90个主题，并分为训练集和测试集两组
		inaugural	演讲语料库，几十个文本，都是总统演说
	# Tokenize组件分割不是分词
		nltk.word_tokenize(sentence) # 分词装入列表
	# 词频统计相关
		修改底层组建让plt能显示中文：
			from pylab import mpl
	    	mpl.rcParams['font.sans-serif'] = ['SimHei']
		重要api:
			nltk.FreqDist():
				fd = nltk.FreqDist(nltk.word_tokenize(text))
				fd.B() # 返回长度
				fd.plot(numbers) # 词频统计图
				print(fd.most_common(100)) # 前100频率 贼好用
			nltk.text.Text():
				txt = nltk.text.Text(words_list)
				txt.concordance('guess') # 匹配显示上下文 不用print
				print(txt.count('better')) # 统计词数
				txt.vocab() # 返回去重字典 顺便统计词数目
		其他api:
			nltk.text.Text():
				txt.common_context(['better','than']]) # 查看同义词习惯
				txt.dispersion_plot(['better', 'guess']) # 画关联词距离 plt
# genism(word2vec模型)
	# 因为需要借助别人pre-train的模型,所以该库的使用方法仅仅是模型导入和预测
	# 模型导入
		bin模式(不可训练):model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)
		# model模式：model = gensim.models.Word2Vec.load(model_path)
	# 模型预测:
		model['computer'] # 直接获得computer单词的词向量 np ndarray
	# 相似度相关
		model.most_similar(['煞笔'])
		[('傻帽', 0.9046832919120789), ('傻叉', 0.8966874480247498), ('傻冒', 0.8605546951293945), ('傻货', 0.8548792600631714), ('坑货', 0.8541077375411987), ('蠢蛋', 0.8530932664871216), ('怂货', 0.840782642364502), ('自恋狂', 0.8356596231460571), ('大流氓', 0.8322853446006775), ('傻比', 0.8322106599807739)]
	# 迁移学习中的数据处理
		'煞笔' in model.vocab,利用这种方式判断是否在词汇表里面
# 自然语言的特征工程
	我们需要先对原始的文本进行分词、去除停用词(利用停词表),去除出现单次的词

# 关于机器学习的部分:
	NLP在小采集的文本分析情况下完全可以使用Native Bayes作为分类器