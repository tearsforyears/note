数据集的特点
数据标签和特征(feature)
目地是为了通过数据特征去预测数据标签
监督机器学习是在数据标签存在的环境中进行学习的
而无监督机器学习则是不带数据标签的
tensorflow的编程栈
high-level:estimator..评价者评估者
mid-level:layers datasets.. metrics原型
low-level:python
tkernel tensorflow disrtibute execution engine
关注上面两个..所包含的api
tensorflow包括变量和常量的两种定义
变量定义tf.Variable(0,name="counter")
常量定义tf.constant(1)
tf.zeros()和constant的类型一样都是tensor
-------------------------------------------------------------------
state = tf.Variable(0, name="counter")
one = tf.constant(1)
new_value = tf.add(state, one)#注意这个new_value理解成一个节点
update = tf.assign(state, new_value)#合并 是图所描绘的表达式的一部分

# 启动图后, 变量必须先经过`初始化` (init) op 初始化,
# 首先必须增加一个`初始化` op 到图中.

init_op = tf.initialize_all_variables()#初始化所有变量

with tf.Session() as sess:
    sess.run(init_op)
    #等于对多有变量初始化
    # 其实写成sess.run(tf.initialize_all_variables())  
    print(sess.run(state))
    for i in range(3):
        sess.run(update)#执行update这个节点的操作
        print(sess.run(state))#如果run的是一个值就输出值
# 输出:
# 0
# 1
# 2
# 3
---------------------------------------------------------------------
Fetch
执行机制是不进行调用run就不会执行
但是在执行的时候 也可以拿到上面new_value的值
Feed
feed机制是可以用形参去定义
------------------------------------------------------------------------
x = tf.placeholder(tf.float32, shape=(1024, 1024))#定义数据类型  
y = tf.matmul(x, x)  
  
with tf.Session() as sess:  
  print(sess.run(y))  # ERROR: 此处x还没有赋值.    
  rand_array = np.random.rand(1024, 1024)  
  print(sess.run(y, feed_dict={x: rand_array}))#通过key-val这种值去定义 
------------------------------------------------------------------------
#实践
# used tensorflow to test place holder
import numpy as np
import tensorflow as tf

x_value=np.array([5.,-6.,7.,-8.]).reshape(4,1)

x=tf.placeholder(dtype="float64",shape=(4,1))
w=tf.Variable(np.random.rand(3,4))
b=tf.Variable(np.random.rand(3,1))
y=tf.matmul(w,x)+b

init=tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print("\ninit time the values\n")
    print("x=",sess.run(x,feed_dict={x:x_value}))
    print("y=",sess.run(y,feed_dict={x:x_value}))
    print("b=",sess.run(b))
    train=tf.train.GradientDescentOptimizer(0.001).minimize(y)
    
    print("\nafter 1000 trains\n")
    for i in range(1000):
        sess.run(train,feed_dict={x:x_value})
    print("y=",sess.run(y,feed_dict={x:x_value}))
    print("b=",sess.run(b))
    
    print("\nafter 2000 trains\n")
    for i in range(1000):
        sess.run(train,feed_dict={x:x_value})
    print("y=",sess.run(y,feed_dict={x:x_value}))
    print("b=",sess.run(b))
        
    print("\nafter 10000 trains\n")
    for i in range(10000):
        sess.run(train,feed_dict={x:x_value})
    print("y=",sess.run(y,feed_dict={x:x_value}))
    print("b=",sess.run(b))
    
    print("\nafter 20000 trains\n")
    for i in range(10000):
        sess.run(train,feed_dict={x:x_value})
    print("y=",sess.run(y,feed_dict={x:x_value}))
    print("b=",sess.run(b))
结果:
init the y and the x
y= 402.0
x= 8.0
after a train
y= 329.25662
x= 7.517

after 1000s train

y= 10.000002
x= 1.0014546

after 2000s train

y= 10.0
x= 1.0000099
it limited at the value x=1.000010 y=10.000000
#根据此我们可以分析 算法没有收敛 cost function选择出错
#此时如果用adam方法根本就不能用 请慎重选择
-------------------------------------------------------------------------
实现binary-classfication
#coding=utf-8
import tensorflow as tf
import numpy as np
def getData():
    f=open(r"1.txt")
    ctx=f.read()
    ctx=ctx.replace("Iris-setosa","0").replace("Iris-versicolor","1").replace("Iris-virginica","2")
    ctx=ctx.split("\n")
    #print(len(ctx))
    temp=np.zeros((len(ctx),5))
    for i in range(len(ctx)):
        for j in range(5):
            temp[i][j]=float(ctx[i].split(",")[j])
    return temp
def data():
    lis=np.zeros(dtype=np.float32,shape=[150,4])
    #print(getData().shape)
    for i in range(150):
        for j in range(5):
            if j!=4:
                lis[i][j]=getData()[i][j]
    return lis
DATA=data()[0:50].T
DATA2=data()[50:100].T
DATA3=data()[100:150].T
#print(DATA2)
#input x
def binary():
    x=tf.placeholder(tf.float32,shape=[4,None]) #4,m matrix
    y_=tf.placeholder(tf.float32,shape=[1,None]) # m个输出

    #神经网络的架构
    wT1=tf.Variable(tf.random_normal([3,4],dtype=tf.float32))
    b1=tf.Variable(tf.random_normal([3,1],dtype=tf.float32))
    y1=tf.matmul(wT1,x)+b1
    z1=tf.sigmoid(y1)

    wT2=tf.Variable(tf.random_normal([4,3],dtype=tf.float32))
    b2=tf.Variable(tf.random_normal([4,1],dtype=tf.float32))
    y2=tf.matmul(wT2,z1)+b2
    z2=tf.sigmoid(y2)

    wT3=tf.Variable(tf.random_normal([2,4],dtype=tf.float32))
    b3=tf.Variable(tf.random_normal([2,1],dtype=tf.float32))
    y3=tf.matmul(wT3,z2)+b3
    z3=tf.sigmoid(y3)

    wT4=tf.Variable(tf.random_normal([1,2],dtype=tf.float32))
    b4=tf.Variable(tf.random_normal([1,1],dtype=tf.float32))
    y4=tf.matmul(wT4,z3)+b4
    z4=tf.sigmoid(y4) #相当于y_head a predict value

    #Cost=-tf.reduce_mean(y_*tf.log(z4)) #或者把-1写到log里
    Cost=-tf.reduce_mean((1-z4)*tf.log(1-y_)+z4*tf.log(y_)) #或者把-1写到log里
    train=tf.train.AdamOptimizer(0.01).minimize(Cost)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())#init
        # print(sess.run(z4,feed_dict={x:np.array(range(8),dtype=np.float32).reshape(4,2)}))
        # 正向传播没有问题 return and 1,m matrix
        print("cost func=", sess.run(Cost, feed_dict={x: DATA, y_: np.zeros(shape=[1, 50]) + 1e-6}))
        for i in range(5000):
            sess.run(train,feed_dict={x:DATA,y_:np.zeros(shape=[1,50])+1e-6})
        print("cost func=", sess.run(Cost, feed_dict={x: DATA, y_: np.zeros(shape=[1, 50]) + 1e-6}))
binary() #出现了nan的结果把网络改小 原因是因为0和1值的关系
#深度加大优化简直超神了

def binary2():
    x=tf.placeholder(tf.float32,shape=[4,None]) #4,m matrix
    y_=tf.placeholder(tf.float32,shape=[1,None]) # m个输出

    #神经网络的架构
    wT1=tf.Variable(tf.random_normal([3,4],dtype=tf.float32))
    b1=tf.Variable(tf.random_normal([3,1],dtype=tf.float32))
    y1=tf.matmul(wT1,x)+b1
    z1=tf.sigmoid(y1)

    wT2=tf.Variable(tf.random_normal([1,3],dtype=tf.float32))
    b2=tf.Variable(tf.random_normal([1,1],dtype=tf.float32))
    y2=tf.matmul(wT2,z1)+b2
    z2=tf.sigmoid(y2)

    #Cost=-tf.reduce_mean(y_*tf.log(z2)) #或者把-1写到log里
    #交叉熵的表现明显比下面这条函数表现的要好
    Cost=-tf.reduce_mean((1-z2)*tf.log(1-y_)+z2*tf.log(y_)) #或者把-1写到log里
    #train=tf.train.GradientDescentOptimizer(0.0001).minimize(Cost)
    train=tf.train.AdamOptimizer(0.01).minimize(Cost)#只要后面没写错是可以直接这么换的
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())#init
        # print(sess.run(z4,feed_dict={x:np.array(range(8),dtype=np.float32).reshape(4,2)}))
        # 正向传播没有问题 return and 1,m matrix
        print("cost func=",sess.run(Cost,feed_dict={x:DATA,y_:np.zeros(shape=[1,50])+1e-6}))
        for i in range(500):
            sess.run(train,feed_dict={x:DATA,y_:np.zeros(shape=[1,50])+1e-6})
        print("cost func=",sess.run(Cost, feed_dict={x:DATA,y_:np.zeros(shape=[1,50])+1e-6})) #这里不建议用0 因为会出现极限inf和nan
        #print("y_predict train =",sess.run(z2,feed_dict={x:DATA}))
        #print("y_predict dev =",sess.run(z2,feed_dict={x:DATA2}))
        #print("y_predict dev =",sess.run(z2, feed_dict={x:DATA3}))

        print("y_predict train mean =", sess.run(tf.reduce_mean(z2), feed_dict={x: DATA}))
        print("y_predict dev mean =", sess.run(tf.reduce_mean(z2), feed_dict={x: DATA2}))
        print("y_predict dev mean =", sess.run(tf.reduce_mean(z2), feed_dict={x: DATA3}))
        #训练是没有问题的 但是因为数据量比较小的关系 差距不明显 我们计算下cost
        #因为数据点比较少 所以算得的结果并不稳定
        #使用交叉熵之后变得稳定
        #由此我们知道这个二分分类器效果并不好
#binary2()
数据集来自于iris鸢尾花数据 size=150
--------------------------------------------------------------------------------
softmax regression
#coding=utf-8
import tensorflow as tf
import numpy as np
import time
def getData():
    f=open(r"1.txt")
    ctx=f.read()
    ctx=ctx.replace("Iris-setosa","0").replace("Iris-versicolor","1").replace("Iris-virginica","2")
    ctx=ctx.split("\n")
    #print(len(ctx))
    temp=np.zeros((len(ctx),5))
    for i in range(len(ctx)):
        for j in range(5):
            temp[i][j]=float(ctx[i].split(",")[j])
    return temp
def data():
    lis=np.zeros(dtype=np.float32,shape=[150,4])
    #print(getData().shape)
    for i in range(150):
        for j in range(5):
            if j!=4:
                lis[i][j]=getData()[i][j]
    return lis
#print(data().T.shape)
#DATA=data()[0:50].T
#DATA2=data()[50:100].T
#DATA3=data()[100:150].T
#因为各种原因softmax 先测试150数据的
#softmax用的是之前的二分分类改进算法
def label():
    li=[]
    for i in range(50):
        li.append([1,0,0])
    for i in range(50):
        li.append([0,1,0])
    for i in range(50):
        li.append([0,0,1])
    li=np.array(li,dtype=np.float32).T
    return li
def softmax():

    x=tf.placeholder(tf.float32,shape=[4,None]) #4,m matrix
    y_=tf.placeholder(tf.float32,shape=[3,None]) #预测结果应该是一个向量

    #神经网络的架构
    wT1=tf.Variable(tf.random_normal([3,4],dtype=tf.float32))
    b1=tf.Variable(tf.random_normal([3,1],dtype=tf.float32))
    y1=tf.matmul(wT1,x)+b1
    z1=tf.sigmoid(y1)

    wT2=tf.Variable(tf.random_normal([4,3],dtype=tf.float32))
    b2=tf.Variable(tf.random_normal([4,1],dtype=tf.float32))
    y2=tf.matmul(wT2,z1)+b2
    z2=tf.sigmoid(y2)

    wT3=tf.Variable(tf.random_normal([2,4],dtype=tf.float32))
    b3=tf.Variable(tf.random_normal([2,1],dtype=tf.float32))
    y3=tf.matmul(wT3,z2)+b3
    z3=tf.sigmoid(y3)

    wT4=tf.Variable(tf.random_normal([3,2],dtype=tf.float32))
    b4=tf.Variable(tf.random_normal([3,1],dtype=tf.float32))
    y4=tf.matmul(wT4,z3)+b4
    #softmax层 3,1 matrix 因为有3个输出结果0，1，2
    z4=tf.nn.softmax(y4)
    #print(z4) #3,150 matrix
    Cost=-tf.reduce_mean(y_*tf.log(z4))#或者把-1写到log里
    #Cost=-tf.reduce_mean((1-z4)*tf.log(1-y_)+z4*tf.log(y_)) #或者把-1写到log里
    train=tf.train.AdamOptimizer(0.001).minimize(Cost) #此处优化的是一个向量 但是结果还是一个值

    xc=data().T
    la=label()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())#init
        #print(data().T.shape,np.array([j for j in range(1,4) for i in range(50)]).reshape(3,50).shape)
        #检查形状后发现后面这里很有问题 主要是因为 我们给的也行该是个向量矩阵 而不是个值
        #print("cost func=", sess.run(Cost, feed_dict={x: data().T, y_: np.array([j for j in range(1,4) for i in range(50)]).reshape(3,50)}))
        print("cost func=", sess.run(Cost, feed_dict={x:xc, y_:la}))
        for i in range(10000):
            sess.run(train,feed_dict={x:xc, y_:la})
        print("cost func=", sess.run(Cost,feed_dict={x:xc, y_:la}))

tic=time.time()
softmax() #出现了nan的结果把网络改小 原因是因为0和1值的关系
#深度加大优化简直超神了
tok=time.time()
print(tok-tic,"s")
--------------------------------------------------------------------------------------
learing rate decay
#计算公式
decayed_learning_rate = learing_rate * decay_rate^(global_step / decay_steps)
#api
tf.train.exponential_decay(learing_rate,global_step,decay_steps,decay_rate,staircase=True,name=None)
staircase这个值 False是连续衰减 True是梯形衰减 如果为True global_step / decay_steps 向下取整
-----------------------------------------------------------------------------------------
持久化模型
saver = tf.train.Saver(max_to_keep=1)#保留最后一代的模型
saver.save(sess,'ckpt/mnist.ckpt',global_step=step) #step表示训练次数 第二个参数是路径

模型的恢复
def rebuild(x):
    graph=tf.get_default_graph()
    with tf.Session() as sess:
        saver = tf.train.import_meta_graph(r'ckpt/softmax.ckpt-10000.meta')
	#恢复MetaGraph这里得到了那张计算图
        saver.restore(sess, tf.train.latest_checkpoint('ckpt/'))
        #print(sess.run('w1:0')) #测试成功获取到保存变量的值 可以获取变量的值也可以

        #手动forward-pop
        w1 = graph.get_tensor_by_name("w1:0")
        w2 = graph.get_tensor_by_name("w2:0")
        w3 = graph.get_tensor_by_name("w3:0")
        w4 = graph.get_tensor_by_name("w4:0")
        w5 = graph.get_tensor_by_name("w5:0")
        w6 = graph.get_tensor_by_name("w6:0")
        b1 = graph.get_tensor_by_name("b1:0")
        b2 = graph.get_tensor_by_name("b2:0")
        b3 = graph.get_tensor_by_name("b3:0")
        b4 = graph.get_tensor_by_name("b4:0")
        b5 = graph.get_tensor_by_name("b5:0")
        b6 = graph.get_tensor_by_name("b6:0")
        y1 = tf.matmul(w1, x) + b1
        z1 = tf.sigmoid(y1)
        y2 = tf.matmul(w2, z1) + b2
        z2 = tf.sigmoid(y2)
        y3 = tf.matmul(w3, z2) + b3
        z3 = tf.sigmoid(y3)
        y4 = tf.matmul(w4, z3) + b4
        z4 = tf.sigmoid(y4)
        y5 = tf.matmul(w5, z4) + b5
        z5 = tf.sigmoid(y5)
        y6 = tf.matmul(w6, z5) + b6
        z6 = tf.sigmoid(y6)
        print("y6",sess.run(y6))
        print("z6",sess.run(z6))
        print("perdict_item",np.where(np.abs(sess.run(z6))==np.max(np.abs(sess.run(z6))))[0][0])

# 其他一些api
    tf.slice(input,begin,size)
    A=np.array([[1,2,3],[4,5,6]])
    tf.slice(A,begin=[0,1],size=[2,2])
    [[1,2,3],
     [4,5,6]]
    A[0,1]=2 #从这一点开始 取2,2矩阵
    结果:
    [[2,3],
     [5,6]]
    # slide window?
