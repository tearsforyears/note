1：transfer learning(迁移学习)：
把alexnet的全连接层全部去掉,直接利用卷积层的参数固定,重新构建全连接层
换句话说迁移的时候参数卷积层的参数是绝对不会发生变化的,可以选择卷积层的深度进行提取
但是卷积层的参数全是常量都已经固定好了的

2：finetune（微调）：例子：在Alexnet的基础上，我们重新加上一个层再去训练网络，比如再加入一个全连接层