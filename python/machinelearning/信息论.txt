信息论
	信息熵 最优编码长度
		entropy=-∑p log p  = H(p)
	交叉熵
		cross_entropy(p,q) = -∑p log q =H(p|q)# 用于衡量两个分布间的差距
		# 相同时
	KL散度(Kullback–Leibler divergence) 相对熵  # 不对称!
		relative_entropy = ∑p log p/q  = D(p||q) or KL(p||q)
		相对熵同时还可以用信息熵和交叉熵计算
		relative_entropy = cross_entropy - entropy
		# 用概率分布q来拟合真实分布p时，产生的信息损耗
		# 相同时为0 差异越大 kl散度越大
	JS散度（Jensen-Shannon divergence）
		JS(p||q) = 0.5 * KL(p||(p+q)/2) + 0.5 * KL(q||(p+q)/2)
统计学:
	后验概率 = 似然估计 * 先验概率 / P(x) # P(x)成为证据
	真实概率 = 参数概率 * 应用该参数的模型概率
	统计学基础:
		条件概率: P(A|B)=P(A,B)/P(B) # P(A,B)为联合分布
		全概率: P(A) = ∑ P(A|Bi)P(Bi)
		bayes公式: P(Bi|A) = P(Bi)P(A|Bi)/∑ P(A|Bj)P(Bj) # 联合分布/全概率
	因为有了参数θ 才有了结果x
	先验分布(prior distribution) (手动目测大法):
		根据个人理解才原因称先验分布
		P(θ)
	后验分布(posterior distributions):
		根据结果(花费的时间)猜原因(交通方式)的是后验分布->
		P(交通方式|花费的时间)
		P(θ;x) # 可以通过贝叶斯公式转换成 先验分布*似然估计
	极大似然估计(MLE):
		求P(x;θ) # 给定观测值x猜p
		# 明显可以得到 MLE 通过最大化观测值的概率求θ
		# 后验分布直接通过给定参数 求得实际观测值
		# 先验分布 直接猜 比如θ是正态分布
	根据贝叶斯公式:
		# 此时B为条件 也就意味着B不可分
		P(A|B) = (P(B|A) * P(A)) / P(B) -> P(A|B) * P(B) = P(B|A) * P(A) = P(A,B)
		# P(A,B)为联合概率分布 就是同时发生A,B的情况下
		P(θ|x) = P(x|θ) * P(θ) / P(x)
		后验概率 = 似然估计 * 先验概率 / P(x)
	最大后验估计(MAP):
		Posterior ∝ (Likelihood∗Prior) # 对于单个样本而言
		# MLE argmax(θ, P(x;θ)) = argmax(θ, ∑ log(P(xi;θ)))
		# 思想就是观测值出现概率最大 argmax(θ,∏ P(xi))
		
		# MAP Maxaposterior
		# 最大后验估计的基本想法就是已知先验分布的情况下让概率最大
		# 思想:argmax(θ, ∏P(θ;x)) --bayes-->argmax(θ, ∏P(x;θ)*p(θ))
		
		最大后验估计本体如下
		argmax(θ, P(θ|x)) = argmax(θ, P(x|θ) * P(θ)) = MLE*P(θ)
		
		# 可以看到 最大后验估计和MLE差别没有很大,它考虑的东西比MLE多了模型本身的概率而已
		# 可计算结果比MLE要好上不少 P(θ) 是有关于θ的分布函数 可以认为是高斯的那个
		# MAP argmax(θ, P(θ|x)) = argmax(θ, ∑ log(P(xi;θ))+log(P(θ)))

		# 和MLE对比: 
		# 两者在数据量很大的情况下趋近一致,两者在数据量很小的情况下,显然后验概率更有用
	朴素贝叶斯:

	贝叶斯估计:
		利用贝叶斯方法对数学期望等进行估计

	# 极大似然估计是可以求得数学模型的 故没有提到对模型的数据期望等做估计
	# 最大后验估计,实际和MLE差不太多,也是对参数估计不过可以求得模型
	# 极大似然估计和最大后验估计都需要预先给定未知参数的模型
	# 贝叶斯估计是同样可以利用朴素贝叶斯求得模型,而贝叶斯估计指的也是求数据特点
	# 